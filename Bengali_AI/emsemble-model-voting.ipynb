{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First model(SE_Resnext_32:0.9696)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as torchtransforms\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "modelpath = \"/kaggle/input/se-resnext50-32x4d-fold2/se_resnext50_32x4d_fold2.pkl\"\n",
    "root_path=\"/kaggle/input/bengaliai-cv19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_transform_valid = torchtransforms.Compose([\n",
    "    torchtransforms.ToTensor(),\n",
    "    torchtransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "SIZE = 128\n",
    "class ClsTestDataset(Dataset):\n",
    "    def __init__(self, df, torchtransforms):\n",
    "        self.df = df\n",
    "        self.pathes = self.df.iloc[:,0].values\n",
    "        self.data = self.df.iloc[:, 1:].values\n",
    "        self.torchtransforms = torchtransforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #row = self.df.iloc[idx].values\n",
    "        path = self.pathes[idx]\n",
    "        img = self.data[idx, :]\n",
    "        img = 255 - img.reshape(HEIGHT, WIDTH).astype(np.uint8)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)       \n",
    "        img = torchtransforms.ToPILImage()(img)\n",
    "        img = self.torchtransforms(img)\n",
    "        return path, img\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "def make_loader(\n",
    "        data_folder,\n",
    "        batch_size=64,\n",
    "        num_workers=2,\n",
    "        is_shuffle = False,\n",
    "):\n",
    "\n",
    "    image_dataset = ClsTestDataset(df = data_folder,\n",
    "                                    torchtransforms = simple_transform_valid)\n",
    "\n",
    "    return DataLoader(\n",
    "    image_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=is_shuffle\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils import model_zoo\n",
    "__all__ = ['SENet', 'se_resnext50_32x4d']\n",
    "class SEModule(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, reduction):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n",
    "                             padding=0)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n",
    "                             padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        module_input = x\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return module_input * x\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for bottlenecks that implements `forward()` method.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out = self.se_module(out) + residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBottleneck(Bottleneck):\n",
    "    \"\"\"\n",
    "    Bottleneck for SENet154.\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
    "                 downsample=None):\n",
    "        super(SEBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes * 2)\n",
    "        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n",
    "                               stride=stride, padding=1, groups=groups,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes * 4)\n",
    "        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "class SEResNetBottleneck(Bottleneck):\n",
    "    \"\"\"\n",
    "    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n",
    "    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n",
    "    (the latter is used in the torchvision implementation of ResNet).\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
    "                 downsample=None):\n",
    "        super(SEResNetBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n",
    "                               stride=stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n",
    "                               groups=groups, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "class SEResNeXtBottleneck(Bottleneck):\n",
    "    \"\"\"\n",
    "    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
    "                 downsample=None, base_width=4):\n",
    "        super(SEResNeXtBottleneck, self).__init__()\n",
    "        width = math.floor(planes * (base_width / 64)) * groups\n",
    "        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n",
    "                               stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(width)\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n",
    "                               padding=1, groups=groups, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width)\n",
    "        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "class SENet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n",
    "                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n",
    "                 downsample_padding=1, num_classes=1000):        \n",
    "        super(SENet, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        if input_3x3:\n",
    "            layer0_modules = [\n",
    "                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn1', nn.BatchNorm2d(64)),\n",
    "                ('relu1', nn.ReLU(inplace=True)),\n",
    "                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn2', nn.BatchNorm2d(64)),\n",
    "                ('relu2', nn.ReLU(inplace=True)),\n",
    "                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn3', nn.BatchNorm2d(inplanes)),\n",
    "                ('relu3', nn.ReLU(inplace=True)),\n",
    "            ]\n",
    "        else:\n",
    "            layer0_modules = [\n",
    "                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n",
    "                                    padding=3, bias=False)),\n",
    "                ('bn1', nn.BatchNorm2d(inplanes)),\n",
    "                ('relu1', nn.ReLU(inplace=True)),\n",
    "            ]\n",
    "        # To preserve compatibility with Caffe weights `ceil_mode=True`\n",
    "        # is used instead of `padding=1`.\n",
    "        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n",
    "                                                    ceil_mode=True)))\n",
    "        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n",
    "        self.layer1 = self._make_layer(\n",
    "            block,\n",
    "            planes=64,\n",
    "            blocks=layers[0],\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=1,\n",
    "            downsample_padding=0\n",
    "        )\n",
    "        self.layer2 = self._make_layer(\n",
    "            block,\n",
    "            planes=128,\n",
    "            blocks=layers[1],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.layer3 = self._make_layer(\n",
    "            block,\n",
    "            planes=256,\n",
    "            blocks=layers[2],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.layer4 = self._make_layer(\n",
    "            block,\n",
    "            planes=512,\n",
    "            blocks=layers[3],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.avg_pool = nn.AvgPool2d(7, stride=1)\n",
    "        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n",
    "        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n",
    "                    downsample_kernel_size=1, downsample_padding=0):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=downsample_kernel_size, stride=stride,\n",
    "                          padding=downsample_padding, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n",
    "                            downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups, reduction))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def features(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "    def logits(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "    \n",
    "def se_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n",
    "    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n",
    "                  dropout_p=None, inplanes=64, input_3x3=False,\n",
    "                  downsample_kernel_size=1, downsample_padding=0,\n",
    "                  num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = se_resnext50_32x4d(pretrained=None)\n",
    "model.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "model.last_linear = nn.Linear(model.last_linear.in_features, 186)\n",
    "modelvalue = torch.load(modelpath, map_location='cuda:0')\n",
    "newmodelvalue = {}\n",
    "for kv in modelvalue:\n",
    "    newmodelvalue[kv[4:]]=modelvalue[kv]        \n",
    "model.load_state_dict(newmodelvalue)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmodeleval(model, dataloaders):\n",
    "    model.eval()\n",
    "    tbar = tqdm(dataloaders)\n",
    "    pathes=[]\n",
    "\n",
    "    alllogit1 = []\n",
    "    alllogit2 = []\n",
    "    alllogit3 = []\n",
    "    for path, img in tbar:\n",
    "        img = img.to(device)\n",
    "        pathes.extend(path)\n",
    "        with torch.no_grad():\n",
    "            output = model(img)\n",
    "        logit1, logit2, logit3 = output[:,: 168],\\\n",
    "                                    output[:,168: 168+11],\\\n",
    "                                    output[:,168+11:]\n",
    "        logit1 = F.softmax(logit1, dim=1).cpu().numpy()  # 对每一行进行softmax\n",
    "        logit2 = F.softmax(logit2, dim=1).cpu().numpy()\n",
    "        logit3 = F.softmax(logit3, dim=1).cpu().numpy()\n",
    "        alllogit1.extend(logit1.tolist())\n",
    "        alllogit2.extend(logit2.tolist())\n",
    "        alllogit3.extend(logit3.tolist())\n",
    "    alllogit1 = np.array(alllogit1)\n",
    "    alllogit2 = np.array(alllogit2)\n",
    "    alllogit3 = np.array(alllogit3)\n",
    "    \n",
    "    print(\"getmodeleval::alllogit1.shape\", alllogit1.shape)\n",
    "    print(\"getmodeleval::alllogit2.shape\", alllogit2.shape)\n",
    "    print(\"getmodeleval::alllogit3.shape\", alllogit3.shape)\n",
    "    return pathes, alllogit1, alllogit2, alllogit3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608d9271ca674ce38a06442d4b62ba42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "getmodeleval::alllogit1.shape (3, 168)\n",
      "getmodeleval::alllogit2.shape (3, 11)\n",
      "getmodeleval::alllogit3.shape (3, 7)\n",
      "0 1097 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7654187cac16477982be9d2654b8e507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "getmodeleval::alllogit1.shape (3, 168)\n",
      "getmodeleval::alllogit2.shape (3, 11)\n",
      "getmodeleval::alllogit3.shape (3, 7)\n",
      "1 173 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8b3f610e6b496d9d8c6123ed51d85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "getmodeleval::alllogit1.shape (3, 168)\n",
      "getmodeleval::alllogit2.shape (3, 11)\n",
      "getmodeleval::alllogit3.shape (3, 7)\n",
      "2 165 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5918ea0942f1435e90b1d31bcdae926c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "getmodeleval::alllogit1.shape (3, 168)\n",
      "getmodeleval::alllogit2.shape (3, 11)\n",
      "getmodeleval::alllogit3.shape (3, 7)\n",
      "3 162 ms\n",
      "12 12 12 12 370440 ms\n"
     ]
    }
   ],
   "source": [
    "allpathes=[]\n",
    "allpreds_root = []\n",
    "allpreds_vowel = []\n",
    "allpreds_consonant = []\n",
    "tAllBegin = time.time()\n",
    "for i in range(4):\n",
    "    test_csv = pd.read_parquet(os.path.join(root_path, f'test_image_data_{i}.parquet'))\n",
    "    tBegin = time.time()\n",
    "    dataloaders = make_loader(data_folder = test_csv,\n",
    "                                           batch_size=8,\n",
    "                                           num_workers = 2,\n",
    "                                           is_shuffle = False)\n",
    "    pathes, logit1, logit2, logit3 = getmodeleval(model, dataloaders)\n",
    "    preds_root = np.argmax(logit1, axis=1)# 其中，axis=1表示按行计算\n",
    "    preds_vowel = np.argmax(logit2, axis=1)# 其中，axis=1表示按行计算\n",
    "    preds_consonant = np.argmax(logit3, axis=1)# 其中，axis=1表示按行计算\n",
    "\n",
    "    allpathes.extend(pathes)\n",
    "    allpreds_root.extend(preds_root.tolist())\n",
    "    allpreds_vowel.extend(preds_vowel.tolist())\n",
    "    allpreds_consonant.extend(preds_consonant.tolist())\n",
    "    tEnd = time.time()\n",
    "    print(i, int(round(tEnd * 1000)) - int(round(tBegin * 1000)), \"ms\")\n",
    "tAllEnd = time.time()\n",
    "print(len(allpathes), len(allpreds_root), len(allpreds_vowel), len(allpreds_consonant),  int(round(tAllEnd * 1000)) - int(round(tAllBegin * 1000)), \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 12 12 12\n"
     ]
    }
   ],
   "source": [
    "print(len(allpathes), len(allpreds_root), len(allpreds_vowel), len(allpreds_consonant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       row_id  target\n",
      "0  Test_0_consonant_diacritic       0\n",
      "1        Test_0_grapheme_root       3\n",
      "2      Test_0_vowel_diacritic       0\n",
      "3  Test_1_consonant_diacritic       0\n",
      "4        Test_1_grapheme_root      93\n",
      "5      Test_1_vowel_diacritic       2\n",
      "6  Test_2_consonant_diacritic       0\n",
      "7        Test_2_grapheme_root      19\n",
      "8      Test_2_vowel_diacritic       0\n",
      "9  Test_3_consonant_diacritic       0\n"
     ]
    }
   ],
   "source": [
    "row_id=[]\n",
    "target=[]\n",
    "for idx, image_id in enumerate(allpathes):\n",
    "    target.extend([allpreds_consonant[idx]])\n",
    "    target.extend([allpreds_root[idx]])\n",
    "    target.extend([allpreds_vowel[idx]])\n",
    "\n",
    "    row_id.extend([str(image_id) + '_consonant_diacritic'])\n",
    "    row_id.extend([str(image_id) + '_grapheme_root'])\n",
    "    row_id.extend([str(image_id) + '_vowel_diacritic'])\n",
    "\n",
    "#print(row_id)\n",
    "#print(target)\n",
    "submission_df = pd.read_csv(root_path + '/sample_submission.csv')\n",
    "#print(submission_df.shape)\n",
    "# print(len(target))\n",
    "# print(len(row_id))\n",
    "# print(target)\n",
    "# print(row_id)\n",
    "submission_df.target = np.hstack(np.array(target).astype(np.int))\n",
    "#submission_df['target'] = np.array(target).astype(np.int)\n",
    "#submission_df['row_id'] = row_id\n",
    "print(submission_df.head(10))\n",
    "#submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            Variable Name|    Memory|\n",
      " ------------------------------------ \n",
      "|               modelvalue|     39072|\n",
      "|            newmodelvalue|     18528|\n",
      "|                 test_csv|     97235|\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n",
    "print(\" ------------------------------------ \")\n",
    "for var_name in dir():\n",
    "    if not var_name.startswith(\"_\") and sys.getsizeof(eval(var_name)) > 10000: #over 10000 byte data pick up\n",
    "        print(\"{}{: >25}{}{: >10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name)),'|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del modelvalue\n",
    "del newmodelvalue\n",
    "del test_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second model(SEResNeXt prediction with pytorch:0.9663)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ../input/pretrainedmodels/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4/ > /dev/null # no output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# --- plotly ---\n",
    "from plotly import tools, subplots\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# --- models ---\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "# --- setup ---\n",
    "pd.set_option('max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug=False\n",
    "submission=True\n",
    "batch_size=256\n",
    "device='cuda:0'\n",
    "out='.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = Path('/kaggle/input/bengaliai-cv19')\n",
    "featherdir = Path('/kaggle/input/bengaliaicv19feather')\n",
    "outdir = Path('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import six\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "class DatasetMixin(Dataset):\n",
    "\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns an example or a sequence of examples.\"\"\"\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        if isinstance(index, slice):\n",
    "            current, stop, step = index.indices(len(self))\n",
    "            return [self.get_example_wrapper(i) for i in\n",
    "                    six.moves.range(current, stop, step)]\n",
    "        elif isinstance(index, list) or isinstance(index, numpy.ndarray):\n",
    "            return [self.get_example_wrapper(i) for i in index]\n",
    "        else:\n",
    "            return self.get_example_wrapper(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of data points.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_example_wrapper(self, i):\n",
    "        \"\"\"Wrapper of `get_example`, to apply `transform` if necessary\"\"\"\n",
    "        example = self.get_example(i)\n",
    "        if self.transform:\n",
    "            example = self.transform(example)\n",
    "        return example\n",
    "\n",
    "    def get_example(self, i):\n",
    "        \"\"\"Returns the i-th example.\n",
    "\n",
    "        Implementations should override it. It should raise :class:`IndexError`\n",
    "        if the index is invalid.\n",
    "\n",
    "        Args:\n",
    "            i (int): The index of the example.\n",
    "\n",
    "        Returns:\n",
    "            The i-th example.\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class BengaliAIDataset(DatasetMixin):\n",
    "    def __init__(self, images, labels=None, transform=None, indices=None):\n",
    "        super(BengaliAIDataset, self).__init__(transform=transform)\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        if indices is None:\n",
    "            indices = np.arange(len(images))\n",
    "        self.indices = indices\n",
    "        self.train = labels is not None\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"return length of this dataset\"\"\"\n",
    "        return len(self.indices)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        \"\"\"Return i-th data\"\"\"\n",
    "        i = self.indices[i]\n",
    "        x = self.images[i]\n",
    "        # Opposite white and black: background will be white (1.0) and\n",
    "        # for future Affine transformation\n",
    "        x = (255 - x).astype(np.float32) / 255.\n",
    "        if self.train:\n",
    "            y = self.labels[i]\n",
    "            return x, y\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from skimage.transform import AffineTransform, warp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def affine_image(img):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        img: (h, w) or (1, h, w)\n",
    "\n",
    "    Returns:\n",
    "        img: (h, w)\n",
    "    \"\"\"\n",
    "    # ch, h, w = img.shape\n",
    "    # img = img / 255.\n",
    "    if img.ndim == 3:\n",
    "        img = img[0]\n",
    "\n",
    "    # --- scale ---\n",
    "    min_scale = 0.8\n",
    "    max_scale = 1.2\n",
    "    sx = np.random.uniform(min_scale, max_scale)\n",
    "    sy = np.random.uniform(min_scale, max_scale)\n",
    "\n",
    "    # --- rotation ---\n",
    "    max_rot_angle = 7\n",
    "    rot_angle = np.random.uniform(-max_rot_angle, max_rot_angle) * np.pi / 180.\n",
    "\n",
    "    # --- shear ---\n",
    "    max_shear_angle = 10\n",
    "    shear_angle = np.random.uniform(-max_shear_angle, max_shear_angle) * np.pi / 180.\n",
    "\n",
    "    # --- translation ---\n",
    "    max_translation = 4\n",
    "    tx = np.random.randint(-max_translation, max_translation)\n",
    "    ty = np.random.randint(-max_translation, max_translation)\n",
    "\n",
    "    tform = AffineTransform(scale=(sx, sy), rotation=rot_angle, shear=shear_angle,\n",
    "                            translation=(tx, ty))\n",
    "    transformed_image = warp(img, tform)\n",
    "    assert transformed_image.ndim == 2\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "def crop_char_image(image, threshold=40./255.):\n",
    "    assert image.ndim == 2\n",
    "    is_black = image > threshold\n",
    "\n",
    "    is_black_vertical = np.sum(is_black, axis=0) > 0\n",
    "    is_black_horizontal = np.sum(is_black, axis=1) > 0\n",
    "    left = np.argmax(is_black_horizontal)\n",
    "    right = np.argmax(is_black_horizontal[::-1])\n",
    "    top = np.argmax(is_black_vertical)\n",
    "    bottom = np.argmax(is_black_vertical[::-1])\n",
    "    height, width = image.shape\n",
    "    cropped_image = image[left:height - right, top:width - bottom]\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "def resize(image, size=(128, 128)):\n",
    "    return cv2.resize(image, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def add_gaussian_noise(x, sigma):\n",
    "    x += np.random.randn(*x.shape) * sigma\n",
    "    x = np.clip(x, 0., 1.)\n",
    "    return x\n",
    "\n",
    "\n",
    "class Transform:\n",
    "    def __init__(self, affine=True, crop=True, size=(64, 64),\n",
    "                 normalize=True, train=True, threshold=40.,\n",
    "                 sigma=-1.):\n",
    "        self.affine = affine\n",
    "        self.crop = crop\n",
    "        self.size = size\n",
    "        self.normalize = normalize\n",
    "        self.train = train\n",
    "        self.threshold = threshold / 255.\n",
    "        self.sigma = sigma / 255.\n",
    "\n",
    "    def __call__(self, example):\n",
    "        if self.train:\n",
    "            x, y = example\n",
    "        else:\n",
    "            x = example\n",
    "        # --- Augmentation ---\n",
    "        if self.affine:\n",
    "            x = affine_image(x)\n",
    "\n",
    "        # --- Train/Test common preprocessing ---\n",
    "        if self.crop:\n",
    "            x = crop_char_image(x, threshold=self.threshold)\n",
    "        if self.size is not None:\n",
    "            x = resize(x, size=self.size)\n",
    "        if self.sigma > 0.:\n",
    "            x = add_gaussian_noise(x, sigma=self.sigma)\n",
    "        if self.normalize:\n",
    "            x = (x.astype(np.float32) - 0.0692) / 0.2051\n",
    "        if x.ndim == 2:\n",
    "            x = x[None, :, :]\n",
    "        x = x.astype(np.float32)\n",
    "        if self.train:\n",
    "            y = y.astype(np.int64)\n",
    "            return x, y\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def residual_add(lhs, rhs):\n",
    "    lhs_ch, rhs_ch = lhs.shape[1], rhs.shape[1]\n",
    "    if lhs_ch < rhs_ch:\n",
    "        out = lhs + rhs[:, :lhs_ch]\n",
    "    elif lhs_ch > rhs_ch:\n",
    "        out = torch.cat([lhs[:, :rhs_ch] + rhs, lhs[:, rhs_ch:]], dim=1)\n",
    "    else:\n",
    "        out = lhs + rhs\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "class LazyLoadModule(nn.Module):\n",
    "    \"\"\"Lazy buffer/parameter loading using load_state_dict_pre_hook\n",
    "\n",
    "    Define all buffer/parameter in `_lazy_buffer_keys`/`_lazy_parameter_keys` and\n",
    "    save buffer with `register_buffer`/`register_parameter`\n",
    "    method, which can be outside of __init__ method.\n",
    "    Then this module can load any shape of Tensor during de-serializing.\n",
    "\n",
    "    Note that default value of lazy buffer is torch.Tensor([]), while lazy parameter is None.\n",
    "    \"\"\"\n",
    "    _lazy_buffer_keys: List[str] = []     # It needs to be override to register lazy buffer\n",
    "    _lazy_parameter_keys: List[str] = []  # It needs to be override to register lazy parameter\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LazyLoadModule, self).__init__()\n",
    "        for k in self._lazy_buffer_keys:\n",
    "            self.register_buffer(k, torch.tensor([]))\n",
    "        for k in self._lazy_parameter_keys:\n",
    "            self.register_parameter(k, None)\n",
    "        self._register_load_state_dict_pre_hook(self._hook)\n",
    "\n",
    "    def _hook(self, state_dict, prefix, local_metadata, strict, missing_keys,\n",
    "             unexpected_keys, error_msgs):\n",
    "        for key in self._lazy_buffer_keys:\n",
    "            self.register_buffer(key, state_dict[prefix + key])\n",
    "\n",
    "        for key in self._lazy_parameter_keys:\n",
    "            self.register_parameter(key, Parameter(state_dict[prefix + key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn import init\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LazyLinear(LazyLoadModule):\n",
    "    \"\"\"Linear module with lazy input inference\n",
    "\n",
    "    `in_features` can be `None`, and it is determined at the first time of forward step dynamically.\n",
    "    \"\"\"\n",
    "\n",
    "    __constants__ = ['bias', 'in_features', 'out_features']\n",
    "    _lazy_parameter_keys = ['weight']\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(LazyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        if in_features is not None:\n",
    "            self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "            self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.weight is None:\n",
    "            self.in_features = input.shape[-1]\n",
    "            self.weight = Parameter(torch.Tensor(self.out_features, self.in_features))\n",
    "            self.reset_parameters()\n",
    "\n",
    "            # Need to send lazy defined parameter to device...\n",
    "            self.to(input.device)\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LinearBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True,\n",
    "                 use_bn=True, activation=F.relu, dropout_ratio=-1, residual=False,):\n",
    "        super(LinearBlock, self).__init__()\n",
    "        if in_features is None:\n",
    "            self.linear = LazyLinear(in_features, out_features, bias=bias)\n",
    "        else:\n",
    "            self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        if use_bn:\n",
    "            self.bn = nn.BatchNorm1d(out_features)\n",
    "        if dropout_ratio > 0.:\n",
    "            self.dropout = nn.Dropout(p=dropout_ratio)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "        self.activation = activation\n",
    "        self.use_bn = use_bn\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.residual = residual\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = self.linear(x)\n",
    "        if self.use_bn:\n",
    "            h = self.bn(h)\n",
    "        if self.activation is not None:\n",
    "            h = self.activation(h)\n",
    "        if self.residual:\n",
    "            h = residual_add(h, x)\n",
    "        if self.dropout_ratio > 0:\n",
    "            h = self.dropout(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretrainedmodels\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential\n",
    "\n",
    "\n",
    "class PretrainedCNN(nn.Module):\n",
    "    def __init__(self, model_name='se_resnext101_32x4d',\n",
    "                 in_channels=1, out_dim=10, use_bn=True,\n",
    "                 pretrained=None):\n",
    "        super(PretrainedCNN, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(\n",
    "            in_channels, 3, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.base_model = pretrainedmodels.__dict__[model_name](pretrained=pretrained)\n",
    "        activation = F.leaky_relu\n",
    "        self.do_pooling = True\n",
    "        if self.do_pooling:\n",
    "            inch = self.base_model.last_linear.in_features\n",
    "        else:\n",
    "            inch = None\n",
    "        hdim = 512\n",
    "        lin1 = LinearBlock(inch, hdim, use_bn=use_bn, activation=activation, residual=False)\n",
    "        lin2 = LinearBlock(hdim, out_dim, use_bn=use_bn, activation=None, residual=False)\n",
    "        self.lin_layers = Sequential(lin1, lin2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv0(x)\n",
    "        h = self.base_model.features(h)\n",
    "\n",
    "        if self.do_pooling:\n",
    "            h = torch.sum(h, dim=(-1, -2))\n",
    "        else:\n",
    "            # [128, 2048, 4, 4] when input is (128, 128)\n",
    "            bs, ch, height, width = h.shape\n",
    "            h = h.view(bs, ch*height*width)\n",
    "        for layer in self.lin_layers:\n",
    "            h = layer(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def accuracy(y, t):\n",
    "    pred_label = torch.argmax(y, dim=1)\n",
    "    count = pred_label.shape[0]\n",
    "    correct = (pred_label == t).sum().type(torch.float32)\n",
    "    acc = correct / count\n",
    "    return acc\n",
    "\n",
    "\n",
    "class BengaliClassifier(nn.Module):\n",
    "    def __init__(self, predictor, n_grapheme=168, n_vowel=11, n_consonant=7):\n",
    "        super(BengaliClassifier, self).__init__()\n",
    "        self.n_grapheme = n_grapheme\n",
    "        self.n_vowel = n_vowel\n",
    "        self.n_consonant = n_consonant\n",
    "        self.n_total_class = self.n_grapheme + self.n_vowel + self.n_consonant\n",
    "        self.predictor = predictor\n",
    "\n",
    "        self.metrics_keys = [\n",
    "            'loss', 'loss_grapheme', 'loss_vowel', 'loss_consonant',\n",
    "            'acc_grapheme', 'acc_vowel', 'acc_consonant']\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        pred = self.predictor(x)\n",
    "        if isinstance(pred, tuple):\n",
    "            assert len(pred) == 3\n",
    "            preds = pred\n",
    "        else:\n",
    "            assert pred.shape[1] == self.n_total_class\n",
    "            preds = torch.split(pred, [self.n_grapheme, self.n_vowel, self.n_consonant], dim=1)\n",
    "        loss_grapheme = F.cross_entropy(preds[0], y[:, 0])\n",
    "        loss_vowel = F.cross_entropy(preds[1], y[:, 1])\n",
    "        loss_consonant = F.cross_entropy(preds[2], y[:, 2])\n",
    "        loss = loss_grapheme + loss_vowel + loss_consonant\n",
    "        metrics = {\n",
    "            'loss': loss.item(),\n",
    "            'loss_grapheme': loss_grapheme.item(),\n",
    "            'loss_vowel': loss_vowel.item(),\n",
    "            'loss_consonant': loss_consonant.item(),\n",
    "            'acc_grapheme': accuracy(preds[0], y[:, 0]),\n",
    "            'acc_vowel': accuracy(preds[1], y[:, 1]),\n",
    "            'acc_consonant': accuracy(preds[2], y[:, 2]),\n",
    "        }\n",
    "        return loss, metrics, pred\n",
    "\n",
    "    def calc(self, data_loader):\n",
    "        device: torch.device = next(self.parameters()).device\n",
    "        self.eval()\n",
    "        output_list = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(data_loader):\n",
    "                # TODO: support general preprocessing.\n",
    "                # If `data` is not `Data` instance, `to` method is not supported!\n",
    "                batch = batch.to(device)\n",
    "                pred = self.predictor(batch)\n",
    "                output_list.append(pred)\n",
    "        output = torch.cat(output_list, dim=0)\n",
    "        preds = torch.split(output, [self.n_grapheme, self.n_vowel, self.n_consonant], dim=1)\n",
    "        return preds\n",
    "\n",
    "    def predict_proba(self, data_loader):\n",
    "        preds = self.calc(data_loader)\n",
    "        return [F.softmax(p, dim=1) for p in preds]\n",
    "\n",
    "    def predict(self, data_loader):\n",
    "        preds = self.calc(data_loader)\n",
    "        pred_labels = [torch.argmax(p, dim=1) for p in preds]\n",
    "        return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_predictor(arch, out_dim, model_name=None):\n",
    "    if arch == 'pretrained':\n",
    "        predictor = PretrainedCNN(in_channels=1, out_dim=out_dim, model_name=model_name)\n",
    "    else:\n",
    "        raise ValueError(\"[ERROR] Unexpected value arch={}\".format(arch))\n",
    "    return predictor\n",
    "\n",
    "\n",
    "def build_classifier(arch, load_model_path, n_total, model_name='', device='cuda:0'):\n",
    "    if isinstance(device, str):\n",
    "        device = torch.device(device)\n",
    "    predictor = build_predictor(arch, out_dim=n_total, model_name=model_name)\n",
    "    print('predictor', type(predictor))\n",
    "    classifier = BengaliClassifier(predictor)\n",
    "    if load_model_path:\n",
    "        predictor.load_state_dict(torch.load(load_model_path))\n",
    "    else:\n",
    "        print(\"[WARNING] Unexpected value load_model_path={}\"\n",
    "              .format(load_model_path))\n",
    "    classifier.to(device)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prepare_image(datadir, featherdir, data_type='train',\n",
    "                  submission=False, indices=[0, 1, 2, 3]):\n",
    "    assert data_type in ['train', 'test']\n",
    "    if submission:\n",
    "        image_df_list = [pd.read_parquet(datadir / f'{data_type}_image_data_{i}.parquet')\n",
    "                         for i in indices]\n",
    "    else:\n",
    "        image_df_list = [pd.read_feather(featherdir / f'{data_type}_image_data_{i}.feather')\n",
    "                         for i in indices]\n",
    "\n",
    "    print('image_df_list', len(image_df_list))\n",
    "    HEIGHT = 137\n",
    "    WIDTH = 236\n",
    "    images = [df.iloc[:, 1:].values.reshape(-1, HEIGHT, WIDTH) for df in image_df_list]\n",
    "    del image_df_list\n",
    "    gc.collect()\n",
    "    images = np.concatenate(images, axis=0)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_core(test_images, image_size, threshold,\n",
    "                 arch, n_total, model_name, load_model_path, batch_size=512, device='cuda:0', **kwargs):\n",
    "    classifier = build_classifier(arch, load_model_path, n_total, model_name, device=device)\n",
    "    test_dataset = BengaliAIDataset(\n",
    "        test_images, None,\n",
    "        transform=Transform(affine=False, crop=True, size=(image_size, image_size),\n",
    "                            threshold=threshold, train=False))\n",
    "    print('test_dataset', len(test_dataset))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_pred_proba = classifier.predict_proba(test_loader)\n",
    "    return test_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_total 186\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(device)\n",
    "n_grapheme = 168\n",
    "n_vowel = 11\n",
    "n_consonant = 7\n",
    "n_total = n_grapheme + n_vowel + n_consonant\n",
    "print('n_total', n_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/chainer_chemistry/__init__.py:13: UserWarning:\n",
      "\n",
      "A module chainer_chemistry.datasets was not imported, probably because RDKit is not installed. To install RDKit, please follow instruction in https://github.com/pfnet-research/chainer-chemistry#installation.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_df_list 1\n",
      "n_dataset=3\n",
      "j 0 updated train_args_dict {'image_size': 128, 'threshold': 20.0, 'arch': 'pretrained', 'model_name': 'se_resnext50_32x4d', 'load_model_path': '/kaggle/input/bengaliaicv19-trainedmodels/predictor_0.pt', 'device': device(type='cuda', index=0), 'batch_size': 256, 'debug': False}\n",
      "predictor <class '__main__.PretrainedCNN'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset 3\n",
      "j 1 updated train_args_dict {'image_size': 128, 'threshold': 20.0, 'arch': 'pretrained', 'model_name': 'se_resnext50_32x4d', 'load_model_path': '/kaggle/input/bengaliaicv19-trainedmodels/predictor_1.pt', 'device': device(type='cuda', index=0), 'batch_size': 256, 'debug': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictor <class '__main__.PretrainedCNN'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 42.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset 3\n",
      "j 2 updated train_args_dict {'image_size': 128, 'threshold': 20.0, 'arch': 'pretrained', 'model_name': 'se_resnext50_32x4d', 'load_model_path': '/kaggle/input/bengaliaicv19-trainedmodels/predictor_2.pt', 'device': device(type='cuda', index=0), 'batch_size': 256, 'debug': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictor <class '__main__.PretrainedCNN'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 42.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset 3\n",
      "j 3 updated train_args_dict {'image_size': 128, 'threshold': 20.0, 'arch': 'pretrained', 'model_name': 'se_resnext50_32x4d', 'load_model_path': '/kaggle/input/bengaliaicv19-trainedmodels/predictor_3.pt', 'device': device(type='cuda', index=0), 'batch_size': 256, 'debug': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictor <class '__main__.PretrainedCNN'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 42.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset 3\n",
      "p0 (3,) p1 (3,) p2 (3,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_df_list 1\n",
      "n_dataset=3\n",
      "j 0 updated train_args_dict {'image_size': 128, 'threshold': 20.0, 'arch': 'pretrained', 'model_name': 'se_resnext50_32x4d', 'load_model_path': '/kaggle/input/bengaliaicv19-trainedmodels/predictor_0.pt', 'device': device(type='cuda', index=0), 'batch_size': 256, 'debug': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictor <class '__main__.PretrainedCNN'>\n",
      "test_dataset 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 34.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j 1 updated train_args_dict {'image_size': 128, 'threshold': 20.0, 'arch': 'pretrained', 'model_name': 'se_resnext50_32x4d', 'load_model_path': '/kaggle/input/bengaliaicv19-trainedmodels/predictor_1.pt', 'device': device(type='cuda', index=0), 'batch_size': 256, 'debug': False}\n",
      "predictor <class '__main__.PretrainedCNN'>\n",
      "test_dataset 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j 2 updated train_args_dict {'image_size': 128, 'threshold': 20.0, 'arch': 'pretrained', 'model_name': 'se_resnext50_32x4d', 'load_model_path': '/kaggle/input/bengaliaicv19-trainedmodels/predictor_2.pt', 'device': device(type='cuda', index=0), 'batch_size': 256, 'debug': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictor <class '__main__.PretrainedCNN'>\n",
      "test_dataset 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 40.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j 3 updated train_args_dict {'image_size': 128, 'threshold': 20.0, 'arch': 'pretrained', 'model_name': 'se_resnext50_32x4d', 'load_model_path': '/kaggle/input/bengaliaicv19-trainedmodels/predictor_3.pt', 'device': device(type='cuda', index=0), 'batch_size': 256, 'debug': False}\n",
      "predictor <class '__main__.PretrainedCNN'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 41.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset 3\n",
      "p0 (3,) p1 (3,) p2 (3,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_df_list 1\n",
      "n_dataset=3\n",
      "j 0 updated train_args_dict {'image_size': 128, 'threshold': 20.0, 'arch': 'pretrained', 'model_name': 'se_resnext50_32x4d', 'load_model_path': '/kaggle/input/bengaliaicv19-trainedmodels/predictor_0.pt', 'device': device(type='cuda', index=0), 'batch_size': 256, 'debug': False}\n",
      "predictor <class '__main__.PretrainedCNN'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 41.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset 3\n",
      "j 1 updated train_args_dict {'image_size': 128, 'threshold': 20.0, 'arch': 'pretrained', 'model_name': 'se_resnext50_32x4d', 'load_model_path': '/kaggle/input/bengaliaicv19-trainedmodels/predictor_1.pt', 'device': device(type='cuda', index=0), 'batch_size': 256, 'debug': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictor <class '__main__.PretrainedCNN'>\n",
      "test_dataset 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 39.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j 2 updated train_args_dict {'image_size': 128, 'threshold': 20.0, 'arch': 'pretrained', 'model_name': 'se_resnext50_32x4d', 'load_model_path': '/kaggle/input/bengaliaicv19-trainedmodels/predictor_2.pt', 'device': device(type='cuda', index=0), 'batch_size': 256, 'debug': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictor <class '__main__.PretrainedCNN'>\n",
      "test_dataset 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 36.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j 3 updated train_args_dict {'image_size': 128, 'threshold': 20.0, 'arch': 'pretrained', 'model_name': 'se_resnext50_32x4d', 'load_model_path': '/kaggle/input/bengaliaicv19-trainedmodels/predictor_3.pt', 'device': device(type='cuda', index=0), 'batch_size': 256, 'debug': False}\n",
      "predictor <class '__main__.PretrainedCNN'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 27.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset 3\n",
      "p0 (3,) p1 (3,) p2 (3,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_df_list 1\n",
      "n_dataset=3\n",
      "j 0 updated train_args_dict {'image_size': 128, 'threshold': 20.0, 'arch': 'pretrained', 'model_name': 'se_resnext50_32x4d', 'load_model_path': '/kaggle/input/bengaliaicv19-trainedmodels/predictor_0.pt', 'device': device(type='cuda', index=0), 'batch_size': 256, 'debug': False}\n",
      "predictor <class '__main__.PretrainedCNN'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 37.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset 3\n",
      "j 1 updated train_args_dict {'image_size': 128, 'threshold': 20.0, 'arch': 'pretrained', 'model_name': 'se_resnext50_32x4d', 'load_model_path': '/kaggle/input/bengaliaicv19-trainedmodels/predictor_1.pt', 'device': device(type='cuda', index=0), 'batch_size': 256, 'debug': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 43.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictor <class '__main__.PretrainedCNN'>\n",
      "test_dataset 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j 2 updated train_args_dict {'image_size': 128, 'threshold': 20.0, 'arch': 'pretrained', 'model_name': 'se_resnext50_32x4d', 'load_model_path': '/kaggle/input/bengaliaicv19-trainedmodels/predictor_2.pt', 'device': device(type='cuda', index=0), 'batch_size': 256, 'debug': False}\n",
      "predictor <class '__main__.PretrainedCNN'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 42.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset 3\n",
      "j 3 updated train_args_dict {'image_size': 128, 'threshold': 20.0, 'arch': 'pretrained', 'model_name': 'se_resnext50_32x4d', 'load_model_path': '/kaggle/input/bengaliaicv19-trainedmodels/predictor_3.pt', 'device': device(type='cuda', index=0), 'batch_size': 256, 'debug': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictor <class '__main__.PretrainedCNN'>\n",
      "test_dataset 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 39.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0 (3,) p1 (3,) p2 (3,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from chainer_chemistry.utils import save_json, load_json\n",
    "\n",
    "\n",
    "#--- Prediction ---\n",
    "traindir = '/kaggle/input/bengaliaicv19-trainedmodels/'\n",
    "data_type = 'test'\n",
    "test_preds_list = []\n",
    "\n",
    "for i in range(4):\n",
    "    # --- prepare data ---\n",
    "    indices = [i]\n",
    "    test_images = prepare_image(\n",
    "        datadir, featherdir, data_type=data_type, submission=submission, indices=indices)\n",
    "    n_dataset = len(test_images)\n",
    "    print(f'n_dataset={n_dataset}')\n",
    "    # print(f'i={i}, n_dataset={n_dataset}')\n",
    "    # test_data_size = 200 if debug else int(n_dataset * 0.9)\n",
    "\n",
    "    model_preds_list = []\n",
    "    for j in range(4):\n",
    "        # --- Depends on train configuration ---\n",
    "        train_args_dict = load_json(os.path.join(traindir, f'args_{j}.json'))\n",
    "        train_args_dict.update({\n",
    "            'load_model_path': os.path.join(traindir, f'predictor_{j}.pt'),\n",
    "            'device': device,\n",
    "            'batch_size': batch_size,\n",
    "            'debug': debug,\n",
    "        })\n",
    "        print(f'j {j} updated train_args_dict {train_args_dict}')\n",
    "        test_preds = predict_core(\n",
    "                test_images=test_images, n_total=n_total,\n",
    "                **train_args_dict)\n",
    "\n",
    "        model_preds_list.append(test_preds)\n",
    "\n",
    "    # --- ensemble ---\n",
    "    proba0 = torch.mean(torch.stack([test_preds[0] for test_preds in model_preds_list], dim=0), dim=0)\n",
    "    proba1 = torch.mean(torch.stack([test_preds[1] for test_preds in model_preds_list], dim=0), dim=0)\n",
    "    proba2 = torch.mean(torch.stack([test_preds[2] for test_preds in model_preds_list], dim=0), dim=0)\n",
    "    p0 = torch.argmax(proba0, dim=1).cpu().numpy()\n",
    "    p1 = torch.argmax(proba1, dim=1).cpu().numpy()\n",
    "    p2 = torch.argmax(proba2, dim=1).cpu().numpy()\n",
    "    print('p0', p0.shape, 'p1', p1.shape, 'p2', p2.shape)\n",
    "\n",
    "    test_preds_list.append([p0, p1, p2])\n",
    "    if debug:\n",
    "        break\n",
    "    del test_images\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 8688.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat: p0 (12,) p1 (12,) p2 (12,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p0 = np.concatenate([test_preds[0] for test_preds in test_preds_list], axis=0)\n",
    "p1 = np.concatenate([test_preds[1] for test_preds in test_preds_list], axis=0)\n",
    "p2 = np.concatenate([test_preds[2] for test_preds in test_preds_list], axis=0)\n",
    "print('concat:', 'p0', p0.shape, 'p1', p1.shape, 'p2', p2.shape)\n",
    "\n",
    "row_id = []\n",
    "target = []\n",
    "for i in tqdm(range(len(p0))):\n",
    "    row_id += [f'Test_{i}_grapheme_root', f'Test_{i}_vowel_diacritic',\n",
    "               f'Test_{i}_consonant_diacritic']\n",
    "    target += [p0[i], p1[i], p2[i]]\n",
    "submission_df2 = pd.DataFrame({'row_id': row_id, 'target': target})\n",
    "#submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            Variable Name|    Memory|\n",
      " ------------------------------------ \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n",
    "print(\" ------------------------------------ \")\n",
    "for var_name in dir():\n",
    "    if not var_name.startswith(\"_\") and sys.getsizeof(eval(var_name)) > 10000: #over 10000 byte data pick up\n",
    "        print(\"{}{: >25}{}{: >10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name)),'|'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third model(Multi_Output_CNN:0.9518)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import time, gc\n",
    "import cv2\n",
    "\n",
    "from tensorflow import keras\n",
    "import matplotlib.image as mpimg\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.models import clone_model\n",
    "from keras.layers import Dense,Conv2D,Flatten,MaxPool2D,Dropout,BatchNormalization, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_df_ = pd.read_csv('/kaggle/input/bengaliai-cv19/train.csv')\n",
    "test_df_ = pd.read_csv('/kaggle/input/bengaliai-cv19/test.csv')\n",
    "class_map_df = pd.read_csv('/kaggle/input/bengaliai-cv19/class_map.csv')\n",
    "sample_sub_df = pd.read_csv('/kaggle/input/bengaliai-cv19/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HEIGHT = 236\n",
    "WIDTH = 236\n",
    "\n",
    "def get_n(df, field, n, top=True):\n",
    "    top_graphemes = df.groupby([field]).size().reset_index(name='counts')['counts'].sort_values(ascending=not top)[:n]\n",
    "    top_grapheme_roots = top_graphemes.index\n",
    "    top_grapheme_counts = top_graphemes.values\n",
    "    top_graphemes = class_map_df[class_map_df['component_type'] == field].reset_index().iloc[top_grapheme_roots]\n",
    "    top_graphemes.drop(['component_type', 'label'], axis=1, inplace=True)\n",
    "    top_graphemes.loc[:, 'count'] = top_grapheme_counts\n",
    "    return top_graphemes\n",
    "\n",
    "def image_from_char(char):\n",
    "    image = Image.new('RGB', (WIDTH, HEIGHT))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    myfont = ImageFont.truetype('/kaggle/input/kalpurush-fonts/kalpurush-2.ttf', 120)\n",
    "    w, h = draw.textsize(char, font=myfont)\n",
    "    draw.text(((WIDTH - w) / 2,(HEIGHT - h) / 3), char, font=myfont)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_df_ = train_df_.drop(['grapheme'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_df_[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']] = train_df_[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']].astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMG_SIZE=64\n",
    "N_CHANNELS=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def resize(df, size=64, need_progress_bar=True):\n",
    "    resized = {}\n",
    "    resize_size=64\n",
    "    if need_progress_bar:\n",
    "        for i in tqdm(range(df.shape[0])):\n",
    "            image=df.loc[df.index[i]].values.reshape(137,236)\n",
    "            _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "            contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
    "\n",
    "            idx = 0 \n",
    "            ls_xmin = []\n",
    "            ls_ymin = []\n",
    "            ls_xmax = []\n",
    "            ls_ymax = []\n",
    "            for cnt in contours:\n",
    "                idx += 1\n",
    "                x,y,w,h = cv2.boundingRect(cnt)\n",
    "                ls_xmin.append(x)\n",
    "                ls_ymin.append(y)\n",
    "                ls_xmax.append(x + w)\n",
    "                ls_ymax.append(y + h)\n",
    "            xmin = min(ls_xmin)\n",
    "            ymin = min(ls_ymin)\n",
    "            xmax = max(ls_xmax)\n",
    "            ymax = max(ls_ymax)\n",
    "\n",
    "            roi = image[ymin:ymax,xmin:xmax]\n",
    "            resized_roi = cv2.resize(roi, (resize_size, resize_size),interpolation=cv2.INTER_AREA)\n",
    "            resized[df.index[i]] = resized_roi.reshape(-1)\n",
    "    else:\n",
    "        for i in range(df.shape[0]):\n",
    "            #image = cv2.resize(df.loc[df.index[i]].values.reshape(137,236),(size,size),None,fx=0.5,fy=0.5,interpolation=cv2.INTER_AREA)\n",
    "            image=df.loc[df.index[i]].values.reshape(137,236)\n",
    "            _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "            contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
    "\n",
    "            idx = 0 \n",
    "            ls_xmin = []\n",
    "            ls_ymin = []\n",
    "            ls_xmax = []\n",
    "            ls_ymax = []\n",
    "            for cnt in contours:\n",
    "                idx += 1\n",
    "                x,y,w,h = cv2.boundingRect(cnt)\n",
    "                ls_xmin.append(x)\n",
    "                ls_ymin.append(y)\n",
    "                ls_xmax.append(x + w)\n",
    "                ls_ymax.append(y + h)\n",
    "            xmin = min(ls_xmin)\n",
    "            ymin = min(ls_ymin)\n",
    "            xmax = max(ls_xmax)\n",
    "            ymax = max(ls_ymax)\n",
    "\n",
    "            roi = image[ymin:ymax,xmin:xmax]\n",
    "            resized_roi = cv2.resize(roi, (resize_size, resize_size),interpolation=cv2.INTER_AREA)\n",
    "            resized[df.index[i]] = resized_roi.reshape(-1)\n",
    "    resized = pd.DataFrame(resized).T\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_dummies(df):\n",
    "    cols = []\n",
    "    for col in df:\n",
    "        cols.append(pd.get_dummies(df[col].astype(str)))\n",
    "    return pd.concat(cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs = Input(shape = (IMG_SIZE, IMG_SIZE, 1))\n",
    "\n",
    "model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1))(inputs)\n",
    "model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "model = Conv2D(filters=32, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
    "model = Dropout(rate=0.3)(model)\n",
    "\n",
    "model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "model = Conv2D(filters=64, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = Dropout(rate=0.3)(model)\n",
    "\n",
    "model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "model = Conv2D(filters=128, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = Dropout(rate=0.3)(model)\n",
    "\n",
    "model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "model = Conv2D(filters=256, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = Dropout(rate=0.3)(model)\n",
    "\n",
    "model = Flatten()(model)\n",
    "model = Dense(1024, activation = \"relu\")(model)\n",
    "model = Dropout(rate=0.3)(model)\n",
    "dense = Dense(512, activation = \"relu\")(model)\n",
    "\n",
    "head_root = Dense(168, activation = 'softmax')(dense)\n",
    "head_vowel = Dense(11, activation = 'softmax')(dense)\n",
    "head_consonant = Dense(7, activation = 'softmax')(dense)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[head_root, head_vowel, head_consonant])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Set a learning rate annealer. Learning rate will be half after 3 epochs if accuracy is not increased\n",
    "learning_rate_reduction_root = ReduceLROnPlateau(monitor='dense_3_accuracy', \n",
    "                                            patience=3, \n",
    "                                            verbose=1,\n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "learning_rate_reduction_vowel = ReduceLROnPlateau(monitor='dense_4_accuracy', \n",
    "                                            patience=3, \n",
    "                                            verbose=1,\n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "learning_rate_reduction_consonant = ReduceLROnPlateau(monitor='dense_5_accuracy', \n",
    "                                            patience=3, \n",
    "                                            verbose=1,\n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size = 256\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class MultiOutputDataGenerator(keras.preprocessing.image.ImageDataGenerator):\n",
    "\n",
    "    def flow(self,\n",
    "             x,\n",
    "             y=None,\n",
    "             batch_size=32,\n",
    "             shuffle=True,\n",
    "             sample_weight=None,\n",
    "             seed=71,\n",
    "             save_to_dir=None,\n",
    "             save_prefix='',\n",
    "             save_format='png',\n",
    "             subset=None):\n",
    "\n",
    "        targets = None\n",
    "        target_lengths = {}\n",
    "        ordered_outputs = []\n",
    "        for output, target in y.items():\n",
    "            if targets is None:\n",
    "                targets = target\n",
    "            else:\n",
    "                targets = np.concatenate((targets, target), axis=1)\n",
    "            target_lengths[output] = target.shape[1]\n",
    "            ordered_outputs.append(output)\n",
    "\n",
    "\n",
    "        for flowx, flowy in super().flow(x, targets, batch_size=batch_size,\n",
    "                                         shuffle=shuffle):\n",
    "            target_dict = {}\n",
    "            i = 0\n",
    "            for output in ordered_outputs:\n",
    "                target_length = target_lengths[output]\n",
    "                target_dict[output] = flowy[:, i: i + target_length]\n",
    "                i += target_length\n",
    "\n",
    "            yield flowx, target_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HEIGHT = 137\n",
    "WIDTH = 236"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "histories = []\n",
    "for i in range(4):\n",
    "    train_df = pd.merge(pd.read_parquet(f'/kaggle/input/bengaliai-cv19/train_image_data_{i}.parquet'), train_df_, on='image_id').drop(['image_id'], axis=1)\n",
    "    \n",
    "    # Visualize few samples of current training dataset\n",
    "    fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16, 8))\n",
    "    count=0\n",
    "    for row in ax:\n",
    "        for col in row:\n",
    "            col.imshow(resize(train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1).iloc[[count]], need_progress_bar=False).values.reshape(-1).reshape(IMG_SIZE, IMG_SIZE).astype(np.float64))\n",
    "            count += 1\n",
    "    plt.show()\n",
    "    \n",
    "    X_train = train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1)\n",
    "    X_train = resize(X_train)/255\n",
    "    \n",
    "    # CNN takes images in shape `(batch_size, h, w, channels)`, so reshape the images\n",
    "    X_train = X_train.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)\n",
    "    \n",
    "    Y_train_root = pd.get_dummies(train_df['grapheme_root']).values\n",
    "    Y_train_vowel = pd.get_dummies(train_df['vowel_diacritic']).values\n",
    "    Y_train_consonant = pd.get_dummies(train_df['consonant_diacritic']).values\n",
    "\n",
    "    print(f'Training images: {X_train.shape}')\n",
    "    print(f'Training labels root: {Y_train_root.shape}')\n",
    "    print(f'Training labels vowel: {Y_train_vowel.shape}')\n",
    "    print(f'Training labels consonants: {Y_train_consonant.shape}')\n",
    "\n",
    "    # Divide the data into training and validation set\n",
    "    x_train, x_test, y_train_root, y_test_root, y_train_vowel, y_test_vowel, y_train_consonant, y_test_consonant = train_test_split(X_train, Y_train_root, Y_train_vowel, Y_train_consonant, test_size=0.08, random_state=71)\n",
    "    del train_df\n",
    "    del X_train\n",
    "    del Y_train_root, Y_train_vowel, Y_train_consonant\n",
    "\n",
    "    # Data augmentation for creating more training data\n",
    "    datagen = MultiOutputDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=8,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.15, # Randomly zoom image \n",
    "        width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "    # This will just calculate parameters required to augment the given data. This won't perform any augmentations\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit_generator(datagen.flow(x_train, {'dense_3': y_train_root, 'dense_4': y_train_vowel, 'dense_5': y_train_consonant}, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (x_test, [y_test_root, y_test_vowel, y_test_consonant]), \n",
    "                              steps_per_epoch=x_train.shape[0] // batch_size, \n",
    "                              callbacks=[learning_rate_reduction_root, learning_rate_reduction_vowel, learning_rate_reduction_consonant])\n",
    "\n",
    "    histories.append(history)\n",
    "    \n",
    "    # Delete to reduce memory usage\n",
    "    del x_train\n",
    "    del x_test\n",
    "    del y_train_root\n",
    "    del y_test_root\n",
    "    del y_train_vowel\n",
    "    del y_test_vowel\n",
    "    del y_train_consonant\n",
    "    del y_test_consonant\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preds_dict = {\n",
    "    'grapheme_root': [],\n",
    "    'vowel_diacritic': [],\n",
    "    'consonant_diacritic': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "components = ['consonant_diacritic', 'grapheme_root', 'vowel_diacritic']\n",
    "target=[] # model predictions placeholder\n",
    "row_id=[] # row_id place holder\n",
    "for i in range(4):\n",
    "    df_test_img = pd.read_parquet('/kaggle/input/bengaliai-cv19/test_image_data_{}.parquet'.format(i)) \n",
    "    df_test_img.set_index('image_id', inplace=True)\n",
    "\n",
    "    X_test = resize(df_test_img, need_progress_bar=False)/255\n",
    "    X_test = X_test.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    for i, p in enumerate(preds_dict):\n",
    "        preds_dict[p] = np.argmax(preds[i], axis=1)\n",
    "\n",
    "    for k,id in enumerate(df_test_img.index.values):  \n",
    "        for i,comp in enumerate(components):\n",
    "            id_sample=id+'_'+comp\n",
    "            row_id.append(id_sample)\n",
    "            target.append(preds_dict[comp][k])\n",
    "    del df_test_img\n",
    "    del X_test\n",
    "    gc.collect()\n",
    "\n",
    "submission_df3 = pd.DataFrame(\n",
    "    {\n",
    "        'row_id': row_id,\n",
    "        'target':target\n",
    "    },\n",
    "    columns = ['row_id','target'] \n",
    ")\n",
    "#df_sample.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth model(Grapheme fast.ai starter [inference]:0.9639)  \n",
    "ref:https://www.kaggle.com/iafoss/grapheme-fast-ai-starter-inference/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "import os\n",
    "#from mish_activation import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MishFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return x * torch.tanh(F.softplus(x))   # x * tanh(ln(1 + exp(x)))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_variables[0]\n",
    "        sigmoid = torch.sigmoid(x)\n",
    "        tanh_sp = torch.tanh(F.softplus(x)) \n",
    "        return grad_output * (tanh_sp + x * sigmoid * (1 - tanh_sp * tanh_sp))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return MishFunction.apply(x)\n",
    "\n",
    "def to_Mish(model):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, nn.ReLU):\n",
    "            setattr(model, child_name, Mish())\n",
    "        else:\n",
    "            to_Mish(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "SIZE = 128\n",
    "bs = 128\n",
    "stats = (0.0692, 0.2051)\n",
    "arch = models.densenet121\n",
    "MODEL = '../input/grapheme-fast-ai-starter-lb-0-964/model_0.pth'\n",
    "nworkers = 2\n",
    "\n",
    "TEST = ['/kaggle/input/bengaliai-cv19/test_image_data_0.parquet',\n",
    "        '/kaggle/input/bengaliai-cv19/test_image_data_1.parquet',\n",
    "        '/kaggle/input/bengaliai-cv19/test_image_data_2.parquet',\n",
    "        '/kaggle/input/bengaliai-cv19/test_image_data_3.parquet']\n",
    "\n",
    "LABELS = '../input/bengaliai-cv19/train.csv'\n",
    "\n",
    "df = pd.read_csv(LABELS)\n",
    "nunique = list(df.nunique())[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, nc, n, ps=0.5):\n",
    "        super().__init__()\n",
    "        layers = [AdaptiveConcatPool2d(), Mish(), Flatten()] + \\\n",
    "            bn_drop_lin(nc*2, 512, True, ps, Mish()) + \\\n",
    "            bn_drop_lin(512, n, True, ps)\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "        self._init_weight()\n",
    "        \n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1.0)\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "#change the first conv to accept 1 chanel input\n",
    "class Dnet_1ch(nn.Module):\n",
    "    def __init__(self, arch=arch, n=nunique, pre=True, ps=0.5):\n",
    "        super().__init__()\n",
    "        m = arch(True) if pre else arch()\n",
    "        \n",
    "        conv = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        w = (m.features.conv0.weight.sum(1)).unsqueeze(1)\n",
    "        conv.weight = nn.Parameter(w)\n",
    "        \n",
    "        self.layer0 = nn.Sequential(conv, m.features.norm0, nn.ReLU(inplace=True))\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False),\n",
    "            m.features.denseblock1)\n",
    "        self.layer2 = nn.Sequential(m.features.transition1,m.features.denseblock2)\n",
    "        self.layer3 = nn.Sequential(m.features.transition2,m.features.denseblock3)\n",
    "        self.layer4 = nn.Sequential(m.features.transition3,m.features.denseblock4,\n",
    "                                    m.features.norm5)\n",
    "        \n",
    "        nc = self.layer4[-1].weight.shape[0]\n",
    "        self.head1 = Head(nc,n[0])\n",
    "        self.head2 = Head(nc,n[1])\n",
    "        self.head3 = Head(nc,n[2])\n",
    "        #to_Mish(self.layer0), to_Mish(self.layer1), to_Mish(self.layer2)\n",
    "        #to_Mish(self.layer3), to_Mish(self.layer4)\n",
    "        \n",
    "    def forward(self, x):    \n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x1 = self.head1(x)\n",
    "        x2 = self.head2(x)\n",
    "        x3 = self.head3(x)\n",
    "        \n",
    "        return x1,x2,x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dnet_1ch(pre=False).cuda()\n",
    "model.load_state_dict(torch.load(MODEL, map_location=torch.device('cpu')));\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check https://www.kaggle.com/iafoss/image-preprocessing-128x128\n",
    "\n",
    "def bbox(img):\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "    return rmin, rmax, cmin, cmax\n",
    "\n",
    "def crop_resize(img0, size=SIZE, pad=16):\n",
    "    #crop a box around pixels large than the threshold \n",
    "    #some images contain line at the sides\n",
    "    ymin,ymax,xmin,xmax = bbox(img0[5:-5,5:-5] > 60)#80)\n",
    "    #cropping may cut too much, so we need to add it back\n",
    "    xmin = xmin - 13 if (xmin > 13) else 0\n",
    "    ymin = ymin - 10 if (ymin > 10) else 0\n",
    "    xmax = xmax + 13 if (xmax < WIDTH - 13) else WIDTH\n",
    "    ymax = ymax + 10 if (ymax < HEIGHT - 10) else HEIGHT\n",
    "    img = img0[ymin:ymax,xmin:xmax]\n",
    "    #remove lo intensity pixels as noise\n",
    "    img[img < 28] = 0\n",
    "    lx, ly = xmax-xmin,ymax-ymin\n",
    "    l = max(lx,ly) + pad\n",
    "    #make sure that the aspect ratio is kept in rescaling\n",
    "    img = np.pad(img, [((l-ly)//2,), ((l-lx)//2,)], mode='constant')\n",
    "    return cv2.resize(img,(size,size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphemeDataset(Dataset):\n",
    "    def __init__(self, fname):\n",
    "        self.df = pd.read_parquet(fname)\n",
    "        self.data = 255 - self.df.iloc[:, 1:].values.reshape(-1, HEIGHT, WIDTH).astype(np.uint8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.df.iloc[idx,0]\n",
    "        #normalize each image by its max val\n",
    "        img = (self.data[idx]*(255.0/self.data[idx].max())).astype(np.uint8)\n",
    "        img = crop_resize(img)\n",
    "        img = (img.astype(np.float32)/255.0 - stats[0])/stats[1]\n",
    "        return img, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9cc41fb1aa4efab547c82a2561994f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68cd1dd00b5b4dd8a8994cf04695754e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82575a9856b47cfa2ed431cf5926315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b343b9c21cfa4112a23d05e7f4303ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "row_id,target = [],[]\n",
    "for fname in TEST:\n",
    "    ds = GraphemeDataset(fname)\n",
    "    dl = DataLoader(ds, batch_size=bs, num_workers=nworkers, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for x,y in tqdm(dl):\n",
    "            x = x.unsqueeze(1).cuda()\n",
    "            p1,p2,p3 = model(x)\n",
    "            p1 = p1.argmax(-1).view(-1).cpu()\n",
    "            p2 = p2.argmax(-1).view(-1).cpu()\n",
    "            p3 = p3.argmax(-1).view(-1).cpu()\n",
    "            for idx,name in enumerate(y):\n",
    "                row_id += [f'{name}_grapheme_root',f'{name}_vowel_diacritic',\n",
    "                           f'{name}_consonant_diacritic']\n",
    "                target += [p1[idx].item(),p2[idx].item(),p3[idx].item()]\n",
    "                \n",
    "submission_df3 = pd.DataFrame({'row_id': row_id, 'target': target})\n",
    "#sub_df.to_csv('submission.csv', index=False)\n",
    "#sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            Variable Name|    Memory|\n",
      " ------------------------------------ \n",
      "|                       df|  38722817|\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n",
    "print(\" ------------------------------------ \")\n",
    "for var_name in dir():\n",
    "    if not var_name.startswith(\"_\") and sys.getsizeof(eval(var_name)) > 10000: #over 10000 byte data pick up\n",
    "        print(\"{}{: >25}{}{: >10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name)),'|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fifth model([Keras]:Grapheme GridMask+AugMix In EfficientNet: 0.9554)  \n",
    "ref:https://www.kaggle.com/ipythonx/keras-grapheme-gridmask-augmix-in-efficientnet/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#sklearns \n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# keras modules \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.optimizers import Adam, Nadam, SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, Conv2D, GlobalMaxPooling2D, concatenate\n",
    "from keras.layers import (MaxPooling2D, Input, Average, Activation, MaxPool2D,\n",
    "                          Flatten, LeakyReLU, BatchNormalization)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from keras import utils as np_utils\n",
    "from keras.callbacks import (Callback, ModelCheckpoint,\n",
    "                                        LearningRateScheduler,EarlyStopping, \n",
    "                                        ReduceLROnPlateau,CSVLogger)\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#declare some parameter\n",
    "SEED = 2020\n",
    "epoch = 100\n",
    "batch_size = 12 \n",
    "dim = (128, 128)\n",
    "SIZE = 128\n",
    "stats = (0.0692, 0.2051)\n",
    "HEIGHT = 137 \n",
    "WIDTH = 236\n",
    "\n",
    "def seed_all(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "#seed all\n",
    "seed_all(SEED)\n",
    "\n",
    "#load files\n",
    "im_path = '../input/grapheme-imgs-128x128/'\n",
    "train = pd.read_csv('../input/bengaliai-cv19/train.csv')\n",
    "test = pd.read_csv('../input/bengaliai-cv19/test.csv')\n",
    "train['filename'] = train.image_id.apply(lambda filename: im_path + filename + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grid Mask\n",
    "# code takesn from https://www.kaggle.com/haqishen/gridmask\n",
    "\n",
    "import albumentations\n",
    "from albumentations.core.transforms_interface import DualTransform, ImageOnlyTransform\n",
    "from albumentations.augmentations import functional as F\n",
    "\n",
    "class GridMask(DualTransform):\n",
    "    \"\"\"GridMask augmentation for image classification and object detection.\n",
    "\n",
    "    Args:\n",
    "        num_grid (int): number of grid in a row or column.\n",
    "        fill_value (int, float, lisf of int, list of float): value for dropped pixels.\n",
    "        rotate ((int, int) or int): range from which a random angle is picked. If rotate is a single int\n",
    "            an angle is picked from (-rotate, rotate). Default: (-90, 90)\n",
    "        mode (int):\n",
    "            0 - cropout a quarter of the square of each grid (left top)\n",
    "            1 - reserve a quarter of the square of each grid (left top)\n",
    "            2 - cropout 2 quarter of the square of each grid (left top & right bottom)\n",
    "\n",
    "    Targets:\n",
    "        image, mask\n",
    "\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "\n",
    "    Reference:\n",
    "    |  https://arxiv.org/abs/2001.04086\n",
    "    |  https://github.com/akuxcw/GridMask\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_grid=3, fill_value=0, rotate=0, mode=0, always_apply=False, p=0.5):\n",
    "        super(GridMask, self).__init__(always_apply, p)\n",
    "        if isinstance(num_grid, int):\n",
    "            num_grid = (num_grid, num_grid)\n",
    "        if isinstance(rotate, int):\n",
    "            rotate = (-rotate, rotate)\n",
    "        self.num_grid = num_grid\n",
    "        self.fill_value = fill_value\n",
    "        self.rotate = rotate\n",
    "        self.mode = mode\n",
    "        self.masks = None\n",
    "        self.rand_h_max = []\n",
    "        self.rand_w_max = []\n",
    "\n",
    "    def init_masks(self, height, width):\n",
    "        if self.masks is None:\n",
    "            self.masks = []\n",
    "            n_masks = self.num_grid[1] - self.num_grid[0] + 1\n",
    "            for n, n_g in enumerate(range(self.num_grid[0], self.num_grid[1] + 1, 1)):\n",
    "                grid_h = height / n_g\n",
    "                grid_w = width / n_g\n",
    "                this_mask = np.ones((int((n_g + 1) * grid_h), int((n_g + 1) * grid_w))).astype(np.uint8)\n",
    "                for i in range(n_g + 1):\n",
    "                    for j in range(n_g + 1):\n",
    "                        this_mask[\n",
    "                             int(i * grid_h) : int(i * grid_h + grid_h / 2),\n",
    "                             int(j * grid_w) : int(j * grid_w + grid_w / 2)\n",
    "                        ] = self.fill_value\n",
    "                        if self.mode == 2:\n",
    "                            this_mask[\n",
    "                                 int(i * grid_h + grid_h / 2) : int(i * grid_h + grid_h),\n",
    "                                 int(j * grid_w + grid_w / 2) : int(j * grid_w + grid_w)\n",
    "                            ] = self.fill_value\n",
    "                \n",
    "                if self.mode == 1:\n",
    "                    this_mask = 1 - this_mask\n",
    "\n",
    "                self.masks.append(this_mask)\n",
    "                self.rand_h_max.append(grid_h)\n",
    "                self.rand_w_max.append(grid_w)\n",
    "\n",
    "    def apply(self, image, mask, rand_h, rand_w, angle, **params):\n",
    "        h, w = image.shape[:2]\n",
    "        mask = F.rotate(mask, angle) if self.rotate[1] > 0 else mask\n",
    "        mask = mask[:,:,np.newaxis] if image.ndim == 3 else mask\n",
    "        image *= mask[rand_h:rand_h+h, rand_w:rand_w+w].astype(image.dtype)\n",
    "        return image\n",
    "\n",
    "    def get_params_dependent_on_targets(self, params):\n",
    "        img = params['image']\n",
    "        height, width = img.shape[:2]\n",
    "        self.init_masks(height, width)\n",
    "\n",
    "        mid = np.random.randint(len(self.masks))\n",
    "        mask = self.masks[mid]\n",
    "        rand_h = np.random.randint(self.rand_h_max[mid])\n",
    "        rand_w = np.random.randint(self.rand_w_max[mid])\n",
    "        angle = np.random.randint(self.rotate[0], self.rotate[1]) if self.rotate[1] > 0 else 0\n",
    "\n",
    "        return {'mask': mask, 'rand_h': rand_h, 'rand_w': rand_w, 'angle': angle}\n",
    "\n",
    "    @property\n",
    "    def targets_as_params(self):\n",
    "        return ['image']\n",
    "\n",
    "    def get_transform_init_args_names(self):\n",
    "        return ('num_grid', 'fill_value', 'rotate', 'mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmix : https://github.com/google-research/augmix\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "import numpy as np\n",
    "\n",
    "def int_parameter(level, maxval):\n",
    "    \"\"\"Helper function to scale `val` between 0 and maxval .\n",
    "    Args:\n",
    "    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n",
    "    maxval: Maximum value that the operation can have. This will be scaled to\n",
    "      level/PARAMETER_MAX.\n",
    "    Returns:\n",
    "    An int that results from scaling `maxval` according to `level`.\n",
    "    \"\"\"\n",
    "    return int(level * maxval / 10)\n",
    "\n",
    "\n",
    "def float_parameter(level, maxval):\n",
    "    \"\"\"Helper function to scale `val` between 0 and maxval.\n",
    "    Args:\n",
    "    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n",
    "    maxval: Maximum value that the operation can have. This will be scaled to\n",
    "      level/PARAMETER_MAX.\n",
    "    Returns:\n",
    "    A float that results from scaling `maxval` according to `level`.\n",
    "    \"\"\"\n",
    "    return float(level) * maxval / 10.\n",
    "\n",
    "def sample_level(n):\n",
    "    return np.random.uniform(low=0.1, high=n)\n",
    "\n",
    "def autocontrast(pil_img, _):\n",
    "    return ImageOps.autocontrast(pil_img)\n",
    "\n",
    "def equalize(pil_img, _):\n",
    "    return ImageOps.equalize(pil_img)\n",
    "\n",
    "def posterize(pil_img, level):\n",
    "    level = int_parameter(sample_level(level), 4)\n",
    "    return ImageOps.posterize(pil_img, 4 - level)\n",
    "\n",
    "def rotate(pil_img, level):\n",
    "    degrees = int_parameter(sample_level(level), 30)\n",
    "    if np.random.uniform() > 0.5:\n",
    "        degrees = -degrees\n",
    "    return pil_img.rotate(degrees, resample=Image.BILINEAR)\n",
    "\n",
    "def solarize(pil_img, level):\n",
    "    level = int_parameter(sample_level(level), 256)\n",
    "    return ImageOps.solarize(pil_img, 256 - level)\n",
    "\n",
    "def shear_x(pil_img, level):\n",
    "    level = float_parameter(sample_level(level), 0.3)\n",
    "    if np.random.uniform() > 0.5:\n",
    "        level = -level\n",
    "    return pil_img.transform((SIZE, SIZE),\n",
    "                           Image.AFFINE, (1, level, 0, 0, 1, 0),\n",
    "                           resample=Image.BILINEAR)\n",
    "\n",
    "def shear_y(pil_img, level):\n",
    "    level = float_parameter(sample_level(level), 0.3)\n",
    "    if np.random.uniform() > 0.5:\n",
    "        level = -level\n",
    "    return pil_img.transform((SIZE, SIZE),\n",
    "                           Image.AFFINE, (1, 0, 0, level, 1, 0),\n",
    "                           resample=Image.BILINEAR)\n",
    "\n",
    "def translate_x(pil_img, level):\n",
    "    level = int_parameter(sample_level(level), SIZE / 3)\n",
    "    if np.random.random() > 0.5:\n",
    "        level = -level\n",
    "    return pil_img.transform((SIZE, SIZE),\n",
    "                           Image.AFFINE, (1, 0, level, 0, 1, 0),\n",
    "                           resample=Image.BILINEAR)\n",
    "\n",
    "\n",
    "def translate_y(pil_img, level):\n",
    "    level = int_parameter(sample_level(level), SIZE / 3)\n",
    "    if np.random.random() > 0.5:\n",
    "        level = -level\n",
    "    return pil_img.transform((SIZE, SIZE),\n",
    "                           Image.AFFINE, (1, 0, 0, 0, 1, level),\n",
    "                           resample=Image.BILINEAR)\n",
    "\n",
    "augmentations = [\n",
    "    autocontrast, equalize, posterize, rotate, solarize, shear_x, shear_y,\n",
    "    translate_x, translate_y\n",
    "]\n",
    "\n",
    "# taken from https://www.kaggle.com/iafoss/image-preprocessing-128x128\n",
    "MEAN = [ 0.06922848809290576,  0.06922848809290576,  0.06922848809290576]\n",
    "STD = [ 0.20515700083327537,  0.20515700083327537,  0.20515700083327537]\n",
    "\n",
    "def normalize(image):\n",
    "    \"\"\"Normalize input image channel-wise to zero mean and unit variance.\"\"\"\n",
    "    image = image.transpose(2, 0, 1)  # Switch to channel-first\n",
    "    mean, std = np.array(MEAN), np.array(STD)\n",
    "    image = (image - mean[:, None, None]) / std[:, None, None]\n",
    "    return image.transpose(1, 2, 0)\n",
    "\n",
    "\n",
    "def apply_op(image, op, severity):\n",
    "    image = np.clip(image * 255., 0, 255).astype(np.uint8)\n",
    "    pil_img = Image.fromarray(image)  # Convert to PIL.Image\n",
    "    pil_img = op(pil_img, severity)\n",
    "    return np.asarray(pil_img) / 255.\n",
    "\n",
    "\n",
    "def augment_and_mix(image, severity=1, width=3, depth=1, alpha=1.):\n",
    "    \"\"\"Perform AugMix augmentations and compute mixture.\n",
    "    Args:\n",
    "    image: Raw input image as float32 np.ndarray of shape (h, w, c)\n",
    "    severity: Severity of underlying augmentation operators (between 1 to 10).\n",
    "    width: Width of augmentation chain\n",
    "    depth: Depth of augmentation chain. -1 enables stochastic depth uniformly\n",
    "      from [1, 3]\n",
    "    alpha: Probability coefficient for Beta and Dirichlet distributions.\n",
    "    Returns:\n",
    "    mixed: Augmented and mixed image.\n",
    "  \"\"\"\n",
    "    ws = np.float32(\n",
    "      np.random.dirichlet([alpha] * width))\n",
    "    m = np.float32(np.random.beta(alpha, alpha))\n",
    "\n",
    "    mix = np.zeros_like(image)\n",
    "    for i in range(width):\n",
    "        image_aug = image.copy()\n",
    "        depth = depth if depth > 0 else np.random.randint(1, 4)\n",
    "        \n",
    "        for _ in range(depth):\n",
    "            op = np.random.choice(augmentations)\n",
    "            image_aug = apply_op(image_aug, op, severity)\n",
    "        mix = np.add(mix, ws[i] * normalize(image_aug), out=mix, \n",
    "                     casting=\"unsafe\")\n",
    "\n",
    "    mixed = (1 - m) * normalize(image) + m * mix\n",
    "    return mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphemeGenerator(Sequence):\n",
    "    def __init__(self, data, batch_size, dim, shuffle=True, transform=None):\n",
    "        self._data = data\n",
    "        self._label_1 = pd.get_dummies(self._data['grapheme_root'], \n",
    "                                       columns = ['grapheme_root'])\n",
    "        self._label_2 = pd.get_dummies(self._data['vowel_diacritic'], \n",
    "                                       columns = ['vowel_diacritic'])\n",
    "        self._label_3 = pd.get_dummies(self._data['consonant_diacritic'], \n",
    "                                       columns = ['consonant_diacritic'])\n",
    "        self._list_idx = data.index.values\n",
    "        self._batch_size = batch_size\n",
    "        self._dim = dim\n",
    "        self._shuffle = shuffle\n",
    "        self.transform = transform\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self._data)/self._batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_idx = self._indices[index*self._batch_size:(index+1)*self._batch_size]\n",
    "        _idx = [self._list_idx[k] for k in batch_idx]\n",
    "\n",
    "        Data     = np.empty((self._batch_size, *self._dim, 1))\n",
    "        Target_1 = np.empty((self._batch_size, 168), dtype = int)\n",
    "        Target_2 = np.empty((self._batch_size, 11 ), dtype = int)\n",
    "        Target_3 = np.empty((self._batch_size,  7 ), dtype = int)\n",
    "        \n",
    "        for i, k in enumerate(_idx):\n",
    "            # load the image file using cv2\n",
    "            image = cv2.imread(im_path + self._data['image_id'][k] + '.png')\n",
    "            image = cv2.resize(image,  self._dim) \n",
    "            \n",
    "            if self.transform is not None:\n",
    "                if np.random.rand() > 0.5:\n",
    "                    # albumentation : grid mask\n",
    "                    res = self.transform(image=image)\n",
    "                    image = res['image']\n",
    "                else:\n",
    "                    # augmix augmentation\n",
    "                    image = augment_and_mix(image)\n",
    "            \n",
    "            # gray scaling \n",
    "            gray = lambda rgb : np.dot(rgb[... , :3] , [0.299 , 0.587, 0.114]) \n",
    "            image = gray(image)  \n",
    "            \n",
    "            # expand the axises \n",
    "            image = image[:, :, np.newaxis]\n",
    "            Data[i,:, :, :] =  image\n",
    "        \n",
    "            Target_1[i,:] = self._label_1.loc[k, :].values\n",
    "            Target_2[i,:] = self._label_2.loc[k, :].values\n",
    "            Target_3[i,:] = self._label_3.loc[k, :].values\n",
    "            \n",
    "        return Data, [Target_1, Target_2, Target_3]\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self._indices = np.arange(len(self._list_idx))\n",
    "        if self._shuffle:\n",
    "            np.random.shuffle(self._indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install ../input/efficientnet-keras-source-code/repository/qubvel-efficientnet-c993591"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import efficientnet.keras as efn \n",
    "\n",
    "#we will be using EfficientNetB0\n",
    "wg = '../input/efficientnet-keras-weights-b0b5/efficientnet-b0_imagenet_1000_notop.h5'\n",
    "efnet = efn.EfficientNetB0(weights=wg, \n",
    "                           include_top = False, \n",
    "                           input_shape=(128, 128, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine import Layer\n",
    "\n",
    "class GroupNormalization(Layer):\n",
    "    \"\"\"Group normalization layer\n",
    "    Group Normalization divides the channels into groups and computes within each group\n",
    "    the mean and variance for normalization. GN's computation is independent of batch sizes,\n",
    "    and its accuracy is stable in a wide range of batch sizes\n",
    "    # Arguments\n",
    "        groups: Integer, the number of groups for Group Normalization.\n",
    "        axis: Integer, the axis that should be normalized\n",
    "            (typically the features axis).\n",
    "            For instance, after a `Conv2D` layer with\n",
    "            `data_format=\"channels_first\"`,\n",
    "            set `axis=1` in `BatchNormalization`.\n",
    "        epsilon: Small float added to variance to avoid dividing by zero.\n",
    "        center: If True, add offset of `beta` to normalized tensor.\n",
    "            If False, `beta` is ignored.\n",
    "        scale: If True, multiply by `gamma`.\n",
    "            If False, `gamma` is not used.\n",
    "            When the next layer is linear (also e.g. `nn.relu`),\n",
    "            this can be disabled since the scaling\n",
    "            will be done by the next layer.\n",
    "        beta_initializer: Initializer for the beta weight.\n",
    "        gamma_initializer: Initializer for the gamma weight.\n",
    "        beta_regularizer: Optional regularizer for the beta weight.\n",
    "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "        beta_constraint: Optional constraint for the beta weight.\n",
    "        gamma_constraint: Optional constraint for the gamma weight.\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    # References\n",
    "        - [Group Normalization](https://arxiv.org/abs/1803.08494)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 groups=32,\n",
    "                 axis=-1,\n",
    "                 epsilon=1e-5,\n",
    "                 center=True,\n",
    "                 scale=True,\n",
    "                 beta_initializer='zeros',\n",
    "                 gamma_initializer='ones',\n",
    "                 beta_regularizer=None,\n",
    "                 gamma_regularizer=None,\n",
    "                 beta_constraint=None,\n",
    "                 gamma_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(GroupNormalization, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.groups = groups\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.beta_initializer = initializers.get(beta_initializer)\n",
    "        self.gamma_initializer = initializers.get(gamma_initializer)\n",
    "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "        self.beta_constraint = constraints.get(beta_constraint)\n",
    "        self.gamma_constraint = constraints.get(gamma_constraint)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        dim = input_shape[self.axis]\n",
    "\n",
    "        if dim is None:\n",
    "            raise ValueError('Axis ' + str(self.axis) + ' of '\n",
    "                             'input tensor should have a defined dimension '\n",
    "                             'but the layer received an input with shape ' +\n",
    "                             str(input_shape) + '.')\n",
    "\n",
    "        if dim < self.groups:\n",
    "            raise ValueError('Number of groups (' + str(self.groups) + ') cannot be '\n",
    "                             'more than the number of channels (' +\n",
    "                             str(dim) + ').')\n",
    "\n",
    "        if dim % self.groups != 0:\n",
    "            raise ValueError('Number of groups (' + str(self.groups) + ') must be a '\n",
    "                             'multiple of the number of channels (' +\n",
    "                             str(dim) + ').')\n",
    "\n",
    "        self.input_spec = InputSpec(ndim=len(input_shape),\n",
    "                                    axes={self.axis: dim})\n",
    "        shape = (dim,)\n",
    "\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(shape=shape,\n",
    "                                         name='gamma',\n",
    "                                         initializer=self.gamma_initializer,\n",
    "                                         regularizer=self.gamma_regularizer,\n",
    "                                         constraint=self.gamma_constraint)\n",
    "        else:\n",
    "            self.gamma = None\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(shape=shape,\n",
    "                                        name='beta',\n",
    "                                        initializer=self.beta_initializer,\n",
    "                                        regularizer=self.beta_regularizer,\n",
    "                                        constraint=self.beta_constraint)\n",
    "        else:\n",
    "            self.beta = None\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        tensor_input_shape = K.shape(inputs)\n",
    "\n",
    "        # Prepare broadcasting shape.\n",
    "        reduction_axes = list(range(len(input_shape)))\n",
    "        del reduction_axes[self.axis]\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "        broadcast_shape.insert(1, self.groups)\n",
    "\n",
    "        reshape_group_shape = K.shape(inputs)\n",
    "        group_axes = [reshape_group_shape[i] for i in range(len(input_shape))]\n",
    "        group_axes[self.axis] = input_shape[self.axis] // self.groups\n",
    "        group_axes.insert(1, self.groups)\n",
    "\n",
    "        # reshape inputs to new group shape\n",
    "        group_shape = [group_axes[0], self.groups] + group_axes[2:]\n",
    "        group_shape = K.stack(group_shape)\n",
    "        inputs = K.reshape(inputs, group_shape)\n",
    "\n",
    "        group_reduction_axes = list(range(len(group_axes)))\n",
    "        group_reduction_axes = group_reduction_axes[2:]\n",
    "\n",
    "        mean = K.mean(inputs, axis=group_reduction_axes, keepdims=True)\n",
    "        variance = K.var(inputs, axis=group_reduction_axes, keepdims=True)\n",
    "\n",
    "        inputs = (inputs - mean) / (K.sqrt(variance + self.epsilon))\n",
    "\n",
    "        # prepare broadcast shape\n",
    "        inputs = K.reshape(inputs, group_shape)\n",
    "        outputs = inputs\n",
    "\n",
    "        # In this case we must explicitly broadcast all parameters.\n",
    "        if self.scale:\n",
    "            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
    "            outputs = outputs * broadcast_gamma\n",
    "\n",
    "        if self.center:\n",
    "            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "            outputs = outputs + broadcast_beta\n",
    "\n",
    "        outputs = K.reshape(outputs, tensor_input_shape)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'groups': self.groups,\n",
    "            'axis': self.axis,\n",
    "            'epsilon': self.epsilon,\n",
    "            'center': self.center,\n",
    "            'scale': self.scale,\n",
    "            'beta_initializer': initializers.serialize(self.beta_initializer),\n",
    "            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n",
    "            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n",
    "            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
    "            'beta_constraint': constraints.serialize(self.beta_constraint),\n",
    "            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n",
    "        }\n",
    "        base_config = super(GroupNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Replace all Batch Normalization layers by Group Normalization layers\n",
    "for i, layer in enumerate(efnet.layers):\n",
    "    if \"batch_normalization\" in layer.name:\n",
    "        efnet.layers[i] = GroupNormalization(groups=32, \n",
    "                                             axis=-1, \n",
    "                                             epsilon=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def create_model(input_dim, output_dim, base_model):\n",
    "    \n",
    "    input_tensor = Input(input_dim)\n",
    "    \n",
    "    x = Conv2D(3, (3, 3), padding='same',  kernel_initializer='he_uniform', \n",
    "               bias_initializer='zeros')(input_tensor)\n",
    "    curr_output = base_model(x)\n",
    "    curr_output = GlobalAveragePooling2D()(curr_output)\n",
    "    curr_output = Dropout(0.5)(curr_output)\n",
    "    curr_output = Dense(512, activation='elu')(curr_output)\n",
    "    curr_output = Dropout(0.5)(curr_output)\n",
    "        \n",
    "    oputput1 = Dense(168,  activation='softmax', name='gra') (curr_output)\n",
    "    oputput2 = Dense(11,  activation='softmax', name='vow') (curr_output)\n",
    "    oputput3 = Dense(7,  activation='softmax', name='cons') (curr_output)\n",
    "    output_tensor = [oputput1, oputput2, oputput3]\n",
    "\n",
    "    model = Model(input_tensor, output_tensor)\n",
    "    \n",
    "    return model\n",
    "\n",
    "#building the complete model\n",
    "model = create_model(input_dim=(128,128,1), output_dim=(168,11,7), base_model = efnet)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code: https://github.com/titu1994/keras-adabound   \n",
    "class AdaBound(keras.optimizers.Optimizer):\n",
    "    \"\"\"AdaBound optimizer.\n",
    "    Default parameters follow those provided in the original paper.\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        final_lr: float >= 0. Final learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        gamma: float >= 0. Convergence speed of the bound function.\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        weight_decay: Weight decay weight.\n",
    "        amsbound: boolean. Whether to apply the AMSBound variant of this\n",
    "            algorithm.\n",
    "    # References\n",
    "        - [Adaptive Gradient Methods with Dynamic Bound of Learning Rate]\n",
    "          (https://openreview.net/forum?id=Bkg3g2R9FX)\n",
    "        - [Adam - A Method for Stochastic Optimization]\n",
    "          (https://arxiv.org/abs/1412.6980v8)\n",
    "        - [On the Convergence of Adam and Beyond]\n",
    "          (https://openreview.net/forum?id=ryQu7f-RZ)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, final_lr=0.1, beta_1=0.9, beta_2=0.999, gamma=1e-3,\n",
    "                 epsilon=None, decay=0., amsbound=False, weight_decay=0.0, **kwargs):\n",
    "        super(AdaBound, self).__init__(**kwargs)\n",
    "\n",
    "        if not 0. <= gamma <= 1.:\n",
    "            raise ValueError(\"Invalid `gamma` parameter. Must lie in [0, 1] range.\")\n",
    "\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.learning_rate = K.variable(learning_rate, name='learning_rate')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "\n",
    "        self.final_lr = final_lr\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.amsbound = amsbound\n",
    "\n",
    "        self.weight_decay = float(weight_decay)\n",
    "        self.base_lr = float(learning_rate)\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        learning_rate = self.learning_rate\n",
    "        if self.initial_decay > 0:\n",
    "            learning_rate = learning_rate * (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                      K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "\n",
    "        # Applies bounds on actual learning rate\n",
    "        step_size = learning_rate * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                          (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        final_lr = self.final_lr * learning_rate / self.base_lr\n",
    "        lower_bound = final_lr * (1. - 1. / (self.gamma * t + 1.))\n",
    "        upper_bound = final_lr * (1. + 1. / (self.gamma * t))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        if self.amsbound:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        else:\n",
    "            vhats = [K.zeros(1) for _ in params]\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "            # apply weight decay\n",
    "            if self.weight_decay != 0.:\n",
    "                g += self.weight_decay * K.stop_gradient(p)\n",
    "\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "\n",
    "            if self.amsbound:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                denom = (K.sqrt(vhat_t) + self.epsilon)\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                denom = (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            # Compute the bounds\n",
    "            step_size_p = step_size * K.ones_like(denom)\n",
    "            step_size_p_bound = step_size_p / denom\n",
    "            bounded_lr_t = m_t * K.minimum(K.maximum(step_size_p_bound,\n",
    "                                                     lower_bound), upper_bound)\n",
    "\n",
    "            p_t = p - bounded_lr_t\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'learning_rate': float(K.get_value(self.learning_rate)),\n",
    "                  'final_lr': float(self.final_lr),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'gamma': float(self.gamma),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon,\n",
    "                  'weight_decay': self.weight_decay,\n",
    "                  'amsbound': self.amsbound}\n",
    "        base_config = super(AdaBound, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras import backend as K\n",
    "\n",
    "#compiling    \n",
    "model.compile(\n",
    "    \n",
    "    optimizer = AdaBound(learning_rate=0.0001,\n",
    "                    final_lr=.1,\n",
    "                    gamma=1e-3,\n",
    "                    weight_decay=5e-4,\n",
    "                    amsbound=False), \n",
    "    \n",
    "    loss = {'gra' : 'categorical_crossentropy', \n",
    "            'vow' : 'categorical_crossentropy', \n",
    "            'cons': 'categorical_crossentropy'},\n",
    "    \n",
    "    loss_weights = {'gra' : 1.0,\n",
    "                    'vow' : 1.0,\n",
    "                    'cons': 1.0},\n",
    "    \n",
    "    metrics={'gra' : 'accuracy', \n",
    "             'vow' : 'accuracy', \n",
    "             'cons': 'accuracy'}\n",
    ")\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-259b0a33d612>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# for way one - data generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m train_labels, val_labels = train_test_split(train, test_size = 0.20, \n\u001b[0m\u001b[1;32m      8\u001b[0m                                             random_state = SEED)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# grid mask augmentation\n",
    "transforms_train = albumentations.Compose([\n",
    "    GridMask(num_grid=3, rotate=15, p=1),\n",
    "])\n",
    "\n",
    "# for way one - data generator\n",
    "train_labels, val_labels = train_test_split(train, test_size = 0.20, \n",
    "                                            random_state = SEED)\n",
    "\n",
    "# training generator\n",
    "train_generator = GraphemeGenerator(train_labels, batch_size, dim, \n",
    "                                shuffle = True, transform=transforms_train)\n",
    "\n",
    "# validation generator: no shuffle , not augmentation\n",
    "val_generator = GraphemeGenerator(val_labels, batch_size, dim, \n",
    "                              shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.callbacks import (ModelCheckpoint, LearningRateScheduler,\n",
    "                             EarlyStopping, ReduceLROnPlateau, TensorBoard, CSVLogger)\n",
    "\n",
    "#some call back function; feel free to add more for experiment\n",
    "def Call_Back():\n",
    "    # model check point\n",
    "    checkpoint = ModelCheckpoint('E0Train_B0.h5', \n",
    "                                 monitor = 'val_loss', \n",
    "                                 verbose = 0, save_best_only=True, \n",
    "                                 mode = 'min',\n",
    "                                 save_weights_only = True)\n",
    "    \n",
    "    #lr_scheduler = LearningRateScheduler(schedule=lambda epoch: 0.001 * (0.9 ** epoch))\n",
    "    csv_logger = CSVLogger('E0.csv')\n",
    "    early = EarlyStopping(monitor='val_loss', \n",
    "                          mode='min', patience=5)\n",
    "    \n",
    "    return [checkpoint, csv_logger, early]\n",
    "\n",
    "#epoch size \n",
    "epochs = 60 # increase the number, ex.: 100/200\n",
    "training = False # setting it true for training the model\n",
    "\n",
    "#calling all callbacks \n",
    "callbacks = Call_Back()\n",
    "\n",
    "if training:\n",
    "    # acatual training (fitting)\n",
    "    train_history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=int(len(train_labels)/batch_size), \n",
    "        validation_data=val_generator,\n",
    "        validation_steps = int(len(val_labels)/batch_size),\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "else: \n",
    "    model.load_weights('../input/efficientb/E0Train_B0.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log_file = \"../input/efficientb/E0.csv\"\n",
    "history = pd.read_csv(log_file) \n",
    "\n",
    "#find the lowest validation loss score\n",
    "print(history.loc[history['val_loss'].idxmin()])\n",
    "history.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-c4a95d46fb14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mplot_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'log_file' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas \n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "def plot_log(filename, show=True):\n",
    "\n",
    "    data = pandas.read_csv(filename)\n",
    "\n",
    "    fig = plt.figure(figsize=(8,10))\n",
    "    fig.subplots_adjust(top=0.95, bottom=0.05, right=0.95)\n",
    "    fig.add_subplot(211)\n",
    "    \n",
    "    for key in data.keys():\n",
    "        if key.find('loss') >= 0:  # training loss\n",
    "            plt.plot(data['epoch'].values, data[key].values, label=key)\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validtion Loss')\n",
    "\n",
    "    fig.add_subplot(212)\n",
    "    for key in data.keys():\n",
    "        if key.find('acc') >= 0:  # acc\n",
    "            plt.plot(data['epoch'].values, data[key].values, label=key)\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "plot_log(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def bbox(img):\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "    return rmin, rmax, cmin, cmax\n",
    "\n",
    "def crop_resize(img0, size=SIZE, pad=16):\n",
    "    #crop a box around pixels large than the threshold \n",
    "    #some images contain line at the sides\n",
    "    ymin,ymax,xmin,xmax = bbox(img0[5:-5,5:-5] > 60)#80)\n",
    "    #cropping may cut too much, so we need to add it back\n",
    "    xmin = xmin - 13 if (xmin > 13) else 0\n",
    "    ymin = ymin - 10 if (ymin > 10) else 0\n",
    "    xmax = xmax + 13 if (xmax < WIDTH - 13) else WIDTH\n",
    "    ymax = ymax + 10 if (ymax < HEIGHT - 10) else HEIGHT\n",
    "    img = img0[ymin:ymax,xmin:xmax]\n",
    "    #remove lo intensity pixels as noise\n",
    "    img[img < 28] = 0\n",
    "    lx, ly = xmax-xmin,ymax-ymin\n",
    "    l = max(lx,ly) + pad\n",
    "    #make sure that the aspect ratio is kept in rescaling\n",
    "    img = np.pad(img, [((l-ly)//2,), ((l-lx)//2,)], mode='constant')\n",
    "    \n",
    "    return cv2.resize(img,(size,size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def test_batch_generator(df, batch_size):\n",
    "    num_imgs = len(df)\n",
    "\n",
    "    for batch_start in range(0, num_imgs, batch_size):\n",
    "        curr_batch_size = min(num_imgs, batch_start + batch_size) - batch_start\n",
    "        idx = np.arange(batch_start, batch_start + curr_batch_size)\n",
    "\n",
    "        names_batch = df.iloc[idx, 0].values\n",
    "        imgs_batch = 255 - df.iloc[idx, 1:].values.reshape(-1, HEIGHT, WIDTH).astype(np.uint8)\n",
    "        X_batch = np.zeros((curr_batch_size, SIZE, SIZE, 1))\n",
    "        \n",
    "        for j in range(curr_batch_size):\n",
    "            img = (imgs_batch[j,]*(255.0/imgs_batch[j,].max())).astype(np.uint8)\n",
    "            img = crop_resize(img, size=SIZE)\n",
    "            img = img[:, :, np.newaxis]\n",
    "            X_batch[j,] = img\n",
    "\n",
    "        yield X_batch, names_batch\n",
    "\n",
    "\n",
    "#load the parquet files \n",
    "TEST = [\n",
    "    \"../input/bengaliai-cv19/test_image_data_0.parquet\",\n",
    "    \"../input/bengaliai-cv19/test_image_data_1.parquet\",\n",
    "    \"../input/bengaliai-cv19/test_image_data_2.parquet\",\n",
    "    \"../input/bengaliai-cv19/test_image_data_3.parquet\",\n",
    "]\n",
    "\n",
    "#placeholders \n",
    "row_id = []\n",
    "target = []\n",
    "\n",
    "#iterative over the test sets\n",
    "for fname in tqdm(TEST):\n",
    "    test_ = pd.read_parquet(fname)\n",
    "    test_gen = test_batch_generator(test_, batch_size=batch_size)\n",
    "\n",
    "    for batch_x, batch_name in test_gen:\n",
    "        batch_predict = model.predict(batch_x)\n",
    "        for idx, name in enumerate(batch_name):\n",
    "            row_id += [\n",
    "                f\"{name}_consonant_diacritic\",\n",
    "                f\"{name}_grapheme_root\",\n",
    "                f\"{name}_vowel_diacritic\",\n",
    "            ]\n",
    "            target += [\n",
    "                np.argmax(batch_predict[2], axis=1)[idx],\n",
    "                np.argmax(batch_predict[0], axis=1)[idx],\n",
    "                np.argmax(batch_predict[1], axis=1)[idx],\n",
    "            ]\n",
    "\n",
    "    del test_\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "submission_df3 = pd.DataFrame(\n",
    "    {\n",
    "        'row_id': row_id,\n",
    "        'target':target\n",
    "    },\n",
    "    columns = ['row_id','target'] \n",
    ")\n",
    "\n",
    "#df_sample.to_csv('submission.csv',index=False)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = pd.merge(submission_df, submission_df2, on=\"row_id\")\n",
    "merge = pd.merge(merge, submission_df3, on=\"row_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target_x</th>\n",
       "      <th>target_y</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_0_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_0_grapheme_root</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_0_vowel_diacritic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_1_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_1_grapheme_root</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       row_id  target_x  target_y  target\n",
       "0  Test_0_consonant_diacritic         0         0       0\n",
       "1        Test_0_grapheme_root         3         3       3\n",
       "2      Test_0_vowel_diacritic         0         0       0\n",
       "3  Test_1_consonant_diacritic         0         0       0\n",
       "4        Test_1_grapheme_root        93        93      93"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=merge[\"target_x\"]\n",
    "model2=merge[\"target_y\"]\n",
    "model3=merge[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight\n",
    "k1=1.2\n",
    "k2=1.0\n",
    "k3=0.8\n",
    "\n",
    "#convert to one-hot\n",
    "from keras.utils import to_categorical\n",
    "model1_onehot=to_categorical(model1, 168)\n",
    "model2_onehot=to_categorical(model2, 168)\n",
    "model3_onehot=to_categorical(model3, 168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voting\n",
    "voting_result=[]\n",
    "for i in range(len(merge)):\n",
    "    voting = np.argmax(model1_onehot[i]*k1 + model2_onehot[i]*k2+ model3_onehot[i]*k3)\n",
    "    voting_result.append(voting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 93,\n",
       " 2,\n",
       " 0,\n",
       " 19,\n",
       " 0,\n",
       " 0,\n",
       " 115,\n",
       " 0,\n",
       " 0,\n",
       " 79,\n",
       " 4,\n",
       " 0,\n",
       " 115,\n",
       " 2,\n",
       " 5,\n",
       " 147,\n",
       " 9,\n",
       " 0,\n",
       " 137,\n",
       " 7,\n",
       " 0,\n",
       " 119,\n",
       " 9,\n",
       " 0,\n",
       " 133,\n",
       " 10,\n",
       " 4,\n",
       " 148,\n",
       " 1,\n",
       " 0,\n",
       " 21,\n",
       " 2]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target_x</th>\n",
       "      <th>target_y</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_0_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_0_grapheme_root</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_0_vowel_diacritic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_1_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_1_grapheme_root</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Test_1_vowel_diacritic</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Test_2_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Test_2_grapheme_root</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Test_2_vowel_diacritic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Test_3_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Test_3_grapheme_root</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Test_3_vowel_diacritic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Test_4_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Test_4_grapheme_root</td>\n",
       "      <td>79</td>\n",
       "      <td>55</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Test_4_vowel_diacritic</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Test_5_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Test_5_grapheme_root</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Test_5_vowel_diacritic</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Test_6_consonant_diacritic</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Test_6_grapheme_root</td>\n",
       "      <td>147</td>\n",
       "      <td>147</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Test_6_vowel_diacritic</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Test_7_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Test_7_grapheme_root</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Test_7_vowel_diacritic</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Test_8_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Test_8_grapheme_root</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Test_8_vowel_diacritic</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Test_9_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Test_9_grapheme_root</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Test_9_vowel_diacritic</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Test_10_consonant_diacritic</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Test_10_grapheme_root</td>\n",
       "      <td>148</td>\n",
       "      <td>148</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Test_10_vowel_diacritic</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Test_11_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Test_11_grapheme_root</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Test_11_vowel_diacritic</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         row_id  target_x  target_y  target\n",
       "0    Test_0_consonant_diacritic         0         0       0\n",
       "1          Test_0_grapheme_root         3         3       3\n",
       "2        Test_0_vowel_diacritic         0         0       0\n",
       "3    Test_1_consonant_diacritic         0         0       0\n",
       "4          Test_1_grapheme_root        93        93      93\n",
       "5        Test_1_vowel_diacritic         2         2       2\n",
       "6    Test_2_consonant_diacritic         0         0       0\n",
       "7          Test_2_grapheme_root        19        19      19\n",
       "8        Test_2_vowel_diacritic         0         0       0\n",
       "9    Test_3_consonant_diacritic         0         0       0\n",
       "10         Test_3_grapheme_root       115       115     115\n",
       "11       Test_3_vowel_diacritic         0         0       0\n",
       "12   Test_4_consonant_diacritic         0         0       0\n",
       "13         Test_4_grapheme_root        79        55      79\n",
       "14       Test_4_vowel_diacritic         4         4       4\n",
       "15   Test_5_consonant_diacritic         0         0       0\n",
       "16         Test_5_grapheme_root       115       115     115\n",
       "17       Test_5_vowel_diacritic         2         2       2\n",
       "18   Test_6_consonant_diacritic         5         5       5\n",
       "19         Test_6_grapheme_root       147       147     147\n",
       "20       Test_6_vowel_diacritic         9         9       9\n",
       "21   Test_7_consonant_diacritic         0         0       0\n",
       "22         Test_7_grapheme_root       137       137     137\n",
       "23       Test_7_vowel_diacritic         7         7       7\n",
       "24   Test_8_consonant_diacritic         0         0       0\n",
       "25         Test_8_grapheme_root       119       119     119\n",
       "26       Test_8_vowel_diacritic         9         9       9\n",
       "27   Test_9_consonant_diacritic         0         0       0\n",
       "28         Test_9_grapheme_root       133       133     133\n",
       "29       Test_9_vowel_diacritic        10        10      10\n",
       "30  Test_10_consonant_diacritic         4         4       4\n",
       "31        Test_10_grapheme_root       148       148     148\n",
       "32      Test_10_vowel_diacritic         1         1       1\n",
       "33  Test_11_consonant_diacritic         0         0       0\n",
       "34        Test_11_grapheme_root        21        21      21\n",
       "35      Test_11_vowel_diacritic         2         2       2"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge=merge.drop([\"target_x\", \"target_y\", \"target\"], axis=1)\n",
    "#merge=merge.drop([\"target_x\", \"target_y\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge[\"target\"]=voting_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_0_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_0_grapheme_root</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_0_vowel_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_1_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_1_grapheme_root</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Test_1_vowel_diacritic</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Test_2_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Test_2_grapheme_root</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Test_2_vowel_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Test_3_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Test_3_grapheme_root</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Test_3_vowel_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Test_4_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Test_4_grapheme_root</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Test_4_vowel_diacritic</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Test_5_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Test_5_grapheme_root</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Test_5_vowel_diacritic</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Test_6_consonant_diacritic</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Test_6_grapheme_root</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Test_6_vowel_diacritic</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Test_7_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Test_7_grapheme_root</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Test_7_vowel_diacritic</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Test_8_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Test_8_grapheme_root</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Test_8_vowel_diacritic</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Test_9_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Test_9_grapheme_root</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Test_9_vowel_diacritic</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Test_10_consonant_diacritic</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Test_10_grapheme_root</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Test_10_vowel_diacritic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Test_11_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Test_11_grapheme_root</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Test_11_vowel_diacritic</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         row_id  target\n",
       "0    Test_0_consonant_diacritic       0\n",
       "1          Test_0_grapheme_root       3\n",
       "2        Test_0_vowel_diacritic       0\n",
       "3    Test_1_consonant_diacritic       0\n",
       "4          Test_1_grapheme_root      93\n",
       "5        Test_1_vowel_diacritic       2\n",
       "6    Test_2_consonant_diacritic       0\n",
       "7          Test_2_grapheme_root      19\n",
       "8        Test_2_vowel_diacritic       0\n",
       "9    Test_3_consonant_diacritic       0\n",
       "10         Test_3_grapheme_root     115\n",
       "11       Test_3_vowel_diacritic       0\n",
       "12   Test_4_consonant_diacritic       0\n",
       "13         Test_4_grapheme_root      79\n",
       "14       Test_4_vowel_diacritic       4\n",
       "15   Test_5_consonant_diacritic       0\n",
       "16         Test_5_grapheme_root     115\n",
       "17       Test_5_vowel_diacritic       2\n",
       "18   Test_6_consonant_diacritic       5\n",
       "19         Test_6_grapheme_root     147\n",
       "20       Test_6_vowel_diacritic       9\n",
       "21   Test_7_consonant_diacritic       0\n",
       "22         Test_7_grapheme_root     137\n",
       "23       Test_7_vowel_diacritic       7\n",
       "24   Test_8_consonant_diacritic       0\n",
       "25         Test_8_grapheme_root     119\n",
       "26       Test_8_vowel_diacritic       9\n",
       "27   Test_9_consonant_diacritic       0\n",
       "28         Test_9_grapheme_root     133\n",
       "29       Test_9_vowel_diacritic      10\n",
       "30  Test_10_consonant_diacritic       4\n",
       "31        Test_10_grapheme_root     148\n",
       "32      Test_10_vowel_diacritic       1\n",
       "33  Test_11_consonant_diacritic       0\n",
       "34        Test_11_grapheme_root      21\n",
       "35      Test_11_vowel_diacritic       2"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0169c3934d554b0290412fe96e266694": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "01d6d56c20b748b2a379dbb53ce4d6bf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "04095b2fae28477db55e92782d0714d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c1386be5fbfb4b86886383eccdbfac93",
       "placeholder": "​",
       "style": "IPY_MODEL_395d8d95efd64945a6ec54c491a5472a",
       "value": " 1/1 [00:00&lt;00:00,  4.90it/s]"
      }
     },
     "07ad918526ec4ce5ba3c006757c3d72b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_da509f09031746239245630cb74058b9",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ae0044ded6ae4ccf8569fcbce5093ce2",
       "value": 1.0
      }
     },
     "0c17c3b1a7374c57a140d043dc042ca7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "0e02b8bd609741efa4608bde52fc4120": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_169e1a2e855642d1a93ec061c9b33ab1",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7f7f316df5174f779f3bdaf869efe0e5",
       "value": 1.0
      }
     },
     "169e1a2e855642d1a93ec061c9b33ab1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1bde998c00144134b12d900d78780291": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2c22a75bfda44fb587645f0670484612": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "33878a735d0b43a0bec0c04aa9955ae3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "343acc38e2ca4009b935b8924c51bf9f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "37c2e466c8f84791b51ed0cb86d00723": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_01d6d56c20b748b2a379dbb53ce4d6bf",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9bb5861f31904794abd2a3bd74ff00ed",
       "value": 1.0
      }
     },
     "395d8d95efd64945a6ec54c491a5472a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3c8b3f610e6b496d9d8c6123ed51d85a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ad4a9a57a60a41d1b7e0b999b1bc7f6d",
        "IPY_MODEL_52e97df07fe4496c815edc1f2dc04c5a"
       ],
       "layout": "IPY_MODEL_d9cf36056c58448e870403903a829cad"
      }
     },
     "44932fc3d0d14c5f891c9b3d24f5b9a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "512bcdd39d5c48e8a8386106ef633956": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "52e97df07fe4496c815edc1f2dc04c5a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8bb2a2944fd24f2881fbfe2567439206",
       "placeholder": "​",
       "style": "IPY_MODEL_343acc38e2ca4009b935b8924c51bf9f",
       "value": " 1/1 [00:00&lt;00:00,  2.76it/s]"
      }
     },
     "5918ea0942f1435e90b1d31bcdae926c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_db83bfc10cee487499722db02abcdb99",
        "IPY_MODEL_04095b2fae28477db55e92782d0714d5"
       ],
       "layout": "IPY_MODEL_c8a9f31e45d44b2d875cbf614db4a83f"
      }
     },
     "5ea0e5806aba4eeca913e2b0753b751b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "608d9271ca674ce38a06442d4b62ba42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_37c2e466c8f84791b51ed0cb86d00723",
        "IPY_MODEL_86cfa056b1934d3abe24752a1a417e6a"
       ],
       "layout": "IPY_MODEL_80c86b694a644b4fbdd011264b635802"
      }
     },
     "63a9fa7e335e4cdab07d345d707b5aad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "68cd1dd00b5b4dd8a8994cf04695754e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_07ad918526ec4ce5ba3c006757c3d72b",
        "IPY_MODEL_f1c843bd38de4889ad7b864d03b4c74c"
       ],
       "layout": "IPY_MODEL_512bcdd39d5c48e8a8386106ef633956"
      }
     },
     "707149cfb5044e53ae5518a8ce0ec39a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "7406408526364932b6fa0e37c580b2a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "7654187cac16477982be9d2654b8e507": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0e02b8bd609741efa4608bde52fc4120",
        "IPY_MODEL_cbb5c66574ac4e8ca04a065fd0e515c2"
       ],
       "layout": "IPY_MODEL_eaa1ea334e1545a88b5d6b684ba61c8f"
      }
     },
     "7c08de82bc134c86a90af07ca05fb829": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7c9cc41fb1aa4efab547c82a2561994f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9b2302cef4ee49d2a99a1b7ba634a005",
        "IPY_MODEL_9db17978d38d478a9bafee5e4087e022"
       ],
       "layout": "IPY_MODEL_941d138765264c0da5d46fc955b11fcb"
      }
     },
     "7f3b03635003424396f0206304ceaa0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7f7f316df5174f779f3bdaf869efe0e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "8025a83ccc0641a18ee8fb0d4bcdbd15": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5ea0e5806aba4eeca913e2b0753b751b",
       "placeholder": "​",
       "style": "IPY_MODEL_997ef3abd03144dfbc06dbca1aeb527e",
       "value": " 1/1 [00:01&lt;00:00,  1.40s/it]"
      }
     },
     "806407440d48413e813d0d881842f05d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "80c86b694a644b4fbdd011264b635802": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "82475127ae914c069bee8265454c856f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "86cfa056b1934d3abe24752a1a417e6a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b5683c47fa894184a5fa7b52cb06c79c",
       "placeholder": "​",
       "style": "IPY_MODEL_9bebf87f38cf45c38e697ea0d5fc25c8",
       "value": " 1/1 [00:01&lt;00:00,  1.27s/it]"
      }
     },
     "8a5695c1694d46fc8170654719cb386c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "8bb2a2944fd24f2881fbfe2567439206": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8ca19300f1dc4fbf8ad44555ae886700": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "941d138765264c0da5d46fc955b11fcb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "997ef3abd03144dfbc06dbca1aeb527e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9b2302cef4ee49d2a99a1b7ba634a005": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8ca19300f1dc4fbf8ad44555ae886700",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0c17c3b1a7374c57a140d043dc042ca7",
       "value": 1.0
      }
     },
     "9bb5861f31904794abd2a3bd74ff00ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "9bebf87f38cf45c38e697ea0d5fc25c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9db17978d38d478a9bafee5e4087e022": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_63a9fa7e335e4cdab07d345d707b5aad",
       "placeholder": "​",
       "style": "IPY_MODEL_cd8f505782d64a88af0831156915509a",
       "value": " 1/1 [00:00&lt;00:00,  2.46it/s]"
      }
     },
     "a35390a676f94d2eb346b9a693c97639": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e7071bdfc4e243e3bd1bf3f422a9b1c1",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8a5695c1694d46fc8170654719cb386c",
       "value": 1.0
      }
     },
     "a82575a9856b47cfa2ed431cf5926315": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fc60057279b8464c89d943a640c475d7",
        "IPY_MODEL_bbd45b5f3ca54f8a920fa40f645a115d"
       ],
       "layout": "IPY_MODEL_d8a506b66777474f8838a5deed235286"
      }
     },
     "ad4a9a57a60a41d1b7e0b999b1bc7f6d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_82475127ae914c069bee8265454c856f",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7406408526364932b6fa0e37c580b2a5",
       "value": 1.0
      }
     },
     "ae0044ded6ae4ccf8569fcbce5093ce2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "b167a76eb3af446987e058161f4b2d34": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b343b9c21cfa4112a23d05e7f4303ed7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a35390a676f94d2eb346b9a693c97639",
        "IPY_MODEL_8025a83ccc0641a18ee8fb0d4bcdbd15"
       ],
       "layout": "IPY_MODEL_33878a735d0b43a0bec0c04aa9955ae3"
      }
     },
     "b5683c47fa894184a5fa7b52cb06c79c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bbd45b5f3ca54f8a920fa40f645a115d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2c22a75bfda44fb587645f0670484612",
       "placeholder": "​",
       "style": "IPY_MODEL_7f3b03635003424396f0206304ceaa0d",
       "value": " 1/1 [00:00&lt;00:00,  2.49it/s]"
      }
     },
     "c1386be5fbfb4b86886383eccdbfac93": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c8a9f31e45d44b2d875cbf614db4a83f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cbb5c66574ac4e8ca04a065fd0e515c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_806407440d48413e813d0d881842f05d",
       "placeholder": "​",
       "style": "IPY_MODEL_44932fc3d0d14c5f891c9b3d24f5b9a5",
       "value": " 1/1 [00:00&lt;00:00,  2.82it/s]"
      }
     },
     "cd8f505782d64a88af0831156915509a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "cfda423aff7a43aabf321e4edf3c72b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "d8a506b66777474f8838a5deed235286": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d9cf36056c58448e870403903a829cad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "da509f09031746239245630cb74058b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "db83bfc10cee487499722db02abcdb99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1bde998c00144134b12d900d78780291",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_cfda423aff7a43aabf321e4edf3c72b0",
       "value": 1.0
      }
     },
     "e7071bdfc4e243e3bd1bf3f422a9b1c1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eaa1ea334e1545a88b5d6b684ba61c8f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f1c843bd38de4889ad7b864d03b4c74c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0169c3934d554b0290412fe96e266694",
       "placeholder": "​",
       "style": "IPY_MODEL_7c08de82bc134c86a90af07ca05fb829",
       "value": " 1/1 [00:00&lt;00:00,  2.58it/s]"
      }
     },
     "fc60057279b8464c89d943a640c475d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b167a76eb3af446987e058161f4b2d34",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_707149cfb5044e53ae5518a8ce0ec39a",
       "value": 1.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
