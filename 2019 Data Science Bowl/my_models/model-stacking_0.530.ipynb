{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction  \n",
    "  \n",
    "In this kernel, I try 2 layers stacking.  \n",
    "Using models are NN, LGB, XGB, CAT.  \n",
    "The score not so high but low overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/data-science-bowl-2019/sample_submission.csv\n",
      "/kaggle/input/data-science-bowl-2019/specs.csv\n",
      "/kaggle/input/data-science-bowl-2019/train_labels.csv\n",
      "/kaggle/input/data-science-bowl-2019/test.csv\n",
      "/kaggle/input/data-science-bowl-2019/train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from catboost import CatBoostRegressor\n",
    "from matplotlib import pyplot\n",
    "import shap\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import gc\n",
    "import json\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Objective\n",
    "\n",
    "* In the last notebook we create our baseline model including a feature selection part. \n",
    "* Cohen cappa score of 0.456 (lb) with a local cv score of 0.529\n",
    "* In this notebook we are going to add more features and remove others that i think they overfitt the train set and then check if our local cv score improve.\n",
    "* Next, we will check if this improvement aligns with the lb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "* Check the distribution of the target variable of the out of folds score and the prediction distribution. A good model should more or less have the same distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_qwk_lgb_regr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fast cappa eval function for lgb.\n",
    "    \"\"\"\n",
    "    dist = Counter(reduce_train['accuracy_group'])\n",
    "    for k in dist:\n",
    "        dist[k] /= len(reduce_train)\n",
    "    reduce_train['accuracy_group'].hist()\n",
    "    \n",
    "    acum = 0\n",
    "    bound = {}\n",
    "    for i in range(3):\n",
    "        acum += dist[i]\n",
    "        bound[i] = np.percentile(y_pred, acum * 100)\n",
    "\n",
    "    def classify(x):\n",
    "        if x <= bound[0]:\n",
    "            return 0\n",
    "        elif x <= bound[1]:\n",
    "            return 1\n",
    "        elif x <= bound[2]:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    y_pred = np.array(list(map(classify, y_pred))).reshape(y_true.shape)\n",
    "\n",
    "    return 'cappa', cohen_kappa_score(y_true, y_pred, weights='quadratic'), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohenkappa(ypred, y):\n",
    "    y = y.get_label().astype(\"int\")\n",
    "    ypred = ypred.reshape((4, -1)).argmax(axis = 0)\n",
    "    loss = cohenkappascore(y, y_pred, weights = 'quadratic')\n",
    "    return \"cappa\", loss, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    print('Reading train.csv file....')\n",
    "    train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\n",
    "    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n",
    "\n",
    "    print('Reading test.csv file....')\n",
    "    test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\n",
    "    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n",
    "\n",
    "    print('Reading train_labels.csv file....')\n",
    "    train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\n",
    "    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n",
    "\n",
    "    print('Reading specs.csv file....')\n",
    "    specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\n",
    "    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n",
    "\n",
    "    print('Reading sample_submission.csv file....')\n",
    "    sample_submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n",
    "    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n",
    "    return train, test, train_labels, specs, sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnt_miss(df):\n",
    "    cnt = 0\n",
    "    for e in range(len(df)):\n",
    "        x = df['event_data'].iloc[e]\n",
    "        y = json.loads(x)['misses']\n",
    "        cnt += y\n",
    "    return cnt\n",
    "\n",
    "def get_4020_acc(df,counter_dict):\n",
    "    \n",
    "    for e in ['Cauldron Filler (Assessment)','Bird Measurer (Assessment)','Mushroom Sorter (Assessment)','Chest Sorter (Assessment)']:\n",
    "        \n",
    "        Assess_4020 = df[(df.event_code == 4020) & (df.title==activities_map[e])]   \n",
    "        true_attempts_ = Assess_4020['event_data'].str.contains('true').sum()\n",
    "        false_attempts_ = Assess_4020['event_data'].str.contains('false').sum()\n",
    "\n",
    "        measure_assess_accuracy_ = true_attempts_/(true_attempts_+false_attempts_) if (true_attempts_+false_attempts_) != 0 else 0\n",
    "        counter_dict[e+\"_4020_accuracy\"] += (counter_dict[e+\"_4020_accuracy\"] + measure_assess_accuracy_) / 2.0\n",
    "    \n",
    "    return counter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_title(train, test, train_labels):\n",
    "    # encode title\n",
    "    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n",
    "    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n",
    "    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n",
    "    \n",
    "    train['type_world'] = list(map(lambda x, y: str(x) + '_' + str(y), train['type'], train['world']))\n",
    "    test['type_world'] = list(map(lambda x, y: str(x) + '_' + str(y), test['type'], test['world']))\n",
    "    all_type_world = list(set(train[\"type_world\"].unique()).union(test[\"type_world\"].unique()))\n",
    "    \n",
    "    # make a list with all the unique 'titles' from the train and test set\n",
    "    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n",
    "    # make a list with all the unique 'event_code' from the train and test set\n",
    "    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n",
    "    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n",
    "    # make a list with all the unique worlds from the train and test set\n",
    "    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n",
    "    # create a dictionary numerating the titles\n",
    "    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n",
    "    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n",
    "    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n",
    "    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n",
    "    # replace the text titles with the number titles from the dict\n",
    "    train['title'] = train['title'].map(activities_map)\n",
    "    test['title'] = test['title'].map(activities_map)\n",
    "    train['world'] = train['world'].map(activities_world)\n",
    "    test['world'] = test['world'].map(activities_world)\n",
    "    train_labels['title'] = train_labels['title'].map(activities_map)\n",
    "    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n",
    "    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n",
    "    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n",
    "    # convert text into datetime\n",
    "    train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "    test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "    \n",
    "    \n",
    "    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code, activities_map,all_type_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the function that convert the raw data into processed features\n",
    "def get_data(user_sample, test_set=False):\n",
    "    '''\n",
    "    The user_sample is a DataFrame from train or test where the only one \n",
    "    installation_id is filtered\n",
    "    And the test_set parameter is related with the labels processing, that is only requered\n",
    "    if test_set=False\n",
    "    '''\n",
    "    # Constants and parameters declaration\n",
    "    last_activity = 0\n",
    "    \n",
    "    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "    \n",
    "    assess_4020_acc_dict = {'Cauldron Filler (Assessment)_4020_accuracy':0,\n",
    "                                'Mushroom Sorter (Assessment)_4020_accuracy':0,\n",
    "                                'Bird Measurer (Assessment)_4020_accuracy':0,\n",
    "                                'Chest Sorter (Assessment)_4020_accuracy':0 }\n",
    "    \n",
    "    game_time_dict = {'Clip_gametime':0, 'Game_gametime':0, 'Activity_gametime':0, 'Assessment_gametime':0}\n",
    "    \n",
    "    # new features: time spent in each activity\n",
    "    last_session_time_sec = 0\n",
    "    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n",
    "    all_assessments = []\n",
    "    accumulated_accuracy_group = 0\n",
    "    accumulated_accuracy = 0\n",
    "    accumulated_correct_attempts = 0 \n",
    "    accumulated_uncorrect_attempts = 0\n",
    "    accumulated_actions = 0\n",
    "    \n",
    "    # Newly added features\n",
    "    accumulated_game_miss = 0\n",
    "    Cauldron_Filler_4025 = 0\n",
    "    mean_game_round = 0\n",
    "    mean_game_duration = 0 \n",
    "    mean_game_level = 0\n",
    "    Assessment_mean_event_count = 0\n",
    "    Game_mean_event_count = 0\n",
    "    Activity_mean_event_count = 0\n",
    "    chest_assessment_uncorrect_sum = 0\n",
    "    \n",
    "    counter = 0\n",
    "    time_first_activity = float(user_sample['timestamp'].values[0])\n",
    "    durations = []\n",
    "    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n",
    "    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n",
    "    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n",
    "    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n",
    "    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n",
    "    type_world_count: Dict[str, int] = {w_eve: 0 for w_eve in all_type_world}\n",
    "        \n",
    "    # last features\n",
    "    sessions_count = 0\n",
    "    \n",
    "    # itarates through each session of one instalation_id\n",
    "    for i, session in user_sample.groupby('game_session', sort=False):\n",
    "        # i = game_session_id\n",
    "        # session is a DataFrame that contain only one game_session\n",
    "        \n",
    "        # get some sessions information\n",
    "        session_type = session['type'].iloc[0]\n",
    "        session_title = session['title'].iloc[0]\n",
    "        session_title_text = activities_labels[session_title]\n",
    "                    \n",
    "        if session_type==\"Activity\":\n",
    "            Activity_mean_event_count = (Activity_mean_event_count + session['event_count'].iloc[-1])/2.0\n",
    "            \n",
    "        if session_type==\"Game\":\n",
    "            \n",
    "            Game_mean_event_count = (Game_mean_event_count + session['event_count'].iloc[-1])/2.0\n",
    "            \n",
    "            game_s = session[session.event_code == 2030]   \n",
    "            misses_cnt = cnt_miss(game_s)\n",
    "            accumulated_game_miss += misses_cnt\n",
    "            \n",
    "            try:\n",
    "                game_round = json.loads(session['event_data'].iloc[-1])[\"round\"]\n",
    "                mean_game_round =  (mean_game_round + game_round)/2.0\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                game_duration = json.loads(session['event_data'].iloc[-1])[\"duration\"]\n",
    "                mean_game_duration = (mean_game_duration + game_duration) /2.0\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                game_level = json.loads(session['event_data'].iloc[-1])[\"level\"]\n",
    "                mean_game_level = (mean_game_level + game_level) /2.0\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        # for each assessment, and only this kind off session, the features below are processed\n",
    "        # and a register are generated\n",
    "        if (session_type == 'Assessment') & (test_set or len(session)>1):\n",
    "            # search for event_code 4100, that represents the assessments trial\n",
    "            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n",
    "            # then, check the numbers of wins and the number of losses\n",
    "            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n",
    "            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n",
    "            # copy a dict to use as feature template, it's initialized with some itens: \n",
    "            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "            features = user_activities_count.copy()\n",
    "            features.update(last_accuracy_title.copy())\n",
    "            features.update(event_code_count.copy())\n",
    "            features.update(event_id_count.copy())\n",
    "            features.update(title_count.copy())\n",
    "            features.update(game_time_dict.copy())\n",
    "            features.update(title_event_code_count.copy())\n",
    "            features.update(last_accuracy_title.copy())\n",
    "            features.update(assess_4020_acc_dict.copy())\n",
    "            features.update(type_world_count.copy())\n",
    "            \n",
    "            #features['installation_session_count'] = sessions_count\n",
    "            features['accumulated_game_miss'] = accumulated_game_miss\n",
    "            features['mean_game_round'] = mean_game_round\n",
    "            features['mean_game_duration'] = mean_game_duration\n",
    "            features['mean_game_level'] = mean_game_level\n",
    "            features['Assessment_mean_event_count'] = Assessment_mean_event_count\n",
    "            features['Game_mean_event_count'] = Game_mean_event_count\n",
    "            features['Activity_mean_event_count'] = Activity_mean_event_count\n",
    "            features['chest_assessment_uncorrect_sum'] = chest_assessment_uncorrect_sum\n",
    "            \n",
    "            variety_features = [('var_event_code', event_code_count),\n",
    "                              ('var_event_id', event_id_count),\n",
    "                               ('var_title', title_count),\n",
    "                               ('var_title_event_code', title_event_code_count),\n",
    "                                ('var_type_world', type_world_count)]\n",
    "            \n",
    "            for name, dict_counts in variety_features:\n",
    "                arr = np.array(list(dict_counts.values()))\n",
    "                features[name] = np.count_nonzero(arr)\n",
    "                 \n",
    "            # get installation_id for aggregated features\n",
    "            features['installation_id'] = session['installation_id'].iloc[-1]\n",
    "            # add title as feature, remembering that title represents the name of the game\n",
    "            features['session_title'] = session['title'].iloc[0]\n",
    "            # the 4 lines below add the feature of the history of the trials of this player\n",
    "            # this is based on the all time attempts so far, at the moment of this assessment\n",
    "            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n",
    "            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n",
    "            accumulated_correct_attempts += true_attempts \n",
    "            accumulated_uncorrect_attempts += false_attempts\n",
    "            # the time spent in the app so far\n",
    "            if durations == []:\n",
    "                features['duration_mean'] = 0\n",
    "                features['duration_std'] = 0\n",
    "            else:\n",
    "                features['duration_mean'] = np.mean(durations)\n",
    "                features['duration_std'] = np.std(durations)\n",
    "            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            # the accurace is the all time wins divided by the all time attempts\n",
    "            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n",
    "            \n",
    "            features['Cauldron_Filler_4025'] = Cauldron_Filler_4025/counter if counter > 0 else 0\n",
    "            ####################\n",
    "            Assess_4025 = session[(session.event_code == 4025) & (session.title=='Cauldron Filler (Assessment)')]   \n",
    "            true_attempts_ = Assess_4025['event_data'].str.contains('true').sum()\n",
    "            false_attempts_ = Assess_4025['event_data'].str.contains('false').sum()\n",
    "\n",
    "            cau_assess_accuracy_ = true_attempts_/(true_attempts_+false_attempts_) if (true_attempts_+false_attempts_) != 0 else 0\n",
    "            Cauldron_Filler_4025 += cau_assess_accuracy_\n",
    "            \n",
    "            chest_assessment_uncorrect_sum += len(session[session.event_id==\"df4fe8b6\"])\n",
    "            \n",
    "            Assessment_mean_event_count = (Assessment_mean_event_count + session['event_count'].iloc[-1])/2.0\n",
    "            ##############################\n",
    "            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n",
    "            accumulated_accuracy += accuracy\n",
    "            last_accuracy_title['acc_' + session_title_text] = accuracy\n",
    "            # a feature of the current accuracy categorized\n",
    "            # it is a counter of how many times this player was in each accuracy group\n",
    "            if accuracy == 0:\n",
    "                features['accuracy_group'] = 0\n",
    "            elif accuracy == 1:\n",
    "                features['accuracy_group'] = 3\n",
    "            elif accuracy == 0.5:\n",
    "                features['accuracy_group'] = 2\n",
    "            else:\n",
    "                features['accuracy_group'] = 1\n",
    "            features.update(accuracy_groups)\n",
    "            accuracy_groups[features['accuracy_group']] += 1\n",
    "            # mean of the all accuracy groups of this player\n",
    "            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n",
    "            accumulated_accuracy_group += features['accuracy_group']\n",
    "            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n",
    "            features['accumulated_actions'] = accumulated_actions\n",
    "            \n",
    "            # there are some conditions to allow this features to be inserted in the datasets\n",
    "            # if it's a test set, all sessions belong to the final dataset\n",
    "            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n",
    "            # that means, must exist an event_code 4100 or 4110\n",
    "            if test_set:\n",
    "                all_assessments.append(features)\n",
    "            elif true_attempts+false_attempts > 0:\n",
    "                all_assessments.append(features)\n",
    "                \n",
    "            counter += 1\n",
    "        \n",
    "        # end session_type==\"Assessment\"\n",
    "        \n",
    "        sessions_count += 1\n",
    "        # this piece counts how many actions was made in each event_code so far\n",
    "        def update_counters(counter: dict, col: str):\n",
    "                num_of_session_count = Counter(session[col])\n",
    "                for k in num_of_session_count.keys():\n",
    "                    x = k\n",
    "                    if col == 'title':\n",
    "                        x = activities_labels[k]\n",
    "                    counter[x] += num_of_session_count[k]\n",
    "                return counter\n",
    "            \n",
    "        event_code_count = update_counters(event_code_count, \"event_code\")\n",
    "        event_id_count = update_counters(event_id_count, \"event_id\")\n",
    "        title_count = update_counters(title_count, 'title')\n",
    "        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n",
    "        type_world_count = update_counters(type_world_count, 'type_world')\n",
    "        \n",
    "        assess_4020_acc_dict = get_4020_acc(session , assess_4020_acc_dict)\n",
    "        game_time_dict[session_type+'_gametime'] = (game_time_dict[session_type+'_gametime'] + (session['game_time'].iloc[-1]/1000.0))/2.0\n",
    "        \n",
    "        # counts how many actions the player has done so far, used in the feature of the same name\n",
    "        accumulated_actions += len(session)\n",
    "        if last_activity != session_type:\n",
    "            user_activities_count[session_type] += 1\n",
    "            last_activitiy = session_type \n",
    "                        \n",
    "    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n",
    "    if test_set:\n",
    "        return all_assessments[-1]\n",
    "    # in the train_set, all assessments goes to the dataset\n",
    "    return all_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test(train, test):\n",
    "    compiled_train = []\n",
    "    compiled_test = []\n",
    "    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total = 17000):\n",
    "        compiled_train += get_data(user_sample)\n",
    "    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n",
    "        test_data = get_data(user_sample, test_set = True)\n",
    "        compiled_test.append(test_data)\n",
    "    reduce_train = pd.DataFrame(compiled_train)\n",
    "    reduce_test = pd.DataFrame(compiled_test)\n",
    "    categoricals = ['session_title']\n",
    "    return reduce_train, reduce_test, categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_Model(object):\n",
    "    \n",
    "    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True,ps={}):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.features = features\n",
    "        self.n_splits = n_splits\n",
    "        self.categoricals = categoricals\n",
    "        self.target = 'accuracy_group'\n",
    "        self.cv = self.get_cv()\n",
    "        self.verbose = verbose\n",
    "        #self.params = self.get_params()\n",
    "        self.params = self.set_params(ps)\n",
    "        self.oof_pred, self.y_pred, self.score, self.model = self.fit()\n",
    "        \n",
    "    def train_model(self, train_set, val_set):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_cv(self):\n",
    "        cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n",
    "        return cv.split(self.train_df, self.train_df[self.target])\n",
    "    \n",
    "    def get_params(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def convert_x(self, x):\n",
    "        return x\n",
    "        \n",
    "    def fit(self):\n",
    "        oof_pred = np.zeros((len(reduce_train), ))\n",
    "        y_pred = np.zeros((len(reduce_test), ))\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv):\n",
    "            x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]\n",
    "            y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]\n",
    "            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n",
    "            model = self.train_model(train_set, val_set)\n",
    "            conv_x_val = self.convert_x(x_val)\n",
    "            oof_pred[val_idx] = model.predict(conv_x_val).reshape(oof_pred[val_idx].shape)\n",
    "            x_test = self.convert_x(self.test_df[self.features])\n",
    "            y_pred += model.predict(x_test).reshape(y_pred.shape) / self.n_splits\n",
    "            print('Partial score of fold {} is: {}'.format(fold, eval_qwk_lgb_regr(y_val, oof_pred[val_idx])[1]))\n",
    "        _, loss_score, _ = eval_qwk_lgb_regr(self.train_df[self.target], oof_pred)\n",
    "        if self.verbose:\n",
    "            print('Our oof cohen kappa score is: ', loss_score)\n",
    "        return oof_pred, y_pred, loss_score, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lgb_Model(Base_Model):\n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        return lgb.train(self.params, train_set, valid_sets=[train_set, val_set], verbose_eval=verbosity)\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        x_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in x_train.columns]\n",
    "        x_val.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in x_val.columns]\n",
    "        train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)\n",
    "        val_set = lgb.Dataset(x_val, y_val, categorical_feature=self.categoricals)\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        params = {'n_estimators':5000,\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'regression',\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.75,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.01,\n",
    "                    'feature_fraction': 0.9,\n",
    "                    'max_depth': 15,\n",
    "                    'lambda_l1': 1,  \n",
    "                    'lambda_l2': 1,\n",
    "                    'early_stopping_rounds': 100\n",
    "                    }\n",
    "        return params\n",
    "    def set_params(self,ps={}):\n",
    "        params = self.get_params()\n",
    "        if 'subsample_freq' in ps:\n",
    "            params['subsample_freq']=int(ps['subsample_freq'])\n",
    "            params['learning_rate']=ps['learning_rate']\n",
    "            params['feature_fraction']=ps['feature_fraction']\n",
    "            params['lambda_l1']=ps['lambda_l1']\n",
    "            params['lambda_l2']=ps['lambda_l2']\n",
    "            params['max_depth']=int(ps['max_depth'])\n",
    "        \n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xgb_Model(Base_Model):\n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        return xgb.train(self.params, train_set, \n",
    "                         num_boost_round=5000, evals=[(train_set, 'train'), (val_set, 'val')], \n",
    "                         verbose_eval=verbosity, early_stopping_rounds=100)\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = xgb.DMatrix(x_train, y_train)\n",
    "        val_set = xgb.DMatrix(x_val, y_val)\n",
    "        return train_set, val_set\n",
    "    \n",
    "    def convert_x(self, x):\n",
    "        return xgb.DMatrix(x)\n",
    "        \n",
    "    def get_params(self):\n",
    "        params = {'colsample_bytree': 0.8,                 \n",
    "            'learning_rate': 0.01,\n",
    "            'max_depth': 10,\n",
    "            'subsample': 1,\n",
    "            'objective':'reg:squarederror',\n",
    "            #'eval_metric':'rmse',\n",
    "            'min_child_weight':3,\n",
    "            'gamma':0.25,\n",
    "            'n_estimators':5000}\n",
    "        return params\n",
    "    \n",
    "    def set_params(self, ps={}):\n",
    "        return self.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Catb_Model(Base_Model):\n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        clf = CatBoostRegressor(**self.params)\n",
    "        clf.fit(train_set['X'], \n",
    "                train_set['y'], \n",
    "                eval_set=(val_set['X'], val_set['y']),\n",
    "                verbose=verbosity, \n",
    "                cat_features=self.categoricals)\n",
    "        return clf\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        params = {'loss_function': 'RMSE',\n",
    "                   'task_type': \"CPU\",\n",
    "                   'iterations': 5000,\n",
    "                   'od_type': \"Iter\",\n",
    "                    'depth': 10,\n",
    "                  'colsample_bylevel': 0.5, \n",
    "                   'early_stopping_rounds': 300,\n",
    "                    'l2_leaf_reg': 18,\n",
    "                   'random_seed': 42,\n",
    "                    'use_best_model': True\n",
    "                    }\n",
    "        return params\n",
    "    \n",
    "    def set_params(self, ps={}):\n",
    "        return self.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "class Nn_Model(Base_Model):\n",
    "    \n",
    "    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n",
    "        features = features.copy()\n",
    "        if len(categoricals) > 0:\n",
    "            for cat in categoricals:\n",
    "                enc = OneHotEncoder()\n",
    "                train_cats = enc.fit_transform(train_df[[cat]])\n",
    "                test_cats = enc.transform(test_df[[cat]])\n",
    "                cat_cols = ['{}_{}'.format(cat, str(col)) for col in enc.active_features_]\n",
    "                features += cat_cols\n",
    "                train_cats = pd.DataFrame(train_cats.toarray(), columns=cat_cols)\n",
    "                test_cats = pd.DataFrame(test_cats.toarray(), columns=cat_cols)\n",
    "                train_df = pd.concat([train_df, train_cats], axis=1)\n",
    "                test_df = pd.concat([test_df, test_cats], axis=1)\n",
    "        scalar = MinMaxScaler()\n",
    "        train_df[features] = scalar.fit_transform(train_df[features])\n",
    "        test_df[features] = scalar.transform(test_df[features])\n",
    "        print(train_df[features].shape)\n",
    "        super().__init__(train_df, test_df, features, categoricals, n_splits, verbose)\n",
    "        \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=(train_set['X'].shape[1],)),\n",
    "            tf.keras.layers.Dense(200, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(100, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(50, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(25, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(1, activation='relu')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=4e-4), loss='mse')\n",
    "        print(model.summary())\n",
    "        save_best = tf.keras.callbacks.ModelCheckpoint('nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "        model.fit(train_set['X'], \n",
    "                train_set['y'], \n",
    "                validation_data=(val_set['X'], val_set['y']),\n",
    "                epochs=100,\n",
    "                 callbacks=[save_best, early_stop])\n",
    "        model.load_weights('nn_model.w8')\n",
    "        return model\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        return None\n",
    "    def set_params(self, ps={}):\n",
    "        return self.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "class Cnn_Model(Base_Model):\n",
    "    \n",
    "    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n",
    "        features = features.copy()\n",
    "        if len(categoricals) > 0:\n",
    "            for cat in categoricals:\n",
    "                enc = OneHotEncoder()\n",
    "                train_cats = enc.fit_transform(train_df[[cat]])\n",
    "                test_cats = enc.transform(test_df[[cat]])\n",
    "                cat_cols = ['{}_{}'.format(cat, str(col)) for col in enc.active_features_]\n",
    "                features += cat_cols\n",
    "                train_cats = pd.DataFrame(train_cats.toarray(), columns=cat_cols)\n",
    "                test_cats = pd.DataFrame(test_cats.toarray(), columns=cat_cols)\n",
    "                train_df = pd.concat([train_df, train_cats], axis=1)\n",
    "                test_df = pd.concat([test_df, test_cats], axis=1)\n",
    "        scalar = MinMaxScaler()\n",
    "        train_df[features] = scalar.fit_transform(train_df[features])\n",
    "        test_df[features] = scalar.transform(test_df[features])\n",
    "        self.create_feat_2d(features)\n",
    "        super().__init__(train_df, test_df, features, categoricals, n_splits, verbose)\n",
    "        \n",
    "    def create_feat_2d(self, features, n_feats_repeat=50):\n",
    "        self.n_feats = len(features)\n",
    "        self.n_feats_repeat = n_feats_repeat\n",
    "        self.mask = np.zeros((self.n_feats_repeat, self.n_feats), dtype=np.int32)\n",
    "        for i in range(self.n_feats_repeat):\n",
    "            l = list(range(self.n_feats))\n",
    "            for j in range(self.n_feats):\n",
    "                c = l.pop(choice(range(len(l))))\n",
    "                self.mask[i, j] = c\n",
    "        self.mask = tf.convert_to_tensor(self.mask)\n",
    "        print(self.mask.shape)\n",
    "       \n",
    "        \n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "\n",
    "        inp = tf.keras.layers.Input(shape=(self.n_feats))\n",
    "        x = tf.keras.layers.Lambda(lambda x: tf.gather(x, self.mask, axis=1))(inp)\n",
    "        x = tf.keras.layers.Reshape((self.n_feats_repeat, self.n_feats, 1))(x)\n",
    "        x = tf.keras.layers.Conv2D(18, (50, 50), strides=50, activation='relu')(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        #x = tf.keras.layers.Dense(200, activation='relu')(x)\n",
    "        #x = tf.keras.layers.LayerNormalization()(x)\n",
    "        #x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
    "        x = tf.keras.layers.LayerNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        x = tf.keras.layers.Dense(50, activation='relu')(x)\n",
    "        x = tf.keras.layers.LayerNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        out = tf.keras.layers.Dense(1)(x)\n",
    "        \n",
    "        model = tf.keras.Model(inp, out)\n",
    "    \n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='mse')\n",
    "        print(model.summary())\n",
    "        save_best = tf.keras.callbacks.ModelCheckpoint('nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "        model.fit(train_set['X'], \n",
    "                train_set['y'], \n",
    "                validation_data=(val_set['X'], val_set['y']),\n",
    "                epochs=100,\n",
    "                 callbacks=[save_best, early_stop])\n",
    "        model.load_weights('nn_model.w8')\n",
    "        return model\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        return None\n",
    "    def set_params(self, ps={}):\n",
    "        return self.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train.csv file....\n",
      "Training.csv file have 11341042 rows and 11 columns\n",
      "Reading test.csv file....\n",
      "Test.csv file have 1156414 rows and 11 columns\n",
      "Reading train_labels.csv file....\n",
      "Train_labels.csv file have 17690 rows and 7 columns\n",
      "Reading specs.csv file....\n",
      "Specs.csv file have 386 rows and 3 columns\n",
      "Reading sample_submission.csv file....\n",
      "Sample_submission.csv file have 1000 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "train, test, train_labels, specs, sample_submission = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get usefull dict with maping encode\n",
    "#train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code = encode_title(train, test, train_labels)\n",
    "train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code, activities_map,all_type_world = encode_title(train, test, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d33acef9afc4fbbb692aa0e031a907a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=17000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# tranform function to get the train and test set\n",
    "reduce_train, reduce_test, categoricals = get_train_and_test(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_train.to_csv('reduce_train.csv', index=False)\n",
    "reduce_test.to_csv('reduce_test.csv', index=False)\n",
    "#categoricals.to_csv('categoricals.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17690, 931)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0026939774600629332\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0026939774600629332"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stract_hists(feature, train=reduce_train, test=reduce_test, adjust=False, plot=False):\n",
    "    n_bins = 10\n",
    "    train_data = train[feature]\n",
    "    test_data = test[feature]\n",
    "    if adjust:\n",
    "        test_data *= train_data.mean() / test_data.mean()\n",
    "    perc_90 = np.percentile(train_data, 95)\n",
    "    train_data = np.clip(train_data, 0, perc_90)\n",
    "    test_data = np.clip(test_data, 0, perc_90)\n",
    "    train_hist = np.histogram(train_data, bins=n_bins)[0] / len(train_data)\n",
    "    test_hist = np.histogram(test_data, bins=n_bins)[0] / len(test_data)\n",
    "    msre = mean_squared_error(train_hist, test_hist)\n",
    "    if plot:\n",
    "        print(msre)\n",
    "        plt.bar(range(n_bins), train_hist, color='blue', alpha=0.5)\n",
    "        plt.bar(range(n_bins), test_hist, color='red', alpha=0.5)\n",
    "        plt.show()\n",
    "    return msre\n",
    "stract_hists('Magma Peak - Level 1_2000', adjust=False, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call feature engineering function\n",
    "features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\n",
    "features = [x for x in features if x not in ['accuracy_group', 'installation_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: FEAT_A: Clip FEAT_B: 27253bdc - Correlation: 0.9999999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: FEAT_A: 2050 FEAT_B: 2040 - Correlation: 0.9965259434878118\n",
      "3: FEAT_A: 2050 FEAT_B: 73757a5e - Correlation: 0.9998050146713992\n",
      "4: FEAT_A: 2050 FEAT_B: 2b9272f4 - Correlation: 0.9999839030068793\n",
      "5: FEAT_A: 2050 FEAT_B: 08fd73f3 - Correlation: 0.9966123918733654\n",
      "6: FEAT_A: 2050 FEAT_B: dcaede90 - Correlation: 0.9965259434878118\n",
      "7: FEAT_A: 2050 FEAT_B: 26fd2d99 - Correlation: 0.9965084543995759\n",
      "8: FEAT_A: 2050 FEAT_B: 37c53127 - Correlation: 1.0\n",
      "9: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_3021 - Correlation: 0.9998050146713992\n",
      "10: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_2020 - Correlation: 0.9965084543995759\n",
      "11: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_2040 - Correlation: 0.9965259434878118\n",
      "12: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_2050 - Correlation: 1.0\n",
      "13: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_2030 - Correlation: 0.9966123918733654\n",
      "14: FEAT_A: 2050 FEAT_B: Scrub-A-Dub_3121 - Correlation: 0.9999839030068793\n",
      "15: FEAT_A: 4230 FEAT_B: 4235 - Correlation: 0.9999995197498746\n",
      "16: FEAT_A: 4230 FEAT_B: ad148f58 - Correlation: 0.9999999999999998\n",
      "17: FEAT_A: 4230 FEAT_B: 85de926c - Correlation: 0.9999995197498746\n",
      "18: FEAT_A: 4230 FEAT_B: Bubble Bath_4230 - Correlation: 0.9999999999999998\n",
      "19: FEAT_A: 4230 FEAT_B: Bubble Bath_4235 - Correlation: 0.9999995197498746\n",
      "20: FEAT_A: 5000 FEAT_B: 5010 - Correlation: 0.9991849213605333\n",
      "21: FEAT_A: 5000 FEAT_B: 71e712d8 - Correlation: 0.9991849213605333\n",
      "22: FEAT_A: 5000 FEAT_B: a6d66e51 - Correlation: 1.0\n",
      "23: FEAT_A: 5000 FEAT_B: Watering Hole (Activity)_5000 - Correlation: 1.0\n",
      "24: FEAT_A: 5000 FEAT_B: Watering Hole (Activity)_5010 - Correlation: 0.9991849213605333\n",
      "25: FEAT_A: 3110 FEAT_B: 3010 - Correlation: 0.9999293402893735\n",
      "26: FEAT_A: 3120 FEAT_B: 3020 - Correlation: 0.9998761417908972\n",
      "27: FEAT_A: 3121 FEAT_B: 3021 - Correlation: 0.9999098200487934\n",
      "28: FEAT_A: 4031 FEAT_B: 1996c610 - Correlation: 1.0\n",
      "29: FEAT_A: 4031 FEAT_B: Dino Drink_4031 - Correlation: 1.0\n",
      "30: FEAT_A: 4050 FEAT_B: a1192f43 - Correlation: 0.9999999999999999\n",
      "31: FEAT_A: 4050 FEAT_B: Crystals Rule_4050 - Correlation: 0.9999999999999999\n",
      "32: FEAT_A: 2020 FEAT_B: 2030 - Correlation: 0.9959933262816534\n",
      "33: FEAT_A: 4220 FEAT_B: 1340b8d7 - Correlation: 1.0\n",
      "34: FEAT_A: 4220 FEAT_B: Bubble Bath_4220 - Correlation: 1.0\n",
      "35: FEAT_A: 6f8106d9 FEAT_B: Dino Drink_4090 - Correlation: 1.0\n",
      "36: FEAT_A: c74f40cd FEAT_B: 9d29771f - Correlation: 0.9999553655332928\n",
      "37: FEAT_A: c74f40cd FEAT_B: 3dfd4aa4 - Correlation: 0.996396898906497\n",
      "38: FEAT_A: c74f40cd FEAT_B: 28ed704e - Correlation: 0.9955185671057012\n",
      "39: FEAT_A: c74f40cd FEAT_B: 83c6c409 - Correlation: 0.9963987221516344\n",
      "40: FEAT_A: c74f40cd FEAT_B: Mushroom Sorter (Assessment)_3021 - Correlation: 0.9999553655332928\n",
      "41: FEAT_A: c74f40cd FEAT_B: Mushroom Sorter (Assessment)_3121 - Correlation: 0.9999999999999999\n",
      "42: FEAT_A: c74f40cd FEAT_B: Mushroom Sorter (Assessment)_4025 - Correlation: 0.9955185671057012\n",
      "43: FEAT_A: c74f40cd FEAT_B: Mushroom Sorter (Assessment)_2035 - Correlation: 0.9963987221516344\n",
      "44: FEAT_A: c74f40cd FEAT_B: Mushroom Sorter (Assessment)_2020 - Correlation: 0.996396898906497\n",
      "45: FEAT_A: e7561dd2 FEAT_B: Pan Balance_4025 - Correlation: 0.9999999999999998\n",
      "46: FEAT_A: 9b01374f FEAT_B: Flower Waterer (Activity)_2000 - Correlation: 1.0\n",
      "47: FEAT_A: 1cf54632 FEAT_B: Bubble Bath_2000 - Correlation: 0.9999999999999999\n",
      "48: FEAT_A: 2c4e6db0 FEAT_B: All Star Sorting_2020 - Correlation: 1.0\n",
      "49: FEAT_A: f28c589a FEAT_B: a1bbe385 - Correlation: 0.9999536797342249\n",
      "50: FEAT_A: f28c589a FEAT_B: Air Show_3110 - Correlation: 0.9999536797342249\n",
      "51: FEAT_A: f28c589a FEAT_B: Air Show_3010 - Correlation: 1.0\n",
      "52: FEAT_A: ec138c1c FEAT_B: Bird Measurer (Assessment)_2020 - Correlation: 0.9999999999999999\n",
      "53: FEAT_A: 5a848010 FEAT_B: Scrub-A-Dub_2080 - Correlation: 1.0\n",
      "54: FEAT_A: 155f62a4 FEAT_B: 5b49460a - Correlation: 0.9999999999999999\n",
      "55: FEAT_A: 155f62a4 FEAT_B: Chest Sorter (Assessment)_2000 - Correlation: 0.9999999999999999\n",
      "56: FEAT_A: 155f62a4 FEAT_B: Chest Sorter (Assessment)_2020 - Correlation: 0.9999999999999999\n",
      "57: FEAT_A: b80e5e84 FEAT_B: 7ab78247 - Correlation: 0.9998336590281085\n",
      "58: FEAT_A: b80e5e84 FEAT_B: Egg Dropper (Activity)_3010 - Correlation: 0.9998336590281085\n",
      "59: FEAT_A: b80e5e84 FEAT_B: Egg Dropper (Activity)_3110 - Correlation: 1.0\n",
      "60: FEAT_A: d2e9262e FEAT_B: 2fb91ec1 - Correlation: 0.9991434495208743\n",
      "61: FEAT_A: d2e9262e FEAT_B: Watering Hole (Activity)_4025 - Correlation: 0.9991434495208743\n",
      "62: FEAT_A: d2e9262e FEAT_B: Watering Hole (Activity)_4020 - Correlation: 0.9999999999999998\n",
      "63: FEAT_A: 895865f3 FEAT_B: c54cf6c5 - Correlation: 0.9964317167511039\n",
      "64: FEAT_A: 895865f3 FEAT_B: d06f75b5 - Correlation: 0.9991496470458585\n",
      "65: FEAT_A: 895865f3 FEAT_B: 3bb91dda - Correlation: 0.9962789226264855\n",
      "66: FEAT_A: 895865f3 FEAT_B: Bubble Bath_2025 - Correlation: 0.9964317167511039\n",
      "67: FEAT_A: 895865f3 FEAT_B: Bubble Bath_2030 - Correlation: 1.0\n",
      "68: FEAT_A: 895865f3 FEAT_B: Bubble Bath_4020 - Correlation: 0.9962789226264855\n",
      "69: FEAT_A: 895865f3 FEAT_B: Bubble Bath_2035 - Correlation: 0.9991496470458585\n",
      "70: FEAT_A: e9c52111 FEAT_B: b7530680 - Correlation: 0.998817689964623\n",
      "71: FEAT_A: e9c52111 FEAT_B: Bottle Filler (Activity)_2030 - Correlation: 1.0\n",
      "72: FEAT_A: e9c52111 FEAT_B: Bottle Filler (Activity)_2020 - Correlation: 0.998817689964623\n",
      "73: FEAT_A: 8fee50e2 FEAT_B: Bird Measurer (Assessment)_4020 - Correlation: 1.0\n",
      "74: FEAT_A: 31973d56 FEAT_B: 5de79a6a - Correlation: 0.9973945339840646\n",
      "75: FEAT_A: 31973d56 FEAT_B: Cart Balancer (Assessment)_3020 - Correlation: 0.9973945339840646\n",
      "76: FEAT_A: 31973d56 FEAT_B: Cart Balancer (Assessment)_3120 - Correlation: 1.0\n",
      "77: FEAT_A: 6cf7d25c FEAT_B: 15f99afc - Correlation: 0.9994848234397947\n",
      "78: FEAT_A: 6cf7d25c FEAT_B: Pan Balance_3110 - Correlation: 0.9994848234397947\n",
      "79: FEAT_A: 6cf7d25c FEAT_B: Pan Balance_3010 - Correlation: 1.0\n",
      "80: FEAT_A: 2dc29e21 FEAT_B: All Star Sorting_4020 - Correlation: 1.0\n",
      "81: FEAT_A: 17113b36 FEAT_B: e37a2b78 - Correlation: 0.9982548122198927\n",
      "82: FEAT_A: 17113b36 FEAT_B: ad2fc29c - Correlation: 0.9988424704659317\n",
      "83: FEAT_A: 17113b36 FEAT_B: Bird Measurer (Assessment)_3020 - Correlation: 0.9988424704659317\n",
      "84: FEAT_A: 17113b36 FEAT_B: Bird Measurer (Assessment)_3120 - Correlation: 0.9982548122198927\n",
      "85: FEAT_A: 17113b36 FEAT_B: Bird Measurer (Assessment)_4110 - Correlation: 0.9999999999999998\n",
      "86: FEAT_A: bb3e370b FEAT_B: Bottle Filler (Activity) - Correlation: 0.9950043311420306\n",
      "87: FEAT_A: bb3e370b FEAT_B: Bottle Filler (Activity)_4030 - Correlation: 0.9999999999999999\n",
      "88: FEAT_A: 562cec5f FEAT_B: Chest Sorter (Assessment)_4025 - Correlation: 1.0\n",
      "89: FEAT_A: e3ff61fb FEAT_B: 709b1251 - Correlation: 0.9995444786291265\n",
      "90: FEAT_A: e3ff61fb FEAT_B: Dino Dive_3021 - Correlation: 1.0\n",
      "91: FEAT_A: e3ff61fb FEAT_B: Dino Dive_3121 - Correlation: 0.9995444786291265\n",
      "92: FEAT_A: 47efca07 FEAT_B: Bottle Filler (Activity)_4090 - Correlation: 1.0\n",
      "93: FEAT_A: ecaab346 FEAT_B: b2e5b0f1 - Correlation: 0.999849464604504\n",
      "94: FEAT_A: ecaab346 FEAT_B: b74258a0 - Correlation: 1.0\n",
      "95: FEAT_A: ecaab346 FEAT_B: Cart Balancer (Assessment)_3121 - Correlation: 1.0\n",
      "96: FEAT_A: ecaab346 FEAT_B: Cart Balancer (Assessment)_2030 - Correlation: 1.0\n",
      "97: FEAT_A: ecaab346 FEAT_B: Cart Balancer (Assessment)_2010 - Correlation: 0.999849464604504\n",
      "98: FEAT_A: a44b10dc FEAT_B: Flower Waterer (Activity)_4070 - Correlation: 1.0\n",
      "99: FEAT_A: 3d8c61b0 FEAT_B: Happy Camel_4030 - Correlation: 0.9999999999999999\n",
      "100: FEAT_A: 44cb4907 FEAT_B: 8b757ab8 - Correlation: 0.999835058794711\n",
      "101: FEAT_A: 44cb4907 FEAT_B: Crystals Rule_3020 - Correlation: 1.0\n",
      "102: FEAT_A: 44cb4907 FEAT_B: Crystals Rule_3120 - Correlation: 0.999835058794711\n",
      "103: FEAT_A: 6c930e6e FEAT_B: a5be6304 - Correlation: 0.9962760616821671\n",
      "104: FEAT_A: 6c930e6e FEAT_B: Mushroom Sorter (Assessment)_2030 - Correlation: 0.9999999999999999\n",
      "105: FEAT_A: 6c930e6e FEAT_B: Mushroom Sorter (Assessment)_2010 - Correlation: 0.9962760616821671\n",
      "106: FEAT_A: 65a38bf7 FEAT_B: 7ad3efc6 - Correlation: 0.9999786265690429\n",
      "107: FEAT_A: 65a38bf7 FEAT_B: Cart Balancer (Assessment)_2000 - Correlation: 0.9999786265690429\n",
      "108: FEAT_A: 65a38bf7 FEAT_B: Cart Balancer (Assessment)_2020 - Correlation: 1.0\n",
      "109: FEAT_A: b120f2ac FEAT_B: c277e121 - Correlation: 0.9999983835744553\n",
      "110: FEAT_A: b120f2ac FEAT_B: d45ed6a1 - Correlation: 0.9979707847816691\n",
      "111: FEAT_A: b120f2ac FEAT_B: All Star Sorting_3020 - Correlation: 0.9999983835744553\n",
      "112: FEAT_A: b120f2ac FEAT_B: All Star Sorting_3120 - Correlation: 0.9979707847816691\n",
      "113: FEAT_A: b120f2ac FEAT_B: All Star Sorting_2025 - Correlation: 1.0\n",
      "114: FEAT_A: 3393b68b FEAT_B: Bird Measurer (Assessment)_2010 - Correlation: 1.0\n",
      "115: FEAT_A: 85d1b0de FEAT_B: Chicken Balancer (Activity)_4090 - Correlation: 1.0\n",
      "116: FEAT_A: 736f9581 FEAT_B: 9b23e8ee - Correlation: 0.9999999999999999\n",
      "117: FEAT_A: 736f9581 FEAT_B: Egg Dropper (Activity)_2000 - Correlation: 0.9999999999999999\n",
      "118: FEAT_A: 736f9581 FEAT_B: Egg Dropper (Activity)_2020 - Correlation: 0.9999999999999999\n",
      "119: FEAT_A: 0330ab6a FEAT_B: 2230fab4 - Correlation: 0.9998673365188063\n",
      "120: FEAT_A: 0330ab6a FEAT_B: Chow Time_3120 - Correlation: 0.9998673365188063\n",
      "121: FEAT_A: 0330ab6a FEAT_B: Chow Time_3020 - Correlation: 1.0\n",
      "122: FEAT_A: 5e3ea25a FEAT_B: Crystals Rule_4070 - Correlation: 1.0\n",
      "123: FEAT_A: 4b5efe37 FEAT_B: b7dc8128 - Correlation: 0.9980151285383981\n",
      "124: FEAT_A: 4b5efe37 FEAT_B: All Star Sorting_2000 - Correlation: 0.9980151285383981\n",
      "125: FEAT_A: 4b5efe37 FEAT_B: All Star Sorting_4010 - Correlation: 1.0\n",
      "126: FEAT_A: d02b7a8e FEAT_B: All Star Sorting_4035 - Correlation: 1.0\n",
      "127: FEAT_A: 9e4c8c7b FEAT_B: 363d3849 - Correlation: 0.9992130941883633\n",
      "128: FEAT_A: 9e4c8c7b FEAT_B: All Star Sorting_3110 - Correlation: 0.9999999999999998\n",
      "129: FEAT_A: 9e4c8c7b FEAT_B: All Star Sorting_3010 - Correlation: 0.9992130941883633\n",
      "130: FEAT_A: 2ec694de FEAT_B: Bug Measurer (Activity)_4080 - Correlation: 0.9999999999999998\n",
      "131: FEAT_A: 7fd1ac25 FEAT_B: Egg Dropper (Activity)_4080 - Correlation: 1.0\n",
      "132: FEAT_A: 8f094001 FEAT_B: 1beb320a - Correlation: 0.9980774800878145\n",
      "133: FEAT_A: 8f094001 FEAT_B: Bubble Bath_4045 - Correlation: 1.0\n",
      "134: FEAT_A: 8f094001 FEAT_B: Bubble Bath_2020 - Correlation: 0.9980774800878145\n",
      "135: FEAT_A: b2dba42b FEAT_B: 1bb5fbdb - Correlation: 0.9999521729413294\n",
      "136: FEAT_A: b2dba42b FEAT_B: Sandcastle Builder (Activity)_3110 - Correlation: 0.9999521729413294\n",
      "137: FEAT_A: b2dba42b FEAT_B: Sandcastle Builder (Activity)_3010 - Correlation: 1.0\n",
      "138: FEAT_A: 19967db1 FEAT_B: Chow Time_4090 - Correlation: 1.0\n",
      "139: FEAT_A: 46cd75b4 FEAT_B: Chicken Balancer (Activity)_4022 - Correlation: 0.9999999999999998\n",
      "140: FEAT_A: 2a512369 FEAT_B: 33505eae - Correlation: 0.9994585292841954\n",
      "141: FEAT_A: 2a512369 FEAT_B: Leaf Leader_3010 - Correlation: 0.9994585292841954\n",
      "142: FEAT_A: 2a512369 FEAT_B: Leaf Leader_3110 - Correlation: 1.0\n",
      "143: FEAT_A: 262136f4 FEAT_B: Leaf Leader_4020 - Correlation: 1.0\n",
      "144: FEAT_A: e4d32835 FEAT_B: Pan Balance_4080 - Correlation: 0.9999999999999998\n",
      "145: FEAT_A: cc5087a3 FEAT_B: Crystals Rule_4010 - Correlation: 1.0\n",
      "146: FEAT_A: ea321fb1 FEAT_B: 84b0e0c8 - Correlation: 0.9993007600205108\n",
      "147: FEAT_A: ea321fb1 FEAT_B: Chicken Balancer (Activity)_3010 - Correlation: 1.0\n",
      "148: FEAT_A: ea321fb1 FEAT_B: Chicken Balancer (Activity)_3110 - Correlation: 0.9993007600205108\n",
      "149: FEAT_A: a592d54e FEAT_B: cf7638f3 - Correlation: 0.9969813738295747\n",
      "150: FEAT_A: a592d54e FEAT_B: 250513af - Correlation: 0.9973020736885158\n",
      "151: FEAT_A: a592d54e FEAT_B: 1c178d24 - Correlation: 0.9973125883100831\n",
      "152: FEAT_A: a592d54e FEAT_B: Pan Balance_2030 - Correlation: 0.9973125883100831\n",
      "153: FEAT_A: a592d54e FEAT_B: Pan Balance_2020 - Correlation: 0.9999999999999999\n",
      "154: FEAT_A: a592d54e FEAT_B: Pan Balance_3121 - Correlation: 0.9969813738295747\n",
      "155: FEAT_A: a592d54e FEAT_B: Pan Balance_3021 - Correlation: 0.9973020736885158\n",
      "156: FEAT_A: bfc77bd6 FEAT_B: Chest Sorter (Assessment)_4080 - Correlation: 0.9999999999999998\n",
      "157: FEAT_A: e64e2cfd FEAT_B: Watering Hole (Activity)_2000 - Correlation: 0.9999999999999998\n",
      "158: FEAT_A: beb0a7b9 FEAT_B: b88f38da - Correlation: 0.9999125179829754\n",
      "159: FEAT_A: beb0a7b9 FEAT_B: Fireworks (Activity)_3010 - Correlation: 1.0\n",
      "160: FEAT_A: beb0a7b9 FEAT_B: Fireworks (Activity)_3110 - Correlation: 0.9999125179829754\n",
      "161: FEAT_A: 9d4e7b25 FEAT_B: Cart Balancer (Assessment)_4040 - Correlation: 1.0\n",
      "162: FEAT_A: a0faea5d FEAT_B: Bubble Bath_4070 - Correlation: 1.0\n",
      "163: FEAT_A: ecc6157f FEAT_B: Cart Balancer (Assessment)_4080 - Correlation: 0.9999999999999999\n",
      "164: FEAT_A: bd612267 FEAT_B: Chest Sorter (Assessment)_4070 - Correlation: 0.9999999999999999\n",
      "165: FEAT_A: c0415e5c FEAT_B: Dino Dive_4020 - Correlation: 1.0\n",
      "166: FEAT_A: 7525289a FEAT_B: 45d01abe - Correlation: 0.9981555049446889\n",
      "167: FEAT_A: 7525289a FEAT_B: Bird Measurer (Assessment)_3021 - Correlation: 0.9981555049446889\n",
      "168: FEAT_A: 7525289a FEAT_B: Bird Measurer (Assessment)_3121 - Correlation: 1.0\n",
      "169: FEAT_A: 3dcdda7f FEAT_B: 3ccd3f02 - Correlation: 0.9977337946782758\n",
      "170: FEAT_A: 3dcdda7f FEAT_B: Chest Sorter (Assessment)_3110 - Correlation: 0.9977337946782758\n",
      "171: FEAT_A: 3dcdda7f FEAT_B: Chest Sorter (Assessment)_3010 - Correlation: 1.0\n",
      "172: FEAT_A: 6bf9e3e1 FEAT_B: Happy Camel_4040 - Correlation: 0.9999999999999999\n",
      "173: FEAT_A: c1cac9a2 FEAT_B: Scrub-A-Dub_2081 - Correlation: 0.9999999999999998\n",
      "174: FEAT_A: 37937459 FEAT_B: Sandcastle Builder (Activity)_4090 - Correlation: 0.9999999999999998\n",
      "175: FEAT_A: f7e47413 FEAT_B: f71c4741 - Correlation: 0.9999426890770878\n",
      "176: FEAT_A: f7e47413 FEAT_B: Scrub-A-Dub_3010 - Correlation: 0.9999426890770878\n",
      "177: FEAT_A: f7e47413 FEAT_B: Scrub-A-Dub_3110 - Correlation: 1.0\n",
      "178: FEAT_A: 99abe2bb FEAT_B: Bubble Bath_2080 - Correlation: 1.0\n",
      "179: FEAT_A: fcfdffb6 FEAT_B: Flower Waterer (Activity)_4022 - Correlation: 1.0\n",
      "180: FEAT_A: 71fe8f75 FEAT_B: 0a08139c - Correlation: 0.9999850342981554\n",
      "181: FEAT_A: 71fe8f75 FEAT_B: Bug Measurer (Activity)_3110 - Correlation: 1.0\n",
      "182: FEAT_A: 71fe8f75 FEAT_B: Bug Measurer (Activity)_3010 - Correlation: 0.9999850342981554\n",
      "183: FEAT_A: 222660ff FEAT_B: e4f1efe6 - Correlation: 0.9984044689083383\n",
      "184: FEAT_A: 222660ff FEAT_B: 38074c54 - Correlation: 0.9999999999999998\n",
      "185: FEAT_A: 222660ff FEAT_B: Chest Sorter (Assessment)_2030 - Correlation: 0.9999999999999998\n",
      "186: FEAT_A: 222660ff FEAT_B: Chest Sorter (Assessment)_2010 - Correlation: 0.9999999999999998\n",
      "187: FEAT_A: 222660ff FEAT_B: Chest Sorter (Assessment)_3121 - Correlation: 0.9984044689083383\n",
      "188: FEAT_A: d2278a3b FEAT_B: Bottle Filler (Activity)_2000 - Correlation: 1.0\n",
      "189: FEAT_A: a16a373e FEAT_B: Bird Measurer (Assessment)_4070 - Correlation: 1.0\n",
      "190: FEAT_A: 67aa2ada FEAT_B: Leaf Leader_4090 - Correlation: 1.0\n",
      "191: FEAT_A: 3babcb9b FEAT_B: 3323d7e9 - Correlation: 0.9952332162875015\n",
      "192: FEAT_A: 3babcb9b FEAT_B: 86c924c4 - Correlation: 0.9988970478897697\n",
      "193: FEAT_A: 3babcb9b FEAT_B: 5154fc30 - Correlation: 0.999986924640935\n",
      "194: FEAT_A: 3babcb9b FEAT_B: 7cf1bc53 - Correlation: 0.9961136759226749\n",
      "195: FEAT_A: 3babcb9b FEAT_B: Crystals Rule_4020 - Correlation: 0.9988970478897697\n",
      "196: FEAT_A: 3babcb9b FEAT_B: Crystals Rule_2030 - Correlation: 0.9952332162875015\n",
      "197: FEAT_A: 3babcb9b FEAT_B: Crystals Rule_3110 - Correlation: 1.0\n",
      "198: FEAT_A: 3babcb9b FEAT_B: Crystals Rule_2020 - Correlation: 0.9961136759226749\n",
      "199: FEAT_A: 3babcb9b FEAT_B: Crystals Rule_3010 - Correlation: 0.999986924640935\n",
      "200: FEAT_A: c952eb01 FEAT_B: Watering Hole (Activity)_4070 - Correlation: 1.0\n",
      "201: FEAT_A: 0ce40006 FEAT_B: Happy Camel_4080 - Correlation: 1.0\n",
      "202: FEAT_A: d3f1e122 FEAT_B: Bottle Filler (Activity)_4035 - Correlation: 1.0\n",
      "203: FEAT_A: 05ad839b FEAT_B: Happy Camel_4090 - Correlation: 1.0\n",
      "204: FEAT_A: 832735e1 FEAT_B: ab3136ba - Correlation: 0.9998637945770242\n",
      "205: FEAT_A: 832735e1 FEAT_B: Dino Dive_3110 - Correlation: 0.9998637945770242\n",
      "206: FEAT_A: 832735e1 FEAT_B: Dino Dive_3010 - Correlation: 1.0\n",
      "207: FEAT_A: 598f4598 FEAT_B: Flower Waterer (Activity)_4025 - Correlation: 0.9999999999999998\n",
      "208: FEAT_A: 30614231 FEAT_B: 37ee8496 - Correlation: 0.9967763987631819\n",
      "209: FEAT_A: 30614231 FEAT_B: Cauldron Filler (Assessment)_4020 - Correlation: 1.0\n",
      "210: FEAT_A: 30614231 FEAT_B: Cauldron Filler (Assessment)_4030 - Correlation: 0.9967763987631819\n",
      "211: FEAT_A: c7fe2a55 FEAT_B: 36fa3ebe - Correlation: 0.9998350234117662\n",
      "212: FEAT_A: c7fe2a55 FEAT_B: a8a78786 - Correlation: 0.9981452039350779\n",
      "213: FEAT_A: c7fe2a55 FEAT_B: Happy Camel_3021 - Correlation: 1.0\n",
      "214: FEAT_A: c7fe2a55 FEAT_B: Happy Camel_3121 - Correlation: 0.9981452039350779\n",
      "215: FEAT_A: c7fe2a55 FEAT_B: Happy Camel_2030 - Correlation: 0.9998350234117662\n",
      "216: FEAT_A: 9de5e594 FEAT_B: 28a4eb9a - Correlation: 0.9995923561196808\n",
      "217: FEAT_A: 9de5e594 FEAT_B: Dino Dive_3020 - Correlation: 1.0\n",
      "218: FEAT_A: 9de5e594 FEAT_B: Dino Dive_3120 - Correlation: 0.9995923561196808\n",
      "219: FEAT_A: 7423acbc FEAT_B: e04fb33d - Correlation: 0.999628997408126\n",
      "220: FEAT_A: 7423acbc FEAT_B: Air Show_3120 - Correlation: 0.999628997408126\n",
      "221: FEAT_A: 7423acbc FEAT_B: Air Show_3020 - Correlation: 1.0\n",
      "222: FEAT_A: 76babcde FEAT_B: Dino Dive_4070 - Correlation: 1.0\n",
      "223: FEAT_A: 4d6737eb FEAT_B: Dino Drink_2070 - Correlation: 1.0\n",
      "224: FEAT_A: 7961e599 FEAT_B: Dino Dive_2020 - Correlation: 0.9999999999999999\n",
      "225: FEAT_A: abc5811c FEAT_B: Happy Camel_4010 - Correlation: 1.0\n",
      "226: FEAT_A: 63f13dd7 FEAT_B: Chow Time_2020 - Correlation: 1.0\n",
      "227: FEAT_A: e7e44842 FEAT_B: Watering Hole (Activity)_4090 - Correlation: 0.9999999999999999\n",
      "228: FEAT_A: db02c830 FEAT_B: 3bfd1a65 - Correlation: 0.9999982205265873\n",
      "229: FEAT_A: db02c830 FEAT_B: Mushroom Sorter (Assessment)_2025 - Correlation: 0.9999999999999999\n",
      "230: FEAT_A: db02c830 FEAT_B: Mushroom Sorter (Assessment)_2000 - Correlation: 0.9999982205265873\n",
      "231: FEAT_A: 7dfe6d8a FEAT_B: Leaf Leader_4070 - Correlation: 0.9999999999999998\n",
      "232: FEAT_A: 3edf6747 FEAT_B: Cauldron Filler (Assessment)_4035 - Correlation: 0.9999999999999999\n",
      "233: FEAT_A: 7040c096 FEAT_B: Scrub-A-Dub_4010 - Correlation: 1.0\n",
      "234: FEAT_A: ac92046e FEAT_B: d88e8f25 - Correlation: 0.9999763070332107\n",
      "235: FEAT_A: ac92046e FEAT_B: Scrub-A-Dub_3020 - Correlation: 0.9999763070332107\n",
      "236: FEAT_A: ac92046e FEAT_B: Scrub-A-Dub_3120 - Correlation: 1.0\n",
      "237: FEAT_A: f56e0afc FEAT_B: Bird Measurer (Assessment)_2000 - Correlation: 1.0\n",
      "238: FEAT_A: 461eace6 FEAT_B: Egg Dropper (Activity)_4020 - Correlation: 1.0\n",
      "239: FEAT_A: 92687c59 FEAT_B: Scrub-A-Dub_4090 - Correlation: 1.0\n",
      "240: FEAT_A: 53c6e11a FEAT_B: Leaf Leader_2075 - Correlation: 0.9999999999999999\n",
      "241: FEAT_A: 4c2ec19f FEAT_B: Egg Dropper (Activity)_4025 - Correlation: 1.0\n",
      "242: FEAT_A: 0086365d FEAT_B: Pan Balance_4010 - Correlation: 0.9999999999999999\n",
      "243: FEAT_A: 499edb7c FEAT_B: Chicken Balancer (Activity)_4020 - Correlation: 1.0\n",
      "244: FEAT_A: 5e109ec3 FEAT_B: Cart Balancer (Assessment)_4030 - Correlation: 1.0\n",
      "245: FEAT_A: 4a09ace1 FEAT_B: Scrub-A-Dub_2083 - Correlation: 1.0\n",
      "246: FEAT_A: 15eb4a7d FEAT_B: 0413e89d - Correlation: 0.9997266832893074\n",
      "247: FEAT_A: 15eb4a7d FEAT_B: Bubble Bath_3010 - Correlation: 0.9997266832893074\n",
      "248: FEAT_A: 15eb4a7d FEAT_B: Bubble Bath_3110 - Correlation: 0.9999999999999998\n",
      "249: FEAT_A: 1cc7cfca FEAT_B: All Star Sorting_4030 - Correlation: 1.0\n",
      "250: FEAT_A: 58a0de5c FEAT_B: f5b8c21a - Correlation: 0.9977888184537715\n",
      "251: FEAT_A: 58a0de5c FEAT_B: 9b4001e4 - Correlation: 0.9999159362216189\n",
      "252: FEAT_A: 58a0de5c FEAT_B: Air Show_3021 - Correlation: 0.9999159362216189\n",
      "253: FEAT_A: 58a0de5c FEAT_B: Air Show_2030 - Correlation: 0.9977888184537715\n",
      "254: FEAT_A: 58a0de5c FEAT_B: Air Show_3121 - Correlation: 1.0\n",
      "255: FEAT_A: c7f7f0e1 FEAT_B: Bug Measurer (Activity)_2000 - Correlation: 1.0\n",
      "256: FEAT_A: 49ed92e9 FEAT_B: bd701df8 - Correlation: 0.9993109138888533\n",
      "257: FEAT_A: 49ed92e9 FEAT_B: Watering Hole (Activity)_3110 - Correlation: 0.9993109138888533\n",
      "258: FEAT_A: 49ed92e9 FEAT_B: Watering Hole (Activity)_3010 - Correlation: 1.0\n",
      "259: FEAT_A: d3640339 FEAT_B: Dino Dive_4090 - Correlation: 1.0\n",
      "260: FEAT_A: 6f4bd64e FEAT_B: Air Show_4090 - Correlation: 0.9999999999999998\n",
      "261: FEAT_A: 28f975ea FEAT_B: Air Show_4020 - Correlation: 1.0\n",
      "262: FEAT_A: 5d042115 FEAT_B: Flower Waterer (Activity)_4030 - Correlation: 1.0\n",
      "263: FEAT_A: 5f0eb72c FEAT_B: Mushroom Sorter (Assessment)_4020 - Correlation: 0.9999999999999998\n",
      "264: FEAT_A: 14de4c5d FEAT_B: Air Show_4100 - Correlation: 1.0\n",
      "265: FEAT_A: 907a054b FEAT_B: c51d8688 - Correlation: 0.9999667370361688\n",
      "266: FEAT_A: 907a054b FEAT_B: Pan Balance_3020 - Correlation: 1.0\n",
      "267: FEAT_A: 907a054b FEAT_B: Pan Balance_3120 - Correlation: 0.9999667370361688\n",
      "268: FEAT_A: 1375ccb7 FEAT_B: bdf49a58 - Correlation: 0.9993801763820348\n",
      "269: FEAT_A: 1375ccb7 FEAT_B: Bird Measurer (Assessment)_3010 - Correlation: 1.0\n",
      "270: FEAT_A: 1375ccb7 FEAT_B: Bird Measurer (Assessment)_3110 - Correlation: 0.9993801763820348\n",
      "271: FEAT_A: 857f21c0 FEAT_B: Bubble Bath_4040 - Correlation: 1.0\n",
      "272: FEAT_A: 0d1da71f FEAT_B: Chow Time_3110 - Correlation: 1.0\n",
      "273: FEAT_A: c7128948 FEAT_B: Mushroom Sorter (Assessment)_4040 - Correlation: 1.0\n",
      "274: FEAT_A: 48349b14 FEAT_B: Crystals Rule_2000 - Correlation: 1.0\n",
      "275: FEAT_A: df4940d3 FEAT_B: 67439901 - Correlation: 0.999935162643595\n",
      "276: FEAT_A: df4940d3 FEAT_B: Bottle Filler (Activity)_3110 - Correlation: 0.9999999999999999\n",
      "277: FEAT_A: df4940d3 FEAT_B: Bottle Filler (Activity)_3010 - Correlation: 0.999935162643595\n",
      "278: FEAT_A: f50fc6c1 FEAT_B: Watering Hole (Activity)_4021 - Correlation: 1.0\n",
      "279: FEAT_A: 91561152 FEAT_B: Cauldron Filler (Assessment)_4025 - Correlation: 1.0\n",
      "280: FEAT_A: ca11f653 FEAT_B: 1f19558b - Correlation: 0.998316427341082\n",
      "281: FEAT_A: ca11f653 FEAT_B: daac11b0 - Correlation: 0.9995251307611354\n",
      "282: FEAT_A: ca11f653 FEAT_B: All Star Sorting_2030 - Correlation: 1.0\n",
      "283: FEAT_A: ca11f653 FEAT_B: All Star Sorting_3021 - Correlation: 0.9995251307611354\n",
      "284: FEAT_A: ca11f653 FEAT_B: All Star Sorting_3121 - Correlation: 0.998316427341082\n",
      "285: FEAT_A: ab4ec3a4 FEAT_B: Dino Drink_4080 - Correlation: 1.0\n",
      "286: FEAT_A: 02a42007 FEAT_B: Fireworks (Activity)_4030 - Correlation: 1.0\n",
      "287: FEAT_A: a8efe47b FEAT_B: Chest Sorter (Assessment)_4030 - Correlation: 1.0\n",
      "288: FEAT_A: 9ce586dd FEAT_B: Chest Sorter (Assessment)_4035 - Correlation: 1.0\n",
      "289: FEAT_A: 160654fd FEAT_B: 88d4a5be - Correlation: 0.9989297172615879\n",
      "290: FEAT_A: 160654fd FEAT_B: Mushroom Sorter (Assessment)_3120 - Correlation: 0.9989297172615879\n",
      "291: FEAT_A: 160654fd FEAT_B: Mushroom Sorter (Assessment)_3020 - Correlation: 1.0\n",
      "292: FEAT_A: 3ddc79c3 FEAT_B: e720d930 - Correlation: 0.9998920962508026\n",
      "293: FEAT_A: 3ddc79c3 FEAT_B: Crystals Rule_3121 - Correlation: 0.9998920962508026\n",
      "294: FEAT_A: 3ddc79c3 FEAT_B: Crystals Rule_3021 - Correlation: 0.9999999999999998\n",
      "295: FEAT_A: 828e68f9 FEAT_B: Cart Balancer (Assessment)_3110 - Correlation: 1.0\n",
      "296: FEAT_A: 8af75982 FEAT_B: Happy Camel_4020 - Correlation: 1.0\n",
      "297: FEAT_A: e694a35b FEAT_B: Fireworks (Activity)_4020 - Correlation: 1.0\n",
      "298: FEAT_A: 51311d7a FEAT_B: Dino Drink_2000 - Correlation: 1.0\n",
      "299: FEAT_A: 7d093bf9 FEAT_B: Chow Time_2000 - Correlation: 0.9999999999999999\n",
      "300: FEAT_A: 3afde5dd FEAT_B: b012cd7f - Correlation: 0.9999689260314981\n",
      "301: FEAT_A: 3afde5dd FEAT_B: e5c9df6f - Correlation: 0.9990910601838011\n",
      "302: FEAT_A: 3afde5dd FEAT_B: Leaf Leader_2030 - Correlation: 0.9999689260314981\n",
      "303: FEAT_A: 3afde5dd FEAT_B: Leaf Leader_3021 - Correlation: 1.0\n",
      "304: FEAT_A: 3afde5dd FEAT_B: Leaf Leader_3121 - Correlation: 0.9990910601838011\n",
      "305: FEAT_A: 9c5ef70c FEAT_B: Pan Balance_2000 - Correlation: 1.0\n",
      "306: FEAT_A: 7f0836bf FEAT_B: a29c5338 - Correlation: 0.9986531654717627\n",
      "307: FEAT_A: 7f0836bf FEAT_B: Dino Drink_3110 - Correlation: 1.0\n",
      "308: FEAT_A: 7f0836bf FEAT_B: Dino Drink_3010 - Correlation: 0.9986531654717627\n",
      "309: FEAT_A: 392e14df FEAT_B: Cauldron Filler (Assessment)_4100 - Correlation: 1.0\n",
      "310: FEAT_A: 9ee1c98c FEAT_B: Sandcastle Builder (Activity)_4021 - Correlation: 0.9999999999999999\n",
      "311: FEAT_A: 25fa8af4 FEAT_B: Mushroom Sorter (Assessment)_4100 - Correlation: 1.0\n",
      "312: FEAT_A: 08ff79ad FEAT_B: Egg Dropper (Activity)_4090 - Correlation: 1.0\n",
      "313: FEAT_A: 13f56524 FEAT_B: Mushroom Sorter (Assessment)_4080 - Correlation: 1.0\n",
      "314: FEAT_A: 16667cc5 FEAT_B: Chicken Balancer (Activity)_4080 - Correlation: 0.9999999999999999\n",
      "315: FEAT_A: 6f445b57 FEAT_B: Chow Time_4080 - Correlation: 1.0\n",
      "316: FEAT_A: 89aace00 FEAT_B: e5734469 - Correlation: 0.9998406115110345\n",
      "317: FEAT_A: 89aace00 FEAT_B: Dino Drink_3020 - Correlation: 0.9998406115110345\n",
      "318: FEAT_A: 89aace00 FEAT_B: Dino Drink_3120 - Correlation: 1.0\n",
      "319: FEAT_A: 37db1c2f FEAT_B: Happy Camel_4045 - Correlation: 1.0\n",
      "320: FEAT_A: 6088b756 FEAT_B: Dino Dive_2070 - Correlation: 1.0\n",
      "321: FEAT_A: 29a42aea FEAT_B: Bubble Bath_4080 - Correlation: 1.0\n",
      "322: FEAT_A: 119b5b02 FEAT_B: Dino Dive_4080 - Correlation: 1.0\n",
      "323: FEAT_A: 90d848e0 FEAT_B: Cauldron Filler (Assessment)_2000 - Correlation: 1.0\n",
      "324: FEAT_A: 29bdd9ba FEAT_B: Dino Dive_2000 - Correlation: 1.0\n",
      "325: FEAT_A: 01ca3a3c FEAT_B: Leaf Leader_4080 - Correlation: 0.9999999999999999\n",
      "326: FEAT_A: 3d0b9317 FEAT_B: Chest Sorter (Assessment)_4040 - Correlation: 1.0\n",
      "327: FEAT_A: 93edfe2e FEAT_B: Crystals Rule_4090 - Correlation: 1.0\n",
      "328: FEAT_A: 3b2048ee FEAT_B: Leaf Leader_4095 - Correlation: 1.0\n",
      "329: FEAT_A: e57dd7af FEAT_B: 763fc34e - Correlation: 0.9972721980394412\n",
      "330: FEAT_A: e57dd7af FEAT_B: Leaf Leader_3020 - Correlation: 0.9972721980394412\n",
      "331: FEAT_A: e57dd7af FEAT_B: Leaf Leader_3120 - Correlation: 1.0\n",
      "332: FEAT_A: 611485c5 FEAT_B: Fireworks (Activity)_4080 - Correlation: 1.0\n",
      "333: FEAT_A: c189aaf2 FEAT_B: Happy Camel_2083 - Correlation: 0.9999999999999999\n",
      "334: FEAT_A: cb6010f8 FEAT_B: 47026d5f - Correlation: 0.9996901555713447\n",
      "335: FEAT_A: cb6010f8 FEAT_B: 56817e2b - Correlation: 0.999047573942978\n",
      "336: FEAT_A: cb6010f8 FEAT_B: Chow Time_2030 - Correlation: 0.999047573942978\n",
      "337: FEAT_A: cb6010f8 FEAT_B: Chow Time_3121 - Correlation: 0.9999999999999999\n",
      "338: FEAT_A: cb6010f8 FEAT_B: Chow Time_3021 - Correlation: 0.9996901555713447\n",
      "339: FEAT_A: eb2c19cd FEAT_B: Mushroom Sorter (Assessment)_4090 - Correlation: 1.0\n",
      "340: FEAT_A: 84538528 FEAT_B: Sandcastle Builder (Activity)_4020 - Correlation: 1.0\n",
      "341: FEAT_A: 2dcad279 FEAT_B: 923afab1 - Correlation: 0.9998567985670083\n",
      "342: FEAT_A: 2dcad279 FEAT_B: Cauldron Filler (Assessment)_3010 - Correlation: 0.9998567985670083\n",
      "343: FEAT_A: 2dcad279 FEAT_B: Cauldron Filler (Assessment)_3110 - Correlation: 0.9999999999999999\n",
      "344: FEAT_A: b5053438 FEAT_B: d3268efa - Correlation: 0.999576326704631\n",
      "345: FEAT_A: b5053438 FEAT_B: 28520915 - Correlation: 0.9990905166101879\n",
      "346: FEAT_A: b5053438 FEAT_B: Cauldron Filler (Assessment)_3121 - Correlation: 1.0\n",
      "347: FEAT_A: b5053438 FEAT_B: Cauldron Filler (Assessment)_3021 - Correlation: 0.999576326704631\n",
      "348: FEAT_A: b5053438 FEAT_B: Cauldron Filler (Assessment)_2030 - Correlation: 0.9990905166101879\n",
      "349: FEAT_A: 731c0cbe FEAT_B: Bird Measurer (Assessment)_4090 - Correlation: 1.0\n",
      "350: FEAT_A: 363c86c9 FEAT_B: Bug Measurer (Activity)_4035 - Correlation: 1.0\n",
      "351: FEAT_A: 29f54413 FEAT_B: Leaf Leader_2060 - Correlation: 0.9999999999999998\n",
      "352: FEAT_A: 7372e1a5 FEAT_B: Chow Time_4070 - Correlation: 1.0\n",
      "353: FEAT_A: 5290eab1 FEAT_B: 04df9b66 - Correlation: 0.9998190477466209\n",
      "354: FEAT_A: 5290eab1 FEAT_B: Cauldron Filler (Assessment)_3020 - Correlation: 0.9998190477466209\n",
      "355: FEAT_A: 5290eab1 FEAT_B: Cauldron Filler (Assessment)_3120 - Correlation: 1.0\n",
      "356: FEAT_A: 565a3990 FEAT_B: Bug Measurer (Activity)_4070 - Correlation: 1.0\n",
      "357: FEAT_A: 93b353f2 FEAT_B: Chest Sorter (Assessment)_4100 - Correlation: 0.9999999999999998\n",
      "358: FEAT_A: c2baf0bd FEAT_B: Happy Camel_2020 - Correlation: 1.0\n",
      "359: FEAT_A: 795e4a37 FEAT_B: Cart Balancer (Assessment)_3010 - Correlation: 1.0\n",
      "360: FEAT_A: cb1178ad FEAT_B: Chest Sorter (Assessment)_4090 - Correlation: 1.0\n",
      "361: FEAT_A: 9e34ea74 FEAT_B: Egg Dropper (Activity)_4070 - Correlation: 1.0\n",
      "362: FEAT_A: 87d743c1 FEAT_B: Dino Dive_4010 - Correlation: 1.0\n",
      "363: FEAT_A: 3d63345e FEAT_B: Cart Balancer (Assessment)_4035 - Correlation: 1.0\n",
      "364: FEAT_A: ecc36b7f FEAT_B: Bubble Bath_4095 - Correlation: 1.0\n",
      "365: FEAT_A: e79f3763 FEAT_B: Bug Measurer (Activity)_4030 - Correlation: 1.0\n",
      "366: FEAT_A: a8876db3 FEAT_B: Cart Balancer (Assessment)_3021 - Correlation: 1.0\n",
      "367: FEAT_A: 3bb91ced FEAT_B: Happy Camel_2081 - Correlation: 1.0\n",
      "368: FEAT_A: 4e5fc6f5 FEAT_B: Cart Balancer (Assessment)_4090 - Correlation: 1.0\n",
      "369: FEAT_A: cdd22e43 FEAT_B: Chicken Balancer (Activity)_4035 - Correlation: 1.0\n",
      "370: FEAT_A: a5e9da97 FEAT_B: Pan Balance_4100 - Correlation: 1.0\n",
      "371: FEAT_A: 77261ab5 FEAT_B: Sandcastle Builder (Activity)_2000 - Correlation: 0.9999999999999999\n",
      "372: FEAT_A: 4ef8cdd3 FEAT_B: Chow Time_4020 - Correlation: 1.0\n",
      "373: FEAT_A: 90efca10 FEAT_B: Bottle Filler (Activity)_4020 - Correlation: 1.0\n",
      "374: FEAT_A: 2b058fe3 FEAT_B: Cauldron Filler (Assessment)_2010 - Correlation: 1.0\n",
      "375: FEAT_A: 6f4adc4b FEAT_B: 55115cbd - Correlation: 0.9997831892615398\n",
      "376: FEAT_A: 6f4adc4b FEAT_B: Bubble Bath_3121 - Correlation: 0.9997831892615398\n",
      "377: FEAT_A: 6f4adc4b FEAT_B: Bubble Bath_3021 - Correlation: 1.0\n",
      "378: FEAT_A: 1575e76c FEAT_B: Air Show_2020 - Correlation: 1.0\n",
      "379: FEAT_A: 15ba1109 FEAT_B: Air Show_2000 - Correlation: 1.0\n",
      "380: FEAT_A: 8d7e386c FEAT_B: 69fdac0a - Correlation: 0.9996590210382708\n",
      "381: FEAT_A: 8d7e386c FEAT_B: Happy Camel_3110 - Correlation: 0.9996590210382708\n",
      "382: FEAT_A: 8d7e386c FEAT_B: Happy Camel_3010 - Correlation: 1.0\n",
      "383: FEAT_A: 9ed8f6da FEAT_B: Dino Drink_2075 - Correlation: 0.9999999999999999\n",
      "384: FEAT_A: 77c76bc5 FEAT_B: Cauldron Filler (Assessment)_4090 - Correlation: 1.0\n",
      "385: FEAT_A: 1325467d FEAT_B: Sandcastle Builder (Activity)_4070 - Correlation: 1.0\n",
      "386: FEAT_A: 5c2f29ca FEAT_B: Cart Balancer (Assessment)_4020 - Correlation: 1.0\n",
      "387: FEAT_A: 756e5507 FEAT_B: Chicken Balancer (Activity)_2000 - Correlation: 1.0\n",
      "388: FEAT_A: a1e4395d FEAT_B: a52b92d5 - Correlation: 0.9991003891313368\n",
      "389: FEAT_A: a1e4395d FEAT_B: Mushroom Sorter (Assessment)_3010 - Correlation: 1.0\n",
      "390: FEAT_A: a1e4395d FEAT_B: Mushroom Sorter (Assessment)_3110 - Correlation: 0.9991003891313368\n",
      "391: FEAT_A: 8d748b58 FEAT_B: Bug Measurer (Activity)_4090 - Correlation: 0.9999999999999998\n",
      "392: FEAT_A: a76029ee FEAT_B: Bird Measurer (Assessment)_4040 - Correlation: 0.9999999999999999\n",
      "393: FEAT_A: 6d90d394 FEAT_B: Scrub-A-Dub_2000 - Correlation: 0.9999999999999999\n",
      "394: FEAT_A: de26c3a6 FEAT_B: Flower Waterer (Activity)_4020 - Correlation: 0.9999999999999999\n",
      "395: FEAT_A: 070a5291 FEAT_B: Bird Measurer (Assessment)_4100 - Correlation: 1.0\n",
      "396: FEAT_A: 9e6b7fb5 FEAT_B: Chow Time_4095 - Correlation: 0.9999999999999998\n",
      "397: FEAT_A: 9554a50b FEAT_B: Cauldron Filler (Assessment)_4080 - Correlation: 0.9999999999999999\n",
      "398: FEAT_A: 56cd3b43 FEAT_B: bbfe0445 - Correlation: 0.9996926215355526\n",
      "399: FEAT_A: 56cd3b43 FEAT_B: Flower Waterer (Activity)_3110 - Correlation: 0.9996926215355526\n",
      "400: FEAT_A: 56cd3b43 FEAT_B: Flower Waterer (Activity)_3010 - Correlation: 1.0\n",
      "401: FEAT_A: 2a444e03 FEAT_B: Pan Balance_4030 - Correlation: 1.0\n",
      "402: FEAT_A: c6971acf FEAT_B: Dino Drink_2060 - Correlation: 0.9999999999999999\n",
      "403: FEAT_A: f32856e4 FEAT_B: Leaf Leader_2020 - Correlation: 1.0\n",
      "404: FEAT_A: d122731b FEAT_B: Cart Balancer (Assessment)_4100 - Correlation: 0.9999999999999998\n",
      "405: FEAT_A: 15a43e5b FEAT_B: Bottle Filler (Activity)_4070 - Correlation: 1.0\n",
      "406: FEAT_A: cfbd47c8 FEAT_B: Chow Time_4030 - Correlation: 1.0\n",
      "407: FEAT_A: 51102b85 FEAT_B: Bird Measurer (Assessment)_4030 - Correlation: 1.0\n",
      "408: FEAT_A: 6c517a88 FEAT_B: Dino Drink_4070 - Correlation: 1.0\n",
      "409: FEAT_A: 46b50ba8 FEAT_B: Happy Camel_4095 - Correlation: 0.9999999999999998\n",
      "410: FEAT_A: 792530f8 FEAT_B: Dino Drink_4030 - Correlation: 1.0\n",
      "411: FEAT_A: 4901243f FEAT_B: Fireworks (Activity)_2000 - Correlation: 1.0\n",
      "412: FEAT_A: d88ca108 FEAT_B: Air Show_2070 - Correlation: 1.0\n",
      "413: FEAT_A: 77ead60d FEAT_B: 4d911100 - Correlation: 0.9998475927724766\n",
      "414: FEAT_A: 77ead60d FEAT_B: 16dffff1 - Correlation: 0.9984674845132689\n",
      "415: FEAT_A: 77ead60d FEAT_B: Dino Drink_3021 - Correlation: 0.9999999999999999\n",
      "416: FEAT_A: 77ead60d FEAT_B: Dino Drink_3121 - Correlation: 0.9998475927724766\n",
      "417: FEAT_A: 77ead60d FEAT_B: Dino Drink_2030 - Correlation: 0.9984674845132689\n",
      "418: FEAT_A: 3ee399c3 FEAT_B: Cauldron Filler (Assessment)_4070 - Correlation: 1.0\n",
      "419: FEAT_A: 47f43a44 FEAT_B: Flower Waterer (Activity)_4090 - Correlation: 1.0\n",
      "420: FEAT_A: 8d84fa81 FEAT_B: Bubble Bath_4010 - Correlation: 1.0\n",
      "421: FEAT_A: 7da34a02 FEAT_B: Mushroom Sorter (Assessment)_4070 - Correlation: 0.9999999999999998\n",
      "422: FEAT_A: 5c3d2b2f FEAT_B: Scrub-A-Dub_4020 - Correlation: 1.0\n",
      "423: FEAT_A: 56bcd38d FEAT_B: Chicken Balancer (Activity)_4030 - Correlation: 0.9999999999999998\n",
      "424: FEAT_A: c58186bf FEAT_B: Sandcastle Builder (Activity)_4035 - Correlation: 1.0\n",
      "425: FEAT_A: 5348fd84 FEAT_B: Cauldron Filler (Assessment)_4040 - Correlation: 1.0\n",
      "426: FEAT_A: 7d5c30a2 FEAT_B: Dino Dive_2060 - Correlation: 1.0\n",
      "427: FEAT_A: 3a4be871 FEAT_B: Flower Waterer (Activity)_4080 - Correlation: 1.0\n",
      "428: FEAT_A: 06372577 FEAT_B: Air Show_2060 - Correlation: 1.0\n",
      "429: FEAT_A: 99ea62f3 FEAT_B: Bubble Bath_2083 - Correlation: 1.0\n",
      "430: FEAT_A: 30df3273 FEAT_B: Sandcastle Builder (Activity)_4080 - Correlation: 1.0\n",
      "431: FEAT_A: f3cd5473 FEAT_B: Pan Balance_4070 - Correlation: 1.0\n",
      "432: FEAT_A: f93fc684 FEAT_B: Chow Time_4010 - Correlation: 0.9999999999999999\n",
      "433: FEAT_A: 6043a2b4 FEAT_B: All Star Sorting_4090 - Correlation: 0.9999999999999999\n",
      "434: FEAT_A: 0d18d96c FEAT_B: Mushroom Sorter (Assessment)_4035 - Correlation: 1.0\n",
      "435: FEAT_A: 884228c8 FEAT_B: Fireworks (Activity)_4070 - Correlation: 1.0\n",
      "436: FEAT_A: 3afb49e6 FEAT_B: Chest Sorter (Assessment)_3021 - Correlation: 1.0\n",
      "437: FEAT_A: 65abac75 FEAT_B: Air Show_4010 - Correlation: 1.0\n",
      "438: FEAT_A: 6aeafed4 FEAT_B: Bubble Bath_4090 - Correlation: 1.0\n",
      "439: FEAT_A: b1d5101d FEAT_B: All Star Sorting_4095 - Correlation: 1.0\n",
      "440: FEAT_A: 00c73085 FEAT_B: Dino Dive_2030 - Correlation: 0.9999999999999999\n",
      "441: FEAT_A: 26a5a3dd FEAT_B: All Star Sorting_4080 - Correlation: 1.0\n",
      "442: FEAT_A: bcceccc6 FEAT_B: Air Show_4070 - Correlation: 1.0\n",
      "443: FEAT_A: 022b4259 FEAT_B: Bug Measurer (Activity)_4025 - Correlation: 0.9999999999999998\n",
      "444: FEAT_A: 587b5989 FEAT_B: All Star Sorting_4070 - Correlation: 1.0\n",
      "445: FEAT_A: e080a381 FEAT_B: Pan Balance_4090 - Correlation: 1.0\n",
      "446: FEAT_A: 5859dfb6 FEAT_B: 90ea0bac - Correlation: 0.998105247261057\n",
      "447: FEAT_A: 5859dfb6 FEAT_B: Bubble Bath_3020 - Correlation: 0.998105247261057\n",
      "448: FEAT_A: 5859dfb6 FEAT_B: Bubble Bath_3120 - Correlation: 0.9999999999999998\n",
      "449: FEAT_A: a2df0760 FEAT_B: Happy Camel_4035 - Correlation: 0.9999999999999999\n",
      "450: FEAT_A: fbaf3456 FEAT_B: Mushroom Sorter (Assessment)_4030 - Correlation: 0.9999999999999999\n",
      "451: FEAT_A: f6947f54 FEAT_B: Bird Measurer (Assessment)_2030 - Correlation: 1.0\n",
      "452: FEAT_A: d2659ab4 FEAT_B: Air Show_2075 - Correlation: 1.0\n",
      "453: FEAT_A: d51b1749 FEAT_B: Happy Camel_2080 - Correlation: 1.0\n",
      "454: FEAT_A: 1af8be29 FEAT_B: 3bf1cf26 - Correlation: 0.9998900847287077\n",
      "455: FEAT_A: 1af8be29 FEAT_B: Happy Camel_3120 - Correlation: 0.9998900847287077\n",
      "456: FEAT_A: 1af8be29 FEAT_B: Happy Camel_3020 - Correlation: 0.9999999999999999\n",
      "457: FEAT_A: d185d3ea FEAT_B: Chow Time_4035 - Correlation: 1.0\n",
      "458: FEAT_A: cf82af56 FEAT_B: Scrub-A-Dub_4070 - Correlation: 1.0\n",
      "459: FEAT_A: f54238ee FEAT_B: Fireworks (Activity)_4090 - Correlation: 1.0\n",
      "460: FEAT_A: 804ee27f FEAT_B: Pan Balance_4020 - Correlation: 1.0\n",
      "461: FEAT_A: 4a4c3d21 FEAT_B: Bird Measurer (Assessment)_4025 - Correlation: 1.0\n",
      "462: FEAT_A: acf5c23f FEAT_B: Cart Balancer (Assessment)_4070 - Correlation: 1.0\n",
      "463: FEAT_A: 8ac7cce4 FEAT_B: Leaf Leader_2000 - Correlation: 1.0\n",
      "464: FEAT_A: df4fe8b6 FEAT_B: ea296733 - Correlation: 0.9972489515829078\n",
      "465: FEAT_A: df4fe8b6 FEAT_B: Chest Sorter (Assessment)_3120 - Correlation: 1.0\n",
      "466: FEAT_A: df4fe8b6 FEAT_B: Chest Sorter (Assessment)_3020 - Correlation: 0.9972489515829078\n",
      "467: FEAT_A: df4fe8b6 FEAT_B: chest_assessment_uncorrect_sum - Correlation: 1.0\n",
      "468: FEAT_A: 86ba578b FEAT_B: Leaf Leader_2070 - Correlation: 1.0\n",
      "469: FEAT_A: dcb55a27 FEAT_B: Air Show_4110 - Correlation: 1.0\n",
      "470: FEAT_A: 5be391b5 FEAT_B: Dino Drink_4010 - Correlation: 1.0\n",
      "471: FEAT_A: fd20ea40 FEAT_B: Leaf Leader_4010 - Correlation: 0.9999999999999998\n",
      "472: FEAT_A: d9c005dd FEAT_B: Happy Camel_2000 - Correlation: 1.0\n",
      "473: FEAT_A: 4bb2f698 FEAT_B: Chicken Balancer (Activity)_4070 - Correlation: 0.9999999999999998\n",
      "474: FEAT_A: 1b54d27f FEAT_B: Watering Hole (Activity)_2010 - Correlation: 1.0\n",
      "475: FEAT_A: d38c2fd7 FEAT_B: Bird Measurer (Assessment)_4035 - Correlation: 0.9999999999999999\n",
      "476: FEAT_A: f806dc10 FEAT_B: Dino Drink_2020 - Correlation: 1.0\n",
      "477: FEAT_A: 532a2afb FEAT_B: Cauldron Filler (Assessment)_2020 - Correlation: 1.0\n",
      "478: FEAT_A: 5e812b27 FEAT_B: Sandcastle Builder (Activity)_4030 - Correlation: 1.0\n",
      "479: FEAT_A: bc8f2793 FEAT_B: Pan Balance_4035 - Correlation: 0.9999999999999999\n",
      "480: FEAT_A: a7640a16 FEAT_B: Happy Camel_4070 - Correlation: 1.0\n",
      "481: FEAT_A: 5f5b2617 FEAT_B: Bottle Filler (Activity)_4080 - Correlation: 1.0\n",
      "482: FEAT_A: 0db6d71d FEAT_B: Chest Sorter (Assessment)_4020 - Correlation: 1.0\n",
      "483: FEAT_A: 6077cc36 FEAT_B: Bird Measurer (Assessment)_4080 - Correlation: 0.9999999999999998\n",
      "484: FEAT_A: 7ec0c298 FEAT_B: Chow Time_3010 - Correlation: 0.9999999999999999\n",
      "485: FEAT_A: 74e5f8a7 FEAT_B: Dino Drink_4020 - Correlation: 1.0\n",
      "486: FEAT_A: Rulers FEAT_B: Rulers_2000 - Correlation: 1.0\n",
      "487: FEAT_A: Slop Problem FEAT_B: Slop Problem_2000 - Correlation: 1.0\n",
      "488: FEAT_A: Pirate's Tale FEAT_B: Pirate's Tale_2000 - Correlation: 0.9999999999999999\n",
      "489: FEAT_A: Crystal Caves - Level 3 FEAT_B: Crystal Caves - Level 3_2000 - Correlation: 1.0\n",
      "490: FEAT_A: Crystal Caves - Level 2 FEAT_B: Crystal Caves - Level 2_2000 - Correlation: 1.0\n",
      "491: FEAT_A: Lifting Heavy Things FEAT_B: Lifting Heavy Things_2000 - Correlation: 0.9999999999999999\n",
      "492: FEAT_A: Costume Box FEAT_B: Costume Box_2000 - Correlation: 1.0\n",
      "493: FEAT_A: Treasure Map FEAT_B: Treasure Map_2000 - Correlation: 1.0\n",
      "494: FEAT_A: Crystal Caves - Level 1 FEAT_B: Crystal Caves - Level 1_2000 - Correlation: 1.0\n",
      "495: FEAT_A: Magma Peak - Level 2 FEAT_B: Magma Peak - Level 2_2000 - Correlation: 1.0\n",
      "496: FEAT_A: 12 Monkeys FEAT_B: 12 Monkeys_2000 - Correlation: 1.0\n",
      "497: FEAT_A: Ordering Spheres FEAT_B: Ordering Spheres_2000 - Correlation: 1.0\n",
      "498: FEAT_A: Welcome to Lost Lagoon! FEAT_B: Welcome to Lost Lagoon!_2000 - Correlation: 1.0\n",
      "499: FEAT_A: Welcome to Lost Lagoon! FEAT_B: Clip_NONE - Correlation: 1.0\n",
      "500: FEAT_A: Tree Top City - Level 1 FEAT_B: Tree Top City - Level 1_2000 - Correlation: 0.9999999999999999\n",
      "501: FEAT_A: Cauldron Filler (Assessment) FEAT_B: Assessment_MAGMAPEAK - Correlation: 1.0\n",
      "502: FEAT_A: Honey Cake FEAT_B: Honey Cake_2000 - Correlation: 1.0\n",
      "503: FEAT_A: Magma Peak - Level 1 FEAT_B: Magma Peak - Level 1_2000 - Correlation: 1.0\n",
      "504: FEAT_A: Balancing Act FEAT_B: Balancing Act_2000 - Correlation: 1.0\n",
      "505: FEAT_A: Heavy, Heavier, Heaviest FEAT_B: Heavy, Heavier, Heaviest_2000 - Correlation: 1.0\n",
      "506: FEAT_A: Tree Top City - Level 2 FEAT_B: Tree Top City - Level 2_2000 - Correlation: 1.0\n",
      "507: FEAT_A: Tree Top City - Level 3 FEAT_B: Tree Top City - Level 3_2000 - Correlation: 1.0\n",
      "508: FEAT_A: var_event_id FEAT_B: var_title_event_code - Correlation: 0.9995254184300616\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "to_remove = []\n",
    "for feat_a in features:\n",
    "    for feat_b in features:\n",
    "        if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n",
    "            c = np.corrcoef(reduce_train[feat_a], reduce_train[feat_b])[0][1]\n",
    "            if c > 0.995:\n",
    "                counter += 1\n",
    "                to_remove.append(feat_b)\n",
    "                print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_Cart Balancer (Assessment) -0.04020325710970116 -0.47065833333333346 0.006732930476733109\n",
      "003cd2ee 0.0 0.0\n",
      "2ec694de 0.008988128886376484 0.0\n",
      "7fd1ac25 0.01978518937252685 0.0\n",
      "e4d32835 0.0013001695873374789 0.0\n",
      "bfc77bd6 0.012832108535895986 0.0\n",
      "ecc6157f 0.007292255511588468 0.0\n",
      "17ca3959 0.0 0.0\n",
      "0ce40006 0.0008479366873940079 0.0\n",
      "dcb1663e 0.0 0.0\n",
      "ab4ec3a4 0.0009044657998869418 0.0\n",
      "4074bac2 0.0 0.0\n",
      "13f56524 0.04392312040700961 0.0\n",
      "29a42aea 0.004070096099491238 0.0\n",
      "119b5b02 0.0002826455624646693 0.0\n",
      "01ca3a3c 0.0004522328999434709 0.0\n",
      "611485c5 0.0013566986998304127 0.0\n",
      "eb2c19cd 0.17382702091577162 0.008 0.0\n",
      "a8cc6fec 0.0 0.0\n",
      "6aeafed4 0.14703222159412097 0.008 0.0\n",
      "5dc079d8 0.0 0.0\n",
      "1b54d27f 0.0007348784624081402 0.0\n",
      "Clip_gametime 0.0 0.0\n",
      "Bubble Bath_4080 0.004070096099491238 0.0\n",
      "Bug Measurer (Activity)_4080 0.008988128886376484 0.0\n",
      "Chest Sorter (Assessment)_4080 0.012832108535895986 0.0\n",
      "Happy Camel_4080 0.0008479366873940079 0.0\n",
      "Air Show_4080 0.0 0.0\n",
      "Pan Balance_4080 0.0013001695873374789 0.0\n",
      "Crystals Rule_2010 0.0 0.0\n",
      "Mushroom Sorter (Assessment)_4080 0.04392312040700961 0.0\n",
      "Cart Balancer (Assessment)_4080 0.007292255511588468 0.0\n",
      "Mushroom Sorter (Assessment)_4090 0.17382702091577162 0.008 0.0\n",
      "Egg Dropper (Activity)_4080 0.01978518937252685 0.0\n",
      "Watering Hole (Activity)_2010 0.0007348784624081402 0.0\n",
      "Dino Dive_4080 0.0002826455624646693 0.0\n",
      "Leaf Leader_4080 0.0004522328999434709 0.0\n",
      "Bubble Bath_4090 0.14703222159412097 0.008 0.0\n",
      "Sandcastle Builder (Activity)_2010 0.0 0.0\n",
      "Dino Drink_4080 0.0009044657998869418 0.0\n",
      "Bottle Filler (Activity)_2010 0.0 0.0\n",
      "Scrub-A-Dub_4080 0.0 0.0\n",
      "Fireworks (Activity)_4080 0.0013566986998304127 0.0\n",
      "Pan Balance_2010 0.0 0.0\n",
      "Cauldron Filler (Assessment)_4020_accuracy 2.303730460456184e+210 5.080517549278432e+102 0.010621630739566321\n",
      "Mushroom Sorter (Assessment)_4020_accuracy 3.681198231410178e+215 2.1682950808824264e+77 0.009350912219076035\n",
      "Bird Measurer (Assessment)_4020_accuracy 4.847657130090902e+214 1.1047334545798042e+28 0.0012386523204577545\n",
      "Chest Sorter (Assessment)_4020_accuracy 5.535196124656113e+211 5.156518164058864e+99 0.0016801678802158021\n",
      "Cauldron_Filler_4025 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "to_exclude = [] \n",
    "ajusted_test = reduce_test.copy()\n",
    "for feature in ajusted_test.columns:\n",
    "    if feature not in ['accuracy_group', 'installation_id', 'accuracy_group', 'session_title']:\n",
    "        data = reduce_train[feature]\n",
    "        train_mean = data.mean()\n",
    "        data = ajusted_test[feature] \n",
    "        test_mean = data.mean()\n",
    "        try:\n",
    "            error = stract_hists(feature, adjust=True)\n",
    "            ajust_factor = train_mean / test_mean\n",
    "            if ajust_factor > 10 or ajust_factor < 0.1:# or error > 0.01:\n",
    "                to_exclude.append(feature)\n",
    "                print(feature, train_mean, test_mean, error)\n",
    "            else:\n",
    "                ajusted_test[feature] *= ajust_factor\n",
    "        except:\n",
    "            to_exclude.append(feature)\n",
    "            print(feature, train_mean, test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17690, 387)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [x for x in features if x not in (to_exclude + to_remove)]\n",
    "reduce_train[features].shape\n",
    "#ajusted_test = reduce_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | featur... | lambda_l1 | lambda_l2 | learni... | max_depth | subsam... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.845938\tvalid_1's rmse: 0.977688\n",
      "[200]\ttraining's rmse: 0.754701\tvalid_1's rmse: 0.980691\n",
      "Early stopping, best iteration is:\n",
      "[116]\ttraining's rmse: 0.828452\tvalid_1's rmse: 0.976582\n",
      "Partial score of fold 0 is: 0.596738339599963\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.84338\tvalid_1's rmse: 0.993055\n",
      "Early stopping, best iteration is:\n",
      "[76]\ttraining's rmse: 0.872893\tvalid_1's rmse: 0.992101\n",
      "Partial score of fold 1 is: 0.5857667717531403\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.843457\tvalid_1's rmse: 0.990775\n",
      "[200]\ttraining's rmse: 0.754127\tvalid_1's rmse: 0.993518\n",
      "Early stopping, best iteration is:\n",
      "[120]\ttraining's rmse: 0.822739\tvalid_1's rmse: 0.990016\n",
      "Partial score of fold 2 is: 0.5897033294297842\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.841622\tvalid_1's rmse: 0.989566\n",
      "Early stopping, best iteration is:\n",
      "[95]\ttraining's rmse: 0.847527\tvalid_1's rmse: 0.989188\n",
      "Partial score of fold 3 is: 0.590419067189174\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.846159\tvalid_1's rmse: 0.988615\n",
      "[200]\ttraining's rmse: 0.754947\tvalid_1's rmse: 0.99117\n",
      "Early stopping, best iteration is:\n",
      "[104]\ttraining's rmse: 0.841534\tvalid_1's rmse: 0.988373\n",
      "Partial score of fold 4 is: 0.5838883445530842\n",
      "Our oof cohen kappa score is:  0.5877657432887609\n",
      "kappa:  0.5877657432887609\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5878  \u001b[0m | \u001b[0m 0.5205  \u001b[0m | \u001b[0m 2.049   \u001b[0m | \u001b[0m 2.944   \u001b[0m | \u001b[0m 0.09099 \u001b[0m | \u001b[0m 13.48   \u001b[0m | \u001b[0m 2.532   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.89429\tvalid_1's rmse: 0.98014\n",
      "[200]\ttraining's rmse: 0.824357\tvalid_1's rmse: 0.977844\n",
      "Early stopping, best iteration is:\n",
      "[169]\ttraining's rmse: 0.84364\tvalid_1's rmse: 0.977424\n",
      "Partial score of fold 0 is: 0.596738339599963\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.890179\tvalid_1's rmse: 0.991177\n",
      "[200]\ttraining's rmse: 0.820669\tvalid_1's rmse: 0.992091\n",
      "Early stopping, best iteration is:\n",
      "[138]\ttraining's rmse: 0.860486\tvalid_1's rmse: 0.990298\n",
      "Partial score of fold 1 is: 0.5852299684335979\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.892232\tvalid_1's rmse: 0.991408\n",
      "[200]\ttraining's rmse: 0.823435\tvalid_1's rmse: 0.990599\n",
      "Early stopping, best iteration is:\n",
      "[145]\ttraining's rmse: 0.857621\tvalid_1's rmse: 0.989759\n",
      "Partial score of fold 2 is: 0.5827248862757335\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.891405\tvalid_1's rmse: 0.98778\n",
      "[200]\ttraining's rmse: 0.821478\tvalid_1's rmse: 0.984274\n",
      "[300]\ttraining's rmse: 0.768069\tvalid_1's rmse: 0.98534\n",
      "Early stopping, best iteration is:\n",
      "[270]\ttraining's rmse: 0.783018\tvalid_1's rmse: 0.983582\n",
      "Partial score of fold 3 is: 0.5995447236213942\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.891511\tvalid_1's rmse: 0.989291\n",
      "[200]\ttraining's rmse: 0.822141\tvalid_1's rmse: 0.984933\n",
      "Early stopping, best iteration is:\n",
      "[184]\ttraining's rmse: 0.831542\tvalid_1's rmse: 0.984563\n",
      "Partial score of fold 4 is: 0.585678457073828\n",
      "Our oof cohen kappa score is:  0.589984750400768\n",
      "kappa:  0.589984750400768\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.59    \u001b[0m | \u001b[95m 0.9701  \u001b[0m | \u001b[95m 2.977   \u001b[0m | \u001b[95m 1.678   \u001b[0m | \u001b[95m 0.04975 \u001b[0m | \u001b[95m 14.98   \u001b[0m | \u001b[95m 1.791   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02169\tvalid_1's rmse: 1.0471\n",
      "[200]\ttraining's rmse: 0.952839\tvalid_1's rmse: 1.00085\n",
      "[300]\ttraining's rmse: 0.916691\tvalid_1's rmse: 0.985108\n",
      "[400]\ttraining's rmse: 0.890991\tvalid_1's rmse: 0.979379\n",
      "[500]\ttraining's rmse: 0.869578\tvalid_1's rmse: 0.976593\n",
      "[600]\ttraining's rmse: 0.850919\tvalid_1's rmse: 0.975877\n",
      "[700]\ttraining's rmse: 0.833857\tvalid_1's rmse: 0.975506\n",
      "[800]\ttraining's rmse: 0.817968\tvalid_1's rmse: 0.97514\n",
      "[900]\ttraining's rmse: 0.802949\tvalid_1's rmse: 0.975388\n",
      "Early stopping, best iteration is:\n",
      "[807]\ttraining's rmse: 0.816848\tvalid_1's rmse: 0.975032\n",
      "Partial score of fold 0 is: 0.6008532545020042\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01988\tvalid_1's rmse: 1.04565\n",
      "[200]\ttraining's rmse: 0.949782\tvalid_1's rmse: 1.0035\n",
      "[300]\ttraining's rmse: 0.913259\tvalid_1's rmse: 0.991278\n",
      "[400]\ttraining's rmse: 0.887137\tvalid_1's rmse: 0.987975\n",
      "[500]\ttraining's rmse: 0.866068\tvalid_1's rmse: 0.986636\n",
      "[600]\ttraining's rmse: 0.847376\tvalid_1's rmse: 0.986762\n",
      "Early stopping, best iteration is:\n",
      "[513]\ttraining's rmse: 0.86351\tvalid_1's rmse: 0.986578\n",
      "Partial score of fold 1 is: 0.5875561161516147\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01975\tvalid_1's rmse: 1.04635\n",
      "[200]\ttraining's rmse: 0.950465\tvalid_1's rmse: 1.00196\n",
      "[300]\ttraining's rmse: 0.914011\tvalid_1's rmse: 0.990259\n",
      "[400]\ttraining's rmse: 0.888508\tvalid_1's rmse: 0.98676\n",
      "[500]\ttraining's rmse: 0.867434\tvalid_1's rmse: 0.984989\n",
      "[600]\ttraining's rmse: 0.848665\tvalid_1's rmse: 0.984595\n",
      "[700]\ttraining's rmse: 0.831877\tvalid_1's rmse: 0.984895\n",
      "Early stopping, best iteration is:\n",
      "[614]\ttraining's rmse: 0.846207\tvalid_1's rmse: 0.984524\n",
      "Partial score of fold 2 is: 0.5914926738282588\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01929\tvalid_1's rmse: 1.05002\n",
      "[200]\ttraining's rmse: 0.949043\tvalid_1's rmse: 1.00642\n",
      "[300]\ttraining's rmse: 0.912387\tvalid_1's rmse: 0.993349\n",
      "[400]\ttraining's rmse: 0.887186\tvalid_1's rmse: 0.988892\n",
      "[500]\ttraining's rmse: 0.866056\tvalid_1's rmse: 0.986375\n",
      "[600]\ttraining's rmse: 0.847355\tvalid_1's rmse: 0.985759\n",
      "[700]\ttraining's rmse: 0.830362\tvalid_1's rmse: 0.985842\n",
      "Early stopping, best iteration is:\n",
      "[624]\ttraining's rmse: 0.843192\tvalid_1's rmse: 0.985492\n",
      "Partial score of fold 3 is: 0.5939977559861231\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01996\tvalid_1's rmse: 1.04943\n",
      "[200]\ttraining's rmse: 0.94994\tvalid_1's rmse: 1.00413\n",
      "[300]\ttraining's rmse: 0.913525\tvalid_1's rmse: 0.989973\n",
      "[400]\ttraining's rmse: 0.888323\tvalid_1's rmse: 0.984991\n",
      "[500]\ttraining's rmse: 0.867539\tvalid_1's rmse: 0.982983\n",
      "[600]\ttraining's rmse: 0.849015\tvalid_1's rmse: 0.981595\n",
      "[700]\ttraining's rmse: 0.832209\tvalid_1's rmse: 0.981176\n",
      "[800]\ttraining's rmse: 0.816481\tvalid_1's rmse: 0.980737\n",
      "[900]\ttraining's rmse: 0.80167\tvalid_1's rmse: 0.980663\n",
      "Early stopping, best iteration is:\n",
      "[866]\ttraining's rmse: 0.806572\tvalid_1's rmse: 0.980344\n",
      "Partial score of fold 4 is: 0.5923018734005805\n",
      "Our oof cohen kappa score is:  0.593528003692521\n",
      "kappa:  0.593528003692521\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.5935  \u001b[0m | \u001b[95m 0.6371  \u001b[0m | \u001b[95m 3.3     \u001b[0m | \u001b[95m 0.4016  \u001b[0m | \u001b[95m 0.01348 \u001b[0m | \u001b[95m 15.7    \u001b[0m | \u001b[95m 2.686   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.882781\tvalid_1's rmse: 0.977791\n",
      "[200]\ttraining's rmse: 0.80873\tvalid_1's rmse: 0.975427\n",
      "Early stopping, best iteration is:\n",
      "[151]\ttraining's rmse: 0.841427\tvalid_1's rmse: 0.974595\n",
      "Partial score of fold 0 is: 0.5988852517227671\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.878346\tvalid_1's rmse: 0.990321\n",
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's rmse: 0.880105\tvalid_1's rmse: 0.99026\n",
      "Partial score of fold 1 is: 0.582545951835886\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.881359\tvalid_1's rmse: 0.992522\n",
      "[200]\ttraining's rmse: 0.807985\tvalid_1's rmse: 0.993555\n",
      "Early stopping, best iteration is:\n",
      "[159]\ttraining's rmse: 0.834794\tvalid_1's rmse: 0.9912\n",
      "Partial score of fold 2 is: 0.5907769360688689\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.879675\tvalid_1's rmse: 0.987413\n",
      "[200]\ttraining's rmse: 0.806743\tvalid_1's rmse: 0.989945\n",
      "Early stopping, best iteration is:\n",
      "[150]\ttraining's rmse: 0.839783\tvalid_1's rmse: 0.985966\n",
      "Partial score of fold 3 is: 0.5949822991475677\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.879929\tvalid_1's rmse: 0.988827\n",
      "[200]\ttraining's rmse: 0.80703\tvalid_1's rmse: 0.988465\n",
      "Early stopping, best iteration is:\n",
      "[144]\ttraining's rmse: 0.844848\tvalid_1's rmse: 0.986903\n",
      "Partial score of fold 4 is: 0.5824562545364891\n",
      "Our oof cohen kappa score is:  0.5896268460278635\n",
      "kappa:  0.5896268460278635\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5896  \u001b[0m | \u001b[0m 0.9697  \u001b[0m | \u001b[0m 4.356   \u001b[0m | \u001b[0m 1.791   \u001b[0m | \u001b[0m 0.06111 \u001b[0m | \u001b[0m 16.52   \u001b[0m | \u001b[0m 5.072   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.924317\tvalid_1's rmse: 0.982593\n",
      "[200]\ttraining's rmse: 0.865163\tvalid_1's rmse: 0.974695\n",
      "[300]\ttraining's rmse: 0.82207\tvalid_1's rmse: 0.974543\n",
      "Early stopping, best iteration is:\n",
      "[255]\ttraining's rmse: 0.840597\tvalid_1's rmse: 0.973951\n",
      "Partial score of fold 0 is: 0.6038947133426433\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.919821\tvalid_1's rmse: 0.9931\n",
      "[200]\ttraining's rmse: 0.86097\tvalid_1's rmse: 0.990713\n",
      "Early stopping, best iteration is:\n",
      "[158]\ttraining's rmse: 0.882479\tvalid_1's rmse: 0.99017\n",
      "Partial score of fold 1 is: 0.5832616895952758\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.922639\tvalid_1's rmse: 0.991962\n",
      "[200]\ttraining's rmse: 0.863702\tvalid_1's rmse: 0.98648\n",
      "Early stopping, best iteration is:\n",
      "[186]\ttraining's rmse: 0.870695\tvalid_1's rmse: 0.985811\n",
      "Partial score of fold 2 is: 0.5864825095125301\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.920041\tvalid_1's rmse: 0.993028\n",
      "[200]\ttraining's rmse: 0.861079\tvalid_1's rmse: 0.988784\n",
      "[300]\ttraining's rmse: 0.817212\tvalid_1's rmse: 0.988656\n",
      "Early stopping, best iteration is:\n",
      "[240]\ttraining's rmse: 0.842146\tvalid_1's rmse: 0.987927\n",
      "Partial score of fold 3 is: 0.5914926738282588\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.921072\tvalid_1's rmse: 0.994764\n",
      "[200]\ttraining's rmse: 0.862089\tvalid_1's rmse: 0.989154\n",
      "[300]\ttraining's rmse: 0.81917\tvalid_1's rmse: 0.987657\n",
      "[400]\ttraining's rmse: 0.783255\tvalid_1's rmse: 0.988382\n",
      "Early stopping, best iteration is:\n",
      "[340]\ttraining's rmse: 0.804516\tvalid_1's rmse: 0.986889\n",
      "Partial score of fold 4 is: 0.5821418142600437\n",
      "Our oof cohen kappa score is:  0.5894478938414114\n",
      "kappa:  0.5894478938414114\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.5894  \u001b[0m | \u001b[0m 0.8525  \u001b[0m | \u001b[0m 3.981   \u001b[0m | \u001b[0m 4.719   \u001b[0m | \u001b[0m 0.03734 \u001b[0m | \u001b[0m 16.64   \u001b[0m | \u001b[0m 5.5     \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.931082\tvalid_1's rmse: 0.993615\n",
      "[200]\ttraining's rmse: 0.864611\tvalid_1's rmse: 0.978543\n",
      "[300]\ttraining's rmse: 0.817481\tvalid_1's rmse: 0.976652\n",
      "Early stopping, best iteration is:\n",
      "[292]\ttraining's rmse: 0.820878\tvalid_1's rmse: 0.976336\n",
      "Partial score of fold 0 is: 0.598169614348499\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.926742\tvalid_1's rmse: 0.998641\n",
      "[200]\ttraining's rmse: 0.860432\tvalid_1's rmse: 0.993319\n",
      "Early stopping, best iteration is:\n",
      "[158]\ttraining's rmse: 0.884552\tvalid_1's rmse: 0.99283\n",
      "Partial score of fold 1 is: 0.5803987385577165\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.928358\tvalid_1's rmse: 0.997959\n",
      "[200]\ttraining's rmse: 0.86267\tvalid_1's rmse: 0.98917\n",
      "[300]\ttraining's rmse: 0.816306\tvalid_1's rmse: 0.988128\n",
      "Early stopping, best iteration is:\n",
      "[284]\ttraining's rmse: 0.822556\tvalid_1's rmse: 0.987287\n",
      "Partial score of fold 2 is: 0.5880929194711572\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.926229\tvalid_1's rmse: 0.999964\n",
      "[200]\ttraining's rmse: 0.861274\tvalid_1's rmse: 0.987273\n",
      "[300]\ttraining's rmse: 0.814199\tvalid_1's rmse: 0.986979\n",
      "[400]\ttraining's rmse: 0.776077\tvalid_1's rmse: 0.987831\n",
      "Early stopping, best iteration is:\n",
      "[338]\ttraining's rmse: 0.798997\tvalid_1's rmse: 0.986589\n",
      "Partial score of fold 3 is: 0.5913137393884114\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.927468\tvalid_1's rmse: 0.998494\n",
      "[200]\ttraining's rmse: 0.862304\tvalid_1's rmse: 0.987083\n",
      "[300]\ttraining's rmse: 0.816099\tvalid_1's rmse: 0.983537\n",
      "[400]\ttraining's rmse: 0.778993\tvalid_1's rmse: 0.98417\n",
      "Early stopping, best iteration is:\n",
      "[328]\ttraining's rmse: 0.80563\tvalid_1's rmse: 0.983056\n",
      "Partial score of fold 4 is: 0.5862154908300512\n",
      "Our oof cohen kappa score is:  0.5893047320922497\n",
      "kappa:  0.5893047320922497\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.5893  \u001b[0m | \u001b[0m 0.5194  \u001b[0m | \u001b[0m 1.336   \u001b[0m | \u001b[0m 0.582   \u001b[0m | \u001b[0m 0.03556 \u001b[0m | \u001b[0m 14.96   \u001b[0m | \u001b[0m 7.815   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01762\tvalid_1's rmse: 1.03798\n",
      "[200]\ttraining's rmse: 0.955228\tvalid_1's rmse: 0.99687\n",
      "[300]\ttraining's rmse: 0.921963\tvalid_1's rmse: 0.983056\n",
      "[400]\ttraining's rmse: 0.89819\tvalid_1's rmse: 0.978065\n",
      "[500]\ttraining's rmse: 0.878501\tvalid_1's rmse: 0.975815\n",
      "[600]\ttraining's rmse: 0.860815\tvalid_1's rmse: 0.974947\n",
      "[700]\ttraining's rmse: 0.844664\tvalid_1's rmse: 0.974137\n",
      "[800]\ttraining's rmse: 0.829803\tvalid_1's rmse: 0.973975\n",
      "Early stopping, best iteration is:\n",
      "[734]\ttraining's rmse: 0.839605\tvalid_1's rmse: 0.973817\n",
      "Partial score of fold 0 is: 0.6028212572812413\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01553\tvalid_1's rmse: 1.03869\n",
      "[200]\ttraining's rmse: 0.951959\tvalid_1's rmse: 1.00134\n",
      "[300]\ttraining's rmse: 0.918771\tvalid_1's rmse: 0.992165\n",
      "[400]\ttraining's rmse: 0.894723\tvalid_1's rmse: 0.989453\n",
      "[500]\ttraining's rmse: 0.875089\tvalid_1's rmse: 0.988303\n",
      "[600]\ttraining's rmse: 0.857347\tvalid_1's rmse: 0.988543\n",
      "Early stopping, best iteration is:\n",
      "[517]\ttraining's rmse: 0.872019\tvalid_1's rmse: 0.988223\n",
      "Partial score of fold 1 is: 0.5861246406328351\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01542\tvalid_1's rmse: 1.04161\n",
      "[200]\ttraining's rmse: 0.952569\tvalid_1's rmse: 1.00259\n",
      "[300]\ttraining's rmse: 0.919673\tvalid_1's rmse: 0.992267\n",
      "[400]\ttraining's rmse: 0.895872\tvalid_1's rmse: 0.98789\n",
      "[500]\ttraining's rmse: 0.876374\tvalid_1's rmse: 0.986186\n",
      "[600]\ttraining's rmse: 0.858773\tvalid_1's rmse: 0.985523\n",
      "[700]\ttraining's rmse: 0.84308\tvalid_1's rmse: 0.985663\n",
      "Early stopping, best iteration is:\n",
      "[636]\ttraining's rmse: 0.853025\tvalid_1's rmse: 0.985215\n",
      "Partial score of fold 2 is: 0.5895243949899367\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01777\tvalid_1's rmse: 1.04426\n",
      "[200]\ttraining's rmse: 0.952836\tvalid_1's rmse: 1.00364\n",
      "[300]\ttraining's rmse: 0.919643\tvalid_1's rmse: 0.991802\n",
      "[400]\ttraining's rmse: 0.895757\tvalid_1's rmse: 0.988091\n",
      "[500]\ttraining's rmse: 0.875855\tvalid_1's rmse: 0.98588\n",
      "[600]\ttraining's rmse: 0.857992\tvalid_1's rmse: 0.984448\n",
      "[700]\ttraining's rmse: 0.842185\tvalid_1's rmse: 0.984555\n",
      "Early stopping, best iteration is:\n",
      "[618]\ttraining's rmse: 0.85501\tvalid_1's rmse: 0.984235\n",
      "Partial score of fold 3 is: 0.5932820182267333\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01751\tvalid_1's rmse: 1.04588\n",
      "[200]\ttraining's rmse: 0.952976\tvalid_1's rmse: 1.00435\n",
      "[300]\ttraining's rmse: 0.920113\tvalid_1's rmse: 0.991306\n",
      "[400]\ttraining's rmse: 0.896536\tvalid_1's rmse: 0.985792\n",
      "[500]\ttraining's rmse: 0.87675\tvalid_1's rmse: 0.982929\n",
      "[600]\ttraining's rmse: 0.85939\tvalid_1's rmse: 0.981248\n",
      "[700]\ttraining's rmse: 0.843729\tvalid_1's rmse: 0.980204\n",
      "[800]\ttraining's rmse: 0.828892\tvalid_1's rmse: 0.980107\n",
      "Early stopping, best iteration is:\n",
      "[732]\ttraining's rmse: 0.838936\tvalid_1's rmse: 0.9799\n",
      "Partial score of fold 4 is: 0.589795715871539\n",
      "Our oof cohen kappa score is:  0.5924900810110985\n",
      "kappa:  0.5924900810110985\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5925  \u001b[0m | \u001b[0m 0.8534  \u001b[0m | \u001b[0m 2.64    \u001b[0m | \u001b[0m 4.351   \u001b[0m | \u001b[0m 0.01207 \u001b[0m | \u001b[0m 14.23   \u001b[0m | \u001b[0m 2.515   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.927722\tvalid_1's rmse: 0.987926\n",
      "[200]\ttraining's rmse: 0.866763\tvalid_1's rmse: 0.97851\n",
      "[300]\ttraining's rmse: 0.822922\tvalid_1's rmse: 0.978624\n",
      "Early stopping, best iteration is:\n",
      "[251]\ttraining's rmse: 0.84356\tvalid_1's rmse: 0.977444\n",
      "Partial score of fold 0 is: 0.598527433035633\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.923558\tvalid_1's rmse: 0.994748\n",
      "[200]\ttraining's rmse: 0.863669\tvalid_1's rmse: 0.991261\n",
      "Early stopping, best iteration is:\n",
      "[180]\ttraining's rmse: 0.873379\tvalid_1's rmse: 0.990806\n",
      "Partial score of fold 1 is: 0.5846931651140554\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.926033\tvalid_1's rmse: 0.994446\n",
      "[200]\ttraining's rmse: 0.865774\tvalid_1's rmse: 0.985256\n",
      "Early stopping, best iteration is:\n",
      "[199]\ttraining's rmse: 0.866255\tvalid_1's rmse: 0.985248\n",
      "Partial score of fold 2 is: 0.5948924281853605\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.925134\tvalid_1's rmse: 0.995198\n",
      "[200]\ttraining's rmse: 0.863942\tvalid_1's rmse: 0.986859\n",
      "[300]\ttraining's rmse: 0.820852\tvalid_1's rmse: 0.986578\n",
      "Early stopping, best iteration is:\n",
      "[257]\ttraining's rmse: 0.838391\tvalid_1's rmse: 0.985701\n",
      "Partial score of fold 3 is: 0.5898822638696317\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.925269\tvalid_1's rmse: 0.994709\n",
      "[200]\ttraining's rmse: 0.8647\tvalid_1's rmse: 0.98589\n",
      "[300]\ttraining's rmse: 0.821371\tvalid_1's rmse: 0.983857\n",
      "Early stopping, best iteration is:\n",
      "[298]\ttraining's rmse: 0.822099\tvalid_1's rmse: 0.983814\n",
      "Partial score of fold 4 is: 0.5819192207802659\n",
      "Our oof cohen kappa score is:  0.5890541990312166\n",
      "kappa:  0.5890541990312166\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.5891  \u001b[0m | \u001b[0m 0.713   \u001b[0m | \u001b[0m 3.884   \u001b[0m | \u001b[0m 2.672   \u001b[0m | \u001b[0m 0.03624 \u001b[0m | \u001b[0m 14.89   \u001b[0m | \u001b[0m 7.059   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.914074\tvalid_1's rmse: 0.98297\n",
      "[200]\ttraining's rmse: 0.849687\tvalid_1's rmse: 0.974546\n",
      "[300]\ttraining's rmse: 0.803289\tvalid_1's rmse: 0.976525\n",
      "Early stopping, best iteration is:\n",
      "[207]\ttraining's rmse: 0.845869\tvalid_1's rmse: 0.974153\n",
      "Partial score of fold 0 is: 0.6024634385941072\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.910831\tvalid_1's rmse: 0.992206\n",
      "[200]\ttraining's rmse: 0.846044\tvalid_1's rmse: 0.987724\n",
      "Early stopping, best iteration is:\n",
      "[195]\ttraining's rmse: 0.848789\tvalid_1's rmse: 0.987288\n",
      "Partial score of fold 1 is: 0.5845066312370308\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.910975\tvalid_1's rmse: 0.991872\n",
      "[200]\ttraining's rmse: 0.848604\tvalid_1's rmse: 0.988432\n",
      "Early stopping, best iteration is:\n",
      "[196]\ttraining's rmse: 0.85058\tvalid_1's rmse: 0.988351\n",
      "Partial score of fold 2 is: 0.5875561161516147\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.911325\tvalid_1's rmse: 0.993401\n",
      "[200]\ttraining's rmse: 0.848306\tvalid_1's rmse: 0.988304\n",
      "[300]\ttraining's rmse: 0.801728\tvalid_1's rmse: 0.987221\n",
      "Early stopping, best iteration is:\n",
      "[289]\ttraining's rmse: 0.806438\tvalid_1's rmse: 0.987184\n",
      "Partial score of fold 3 is: 0.594713493745513\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.911103\tvalid_1's rmse: 0.990119\n",
      "[200]\ttraining's rmse: 0.847369\tvalid_1's rmse: 0.98279\n",
      "[300]\ttraining's rmse: 0.801426\tvalid_1's rmse: 0.983264\n",
      "Early stopping, best iteration is:\n",
      "[278]\ttraining's rmse: 0.810679\tvalid_1's rmse: 0.981887\n",
      "Partial score of fold 4 is: 0.5896167046194646\n",
      "Our oof cohen kappa score is:  0.5907721400211575\n",
      "kappa:  0.5907721400211575\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5908  \u001b[0m | \u001b[0m 0.6277  \u001b[0m | \u001b[0m 3.202   \u001b[0m | \u001b[0m 3.754   \u001b[0m | \u001b[0m 0.04358 \u001b[0m | \u001b[0m 13.63   \u001b[0m | \u001b[0m 9.341   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.916738\tvalid_1's rmse: 0.984658\n",
      "[200]\ttraining's rmse: 0.853054\tvalid_1's rmse: 0.978079\n",
      "[300]\ttraining's rmse: 0.807519\tvalid_1's rmse: 0.978588\n",
      "Early stopping, best iteration is:\n",
      "[242]\ttraining's rmse: 0.83301\tvalid_1's rmse: 0.977453\n",
      "Partial score of fold 0 is: 0.6006320330210084\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.912592\tvalid_1's rmse: 0.991781\n",
      "[200]\ttraining's rmse: 0.849575\tvalid_1's rmse: 0.990009\n",
      "Early stopping, best iteration is:\n",
      "[191]\ttraining's rmse: 0.854197\tvalid_1's rmse: 0.989976\n",
      "Partial score of fold 1 is: 0.5816512796366488\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.913625\tvalid_1's rmse: 0.99263\n",
      "[200]\ttraining's rmse: 0.850956\tvalid_1's rmse: 0.989325\n",
      "Early stopping, best iteration is:\n",
      "[153]\ttraining's rmse: 0.877402\tvalid_1's rmse: 0.988612\n",
      "Partial score of fold 2 is: 0.5868403783922249\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.915096\tvalid_1's rmse: 0.994488\n",
      "[200]\ttraining's rmse: 0.852402\tvalid_1's rmse: 0.99028\n",
      "Early stopping, best iteration is:\n",
      "[172]\ttraining's rmse: 0.867822\tvalid_1's rmse: 0.98946\n",
      "Partial score of fold 3 is: 0.5945345593056655\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.914964\tvalid_1's rmse: 0.992532\n",
      "[200]\ttraining's rmse: 0.852636\tvalid_1's rmse: 0.987401\n",
      "[300]\ttraining's rmse: 0.806334\tvalid_1's rmse: 0.986632\n",
      "Early stopping, best iteration is:\n",
      "[269]\ttraining's rmse: 0.818954\tvalid_1's rmse: 0.986358\n",
      "Partial score of fold 4 is: 0.581246659620498\n",
      "Our oof cohen kappa score is:  0.5897342173397349\n",
      "kappa:  0.5897342173397349\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.5897  \u001b[0m | \u001b[0m 0.9993  \u001b[0m | \u001b[0m 0.5259  \u001b[0m | \u001b[0m 4.569   \u001b[0m | \u001b[0m 0.03796 \u001b[0m | \u001b[0m 14.9    \u001b[0m | \u001b[0m 9.381   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.832908\tvalid_1's rmse: 0.980299\n",
      "Early stopping, best iteration is:\n",
      "[72]\ttraining's rmse: 0.865769\tvalid_1's rmse: 0.978098\n",
      "Partial score of fold 0 is: 0.6013899825327051\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.829067\tvalid_1's rmse: 0.995356\n",
      "Early stopping, best iteration is:\n",
      "[79]\ttraining's rmse: 0.85473\tvalid_1's rmse: 0.993816\n",
      "Partial score of fold 1 is: 0.5802198041178691\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.831484\tvalid_1's rmse: 0.995281\n",
      "Early stopping, best iteration is:\n",
      "[74]\ttraining's rmse: 0.861525\tvalid_1's rmse: 0.993864\n",
      "Partial score of fold 2 is: 0.5803987385577165\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.832755\tvalid_1's rmse: 0.991854\n",
      "[200]\ttraining's rmse: 0.735788\tvalid_1's rmse: 0.997983\n",
      "Early stopping, best iteration is:\n",
      "[100]\ttraining's rmse: 0.832755\tvalid_1's rmse: 0.991854\n",
      "Partial score of fold 3 is: 0.5893454605500893\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.830118\tvalid_1's rmse: 0.992373\n",
      "Early stopping, best iteration is:\n",
      "[89]\ttraining's rmse: 0.842847\tvalid_1's rmse: 0.990743\n",
      "Partial score of fold 4 is: 0.5795920745032989\n",
      "Our oof cohen kappa score is:  0.5844372326207504\n",
      "kappa:  0.5844372326207504\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.5844  \u001b[0m | \u001b[0m 0.9364  \u001b[0m | \u001b[0m 0.2751  \u001b[0m | \u001b[0m 4.832   \u001b[0m | \u001b[0m 0.09599 \u001b[0m | \u001b[0m 13.28   \u001b[0m | \u001b[0m 5.325   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.925456\tvalid_1's rmse: 0.983835\n",
      "[200]\ttraining's rmse: 0.862861\tvalid_1's rmse: 0.97433\n",
      "Early stopping, best iteration is:\n",
      "[199]\ttraining's rmse: 0.863407\tvalid_1's rmse: 0.974248\n",
      "Partial score of fold 0 is: 0.6026006853341471\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.92278\tvalid_1's rmse: 0.995592\n",
      "[200]\ttraining's rmse: 0.8605\tvalid_1's rmse: 0.993154\n",
      "Early stopping, best iteration is:\n",
      "[136]\ttraining's rmse: 0.896823\tvalid_1's rmse: 0.992657\n",
      "Partial score of fold 1 is: 0.5818302140764962\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.922496\tvalid_1's rmse: 0.996727\n",
      "[200]\ttraining's rmse: 0.860235\tvalid_1's rmse: 0.988796\n",
      "Early stopping, best iteration is:\n",
      "[182]\ttraining's rmse: 0.869539\tvalid_1's rmse: 0.988636\n",
      "Partial score of fold 2 is: 0.5832616895952758\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.920969\tvalid_1's rmse: 0.993493\n",
      "[200]\ttraining's rmse: 0.859609\tvalid_1's rmse: 0.986455\n",
      "[300]\ttraining's rmse: 0.815147\tvalid_1's rmse: 0.986395\n",
      "Early stopping, best iteration is:\n",
      "[270]\ttraining's rmse: 0.826922\tvalid_1's rmse: 0.985752\n",
      "Partial score of fold 3 is: 0.592745214907191\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.923224\tvalid_1's rmse: 0.993704\n",
      "[200]\ttraining's rmse: 0.8627\tvalid_1's rmse: 0.983567\n",
      "[300]\ttraining's rmse: 0.817949\tvalid_1's rmse: 0.982011\n",
      "Early stopping, best iteration is:\n",
      "[280]\ttraining's rmse: 0.826065\tvalid_1's rmse: 0.981409\n",
      "Partial score of fold 4 is: 0.589795715871539\n",
      "Our oof cohen kappa score is:  0.5883383902854079\n",
      "kappa:  0.5883383902854079\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.5883  \u001b[0m | \u001b[0m 0.755   \u001b[0m | \u001b[0m 1.843   \u001b[0m | \u001b[0m 2.172   \u001b[0m | \u001b[0m 0.03499 \u001b[0m | \u001b[0m 14.26   \u001b[0m | \u001b[0m 6.525   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.884848\tvalid_1's rmse: 0.982209\n",
      "[200]\ttraining's rmse: 0.808956\tvalid_1's rmse: 0.980436\n",
      "Early stopping, best iteration is:\n",
      "[187]\ttraining's rmse: 0.817546\tvalid_1's rmse: 0.980176\n",
      "Partial score of fold 0 is: 0.599243070409901\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.880359\tvalid_1's rmse: 0.988763\n",
      "[200]\ttraining's rmse: 0.806584\tvalid_1's rmse: 0.992195\n",
      "Early stopping, best iteration is:\n",
      "[111]\ttraining's rmse: 0.871349\tvalid_1's rmse: 0.988133\n",
      "Partial score of fold 1 is: 0.5836195584749708\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.881974\tvalid_1's rmse: 0.9902\n",
      "[200]\ttraining's rmse: 0.806803\tvalid_1's rmse: 0.992186\n",
      "Early stopping, best iteration is:\n",
      "[121]\ttraining's rmse: 0.864321\tvalid_1's rmse: 0.988686\n",
      "Partial score of fold 2 is: 0.590419067189174\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.879111\tvalid_1's rmse: 0.989419\n",
      "[200]\ttraining's rmse: 0.805523\tvalid_1's rmse: 0.990091\n",
      "Early stopping, best iteration is:\n",
      "[127]\ttraining's rmse: 0.855878\tvalid_1's rmse: 0.987946\n",
      "Partial score of fold 3 is: 0.5875561161516147\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.880704\tvalid_1's rmse: 0.988211\n",
      "[200]\ttraining's rmse: 0.807411\tvalid_1's rmse: 0.985497\n",
      "Early stopping, best iteration is:\n",
      "[178]\ttraining's rmse: 0.820408\tvalid_1's rmse: 0.985474\n",
      "Partial score of fold 4 is: 0.5869315358383488\n",
      "Our oof cohen kappa score is:  0.5892689416549592\n",
      "kappa:  0.5892689416549592\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.5893  \u001b[0m | \u001b[0m 0.6994  \u001b[0m | \u001b[0m 1.488   \u001b[0m | \u001b[0m 3.371   \u001b[0m | \u001b[0m 0.05839 \u001b[0m | \u001b[0m 16.8    \u001b[0m | \u001b[0m 5.218   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.913882\tvalid_1's rmse: 0.98558\n",
      "[200]\ttraining's rmse: 0.848678\tvalid_1's rmse: 0.976645\n",
      "[300]\ttraining's rmse: 0.800646\tvalid_1's rmse: 0.975763\n",
      "[400]\ttraining's rmse: 0.759386\tvalid_1's rmse: 0.976466\n",
      "Early stopping, best iteration is:\n",
      "[337]\ttraining's rmse: 0.784887\tvalid_1's rmse: 0.975684\n",
      "Partial score of fold 0 is: 0.598527433035633\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.9105\tvalid_1's rmse: 0.990679\n",
      "[200]\ttraining's rmse: 0.845569\tvalid_1's rmse: 0.987268\n",
      "Early stopping, best iteration is:\n",
      "[170]\ttraining's rmse: 0.862751\tvalid_1's rmse: 0.98623\n",
      "Partial score of fold 1 is: 0.5864825095125301\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.912868\tvalid_1's rmse: 0.994288\n",
      "[200]\ttraining's rmse: 0.848086\tvalid_1's rmse: 0.989818\n",
      "Early stopping, best iteration is:\n",
      "[170]\ttraining's rmse: 0.86438\tvalid_1's rmse: 0.988738\n",
      "Partial score of fold 2 is: 0.5836195584749708\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.911631\tvalid_1's rmse: 0.993107\n",
      "[200]\ttraining's rmse: 0.846189\tvalid_1's rmse: 0.988386\n",
      "Early stopping, best iteration is:\n",
      "[187]\ttraining's rmse: 0.85308\tvalid_1's rmse: 0.988206\n",
      "Partial score of fold 3 is: 0.592387346027496\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.911681\tvalid_1's rmse: 0.990203\n",
      "[200]\ttraining's rmse: 0.847174\tvalid_1's rmse: 0.983275\n",
      "[300]\ttraining's rmse: 0.7989\tvalid_1's rmse: 0.983444\n",
      "Early stopping, best iteration is:\n",
      "[254]\ttraining's rmse: 0.819966\tvalid_1's rmse: 0.98188\n",
      "Partial score of fold 4 is: 0.5880056033507951\n",
      "Our oof cohen kappa score is:  0.5903784452109627\n",
      "kappa:  0.5903784452109627\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.5904  \u001b[0m | \u001b[0m 0.6269  \u001b[0m | \u001b[0m 0.9706  \u001b[0m | \u001b[0m 4.516   \u001b[0m | \u001b[0m 0.04149 \u001b[0m | \u001b[0m 15.71   \u001b[0m | \u001b[0m 5.414   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.927677\tvalid_1's rmse: 0.986795\n",
      "[200]\ttraining's rmse: 0.864361\tvalid_1's rmse: 0.975669\n",
      "[300]\ttraining's rmse: 0.818241\tvalid_1's rmse: 0.975851\n",
      "Early stopping, best iteration is:\n",
      "[260]\ttraining's rmse: 0.836288\tvalid_1's rmse: 0.974579\n",
      "Partial score of fold 0 is: 0.6012110731891381\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.92337\tvalid_1's rmse: 0.994307\n",
      "[200]\ttraining's rmse: 0.861729\tvalid_1's rmse: 0.991903\n",
      "Early stopping, best iteration is:\n",
      "[178]\ttraining's rmse: 0.872917\tvalid_1's rmse: 0.990552\n",
      "Partial score of fold 1 is: 0.5829910548006332\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.923724\tvalid_1's rmse: 0.996304\n",
      "[200]\ttraining's rmse: 0.862331\tvalid_1's rmse: 0.988862\n",
      "[300]\ttraining's rmse: 0.818223\tvalid_1's rmse: 0.988323\n",
      "Early stopping, best iteration is:\n",
      "[292]\ttraining's rmse: 0.821615\tvalid_1's rmse: 0.987771\n",
      "Partial score of fold 2 is: 0.5897033294297842\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.923603\tvalid_1's rmse: 0.996711\n",
      "[200]\ttraining's rmse: 0.861579\tvalid_1's rmse: 0.989923\n",
      "[300]\ttraining's rmse: 0.816043\tvalid_1's rmse: 0.990466\n",
      "Early stopping, best iteration is:\n",
      "[230]\ttraining's rmse: 0.846862\tvalid_1's rmse: 0.989226\n",
      "Partial score of fold 3 is: 0.5873771817117673\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.923473\tvalid_1's rmse: 0.994575\n",
      "[200]\ttraining's rmse: 0.861044\tvalid_1's rmse: 0.987299\n",
      "[300]\ttraining's rmse: 0.815818\tvalid_1's rmse: 0.984269\n",
      "[400]\ttraining's rmse: 0.77889\tvalid_1's rmse: 0.986377\n",
      "Early stopping, best iteration is:\n",
      "[327]\ttraining's rmse: 0.806289\tvalid_1's rmse: 0.983851\n",
      "Partial score of fold 4 is: 0.5837093333010097\n",
      "Our oof cohen kappa score is:  0.5898773790888967\n",
      "kappa:  0.5898773790888967\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.5899  \u001b[0m | \u001b[0m 0.853   \u001b[0m | \u001b[0m 1.536   \u001b[0m | \u001b[0m 0.4828  \u001b[0m | \u001b[0m 0.03343 \u001b[0m | \u001b[0m 14.28   \u001b[0m | \u001b[0m 7.365   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.878771\tvalid_1's rmse: 0.982883\n",
      "[200]\ttraining's rmse: 0.799934\tvalid_1's rmse: 0.981926\n",
      "Early stopping, best iteration is:\n",
      "[157]\ttraining's rmse: 0.830911\tvalid_1's rmse: 0.981288\n",
      "Partial score of fold 0 is: 0.5958437928821279\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.875637\tvalid_1's rmse: 0.993656\n",
      "[200]\ttraining's rmse: 0.798504\tvalid_1's rmse: 0.99675\n",
      "Early stopping, best iteration is:\n",
      "[110]\ttraining's rmse: 0.86672\tvalid_1's rmse: 0.992832\n",
      "Partial score of fold 1 is: 0.5811144763171063\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.875592\tvalid_1's rmse: 0.990768\n",
      "[200]\ttraining's rmse: 0.800229\tvalid_1's rmse: 0.992043\n",
      "Early stopping, best iteration is:\n",
      "[127]\ttraining's rmse: 0.852413\tvalid_1's rmse: 0.990201\n",
      "Partial score of fold 2 is: 0.5922084115876486\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.874458\tvalid_1's rmse: 0.987682\n",
      "[200]\ttraining's rmse: 0.796989\tvalid_1's rmse: 0.985106\n",
      "Early stopping, best iteration is:\n",
      "[174]\ttraining's rmse: 0.815273\tvalid_1's rmse: 0.984252\n",
      "Partial score of fold 3 is: 0.5936398871064282\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.875994\tvalid_1's rmse: 0.987671\n",
      "[200]\ttraining's rmse: 0.800233\tvalid_1's rmse: 0.987075\n",
      "Early stopping, best iteration is:\n",
      "[117]\ttraining's rmse: 0.861836\tvalid_1's rmse: 0.985836\n",
      "Partial score of fold 4 is: 0.5819192207802659\n",
      "Our oof cohen kappa score is:  0.5901279121499297\n",
      "kappa:  0.5901279121499297\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.5901  \u001b[0m | \u001b[0m 0.6327  \u001b[0m | \u001b[0m 1.826   \u001b[0m | \u001b[0m 1.33    \u001b[0m | \u001b[0m 0.06199 \u001b[0m | \u001b[0m 14.77   \u001b[0m | \u001b[0m 7.747   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.85787\tvalid_1's rmse: 0.978636\n",
      "[200]\ttraining's rmse: 0.775084\tvalid_1's rmse: 0.981728\n",
      "Early stopping, best iteration is:\n",
      "[116]\ttraining's rmse: 0.84208\tvalid_1's rmse: 0.977387\n",
      "Partial score of fold 0 is: 0.597811795661365\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.852958\tvalid_1's rmse: 0.991792\n",
      "Early stopping, best iteration is:\n",
      "[70]\ttraining's rmse: 0.887257\tvalid_1's rmse: 0.990553\n",
      "Partial score of fold 1 is: 0.5820091485163437\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.854993\tvalid_1's rmse: 0.989882\n",
      "[200]\ttraining's rmse: 0.772551\tvalid_1's rmse: 0.992881\n",
      "Early stopping, best iteration is:\n",
      "[130]\ttraining's rmse: 0.826407\tvalid_1's rmse: 0.989231\n",
      "Partial score of fold 2 is: 0.5902401327493265\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.853612\tvalid_1's rmse: 0.992634\n",
      "[200]\ttraining's rmse: 0.771987\tvalid_1's rmse: 0.992091\n",
      "Early stopping, best iteration is:\n",
      "[171]\ttraining's rmse: 0.792824\tvalid_1's rmse: 0.9904\n",
      "Partial score of fold 3 is: 0.590419067189174\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.85484\tvalid_1's rmse: 0.990605\n",
      "[200]\ttraining's rmse: 0.772208\tvalid_1's rmse: 0.99063\n",
      "Early stopping, best iteration is:\n",
      "[120]\ttraining's rmse: 0.835522\tvalid_1's rmse: 0.989\n",
      "Partial score of fold 4 is: 0.5824517084270292\n",
      "Our oof cohen kappa score is:  0.5867278206073383\n",
      "kappa:  0.5867278206073383\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.5867  \u001b[0m | \u001b[0m 0.6655  \u001b[0m | \u001b[0m 4.927   \u001b[0m | \u001b[0m 0.004917\u001b[0m | \u001b[0m 0.08276 \u001b[0m | \u001b[0m 16.98   \u001b[0m | \u001b[0m 3.692   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.04953\tvalid_1's rmse: 1.07244\n",
      "[200]\ttraining's rmse: 0.974831\tvalid_1's rmse: 1.01687\n",
      "[300]\ttraining's rmse: 0.935794\tvalid_1's rmse: 0.994912\n",
      "[400]\ttraining's rmse: 0.909486\tvalid_1's rmse: 0.98465\n",
      "[500]\ttraining's rmse: 0.888692\tvalid_1's rmse: 0.97962\n",
      "[600]\ttraining's rmse: 0.871115\tvalid_1's rmse: 0.977533\n",
      "[700]\ttraining's rmse: 0.855396\tvalid_1's rmse: 0.975795\n",
      "[800]\ttraining's rmse: 0.840732\tvalid_1's rmse: 0.974677\n",
      "[900]\ttraining's rmse: 0.827238\tvalid_1's rmse: 0.974579\n",
      "Early stopping, best iteration is:\n",
      "[849]\ttraining's rmse: 0.834076\tvalid_1's rmse: 0.974355\n",
      "Partial score of fold 0 is: 0.6031790759683753\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.04915\tvalid_1's rmse: 1.07143\n",
      "[200]\ttraining's rmse: 0.973087\tvalid_1's rmse: 1.01953\n",
      "[300]\ttraining's rmse: 0.934063\tvalid_1's rmse: 1.00167\n",
      "[400]\ttraining's rmse: 0.907302\tvalid_1's rmse: 0.993665\n",
      "[500]\ttraining's rmse: 0.886706\tvalid_1's rmse: 0.990091\n",
      "[600]\ttraining's rmse: 0.869148\tvalid_1's rmse: 0.988405\n",
      "[700]\ttraining's rmse: 0.853244\tvalid_1's rmse: 0.987338\n",
      "[800]\ttraining's rmse: 0.838768\tvalid_1's rmse: 0.987418\n",
      "Early stopping, best iteration is:\n",
      "[730]\ttraining's rmse: 0.848759\tvalid_1's rmse: 0.987107\n",
      "Partial score of fold 1 is: 0.5877350505914622\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.04884\tvalid_1's rmse: 1.07165\n",
      "[200]\ttraining's rmse: 0.973112\tvalid_1's rmse: 1.01745\n",
      "[300]\ttraining's rmse: 0.934113\tvalid_1's rmse: 0.99817\n",
      "[400]\ttraining's rmse: 0.907835\tvalid_1's rmse: 0.990414\n",
      "[500]\ttraining's rmse: 0.887054\tvalid_1's rmse: 0.98702\n",
      "[600]\ttraining's rmse: 0.869346\tvalid_1's rmse: 0.985537\n",
      "[700]\ttraining's rmse: 0.853753\tvalid_1's rmse: 0.985433\n",
      "[800]\ttraining's rmse: 0.83948\tvalid_1's rmse: 0.984956\n",
      "[900]\ttraining's rmse: 0.826344\tvalid_1's rmse: 0.984918\n",
      "Early stopping, best iteration is:\n",
      "[878]\ttraining's rmse: 0.829169\tvalid_1's rmse: 0.984811\n",
      "Partial score of fold 2 is: 0.594713493745513\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.04312\tvalid_1's rmse: 1.06915\n",
      "[200]\ttraining's rmse: 0.968261\tvalid_1's rmse: 1.01675\n",
      "[300]\ttraining's rmse: 0.930164\tvalid_1's rmse: 0.998739\n",
      "[400]\ttraining's rmse: 0.905329\tvalid_1's rmse: 0.992102\n",
      "[500]\ttraining's rmse: 0.885611\tvalid_1's rmse: 0.988781\n",
      "[600]\ttraining's rmse: 0.868204\tvalid_1's rmse: 0.987089\n",
      "[700]\ttraining's rmse: 0.852871\tvalid_1's rmse: 0.986609\n",
      "[800]\ttraining's rmse: 0.838555\tvalid_1's rmse: 0.986011\n",
      "[900]\ttraining's rmse: 0.825371\tvalid_1's rmse: 0.986109\n",
      "Early stopping, best iteration is:\n",
      "[854]\ttraining's rmse: 0.831475\tvalid_1's rmse: 0.985856\n",
      "Partial score of fold 3 is: 0.594355624865818\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.04386\tvalid_1's rmse: 1.06828\n",
      "[200]\ttraining's rmse: 0.969483\tvalid_1's rmse: 1.0157\n",
      "[300]\ttraining's rmse: 0.931622\tvalid_1's rmse: 0.996718\n",
      "[400]\ttraining's rmse: 0.906932\tvalid_1's rmse: 0.989738\n",
      "[500]\ttraining's rmse: 0.887028\tvalid_1's rmse: 0.98617\n",
      "[600]\ttraining's rmse: 0.869413\tvalid_1's rmse: 0.983571\n",
      "[700]\ttraining's rmse: 0.854065\tvalid_1's rmse: 0.982555\n",
      "[800]\ttraining's rmse: 0.839924\tvalid_1's rmse: 0.981807\n",
      "[900]\ttraining's rmse: 0.826521\tvalid_1's rmse: 0.981332\n",
      "[1000]\ttraining's rmse: 0.813984\tvalid_1's rmse: 0.981476\n",
      "Early stopping, best iteration is:\n",
      "[945]\ttraining's rmse: 0.82086\tvalid_1's rmse: 0.981195\n",
      "Partial score of fold 4 is: 0.5908697833839853\n",
      "Our oof cohen kappa score is:  0.5935637941298115\n",
      "kappa:  0.5935637941298115\n",
      "| \u001b[95m 18      \u001b[0m | \u001b[95m 0.5936  \u001b[0m | \u001b[95m 0.5102  \u001b[0m | \u001b[95m 4.566   \u001b[0m | \u001b[95m 0.2057  \u001b[0m | \u001b[95m 0.01187 \u001b[0m | \u001b[95m 16.8    \u001b[0m | \u001b[95m 1.218   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.847876\tvalid_1's rmse: 0.982035\n",
      "[200]\ttraining's rmse: 0.758577\tvalid_1's rmse: 0.984484\n",
      "Early stopping, best iteration is:\n",
      "[107]\ttraining's rmse: 0.839923\tvalid_1's rmse: 0.981448\n",
      "Partial score of fold 0 is: 0.598348523692066\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.845352\tvalid_1's rmse: 0.99492\n",
      "Early stopping, best iteration is:\n",
      "[76]\ttraining's rmse: 0.87379\tvalid_1's rmse: 0.994654\n",
      "Partial score of fold 1 is: 0.578251525279547\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.843932\tvalid_1's rmse: 0.997216\n",
      "Early stopping, best iteration is:\n",
      "[73]\ttraining's rmse: 0.877228\tvalid_1's rmse: 0.996612\n",
      "Partial score of fold 2 is: 0.5871982472719199\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.844628\tvalid_1's rmse: 0.990715\n",
      "Early stopping, best iteration is:\n",
      "[91]\ttraining's rmse: 0.855078\tvalid_1's rmse: 0.989391\n",
      "Partial score of fold 3 is: 0.5909558705087163\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.8478\tvalid_1's rmse: 0.989424\n",
      "[200]\ttraining's rmse: 0.756955\tvalid_1's rmse: 0.989071\n",
      "Early stopping, best iteration is:\n",
      "[125]\ttraining's rmse: 0.821294\tvalid_1's rmse: 0.988112\n",
      "Partial score of fold 4 is: 0.5880056033507951\n",
      "Our oof cohen kappa score is:  0.5883741807226983\n",
      "kappa:  0.5883741807226983\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.5884  \u001b[0m | \u001b[0m 0.5038  \u001b[0m | \u001b[0m 0.2239  \u001b[0m | \u001b[0m 4.957   \u001b[0m | \u001b[0m 0.08761 \u001b[0m | \u001b[0m 16.87   \u001b[0m | \u001b[0m 1.128   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.944694\tvalid_1's rmse: 0.994151\n",
      "[200]\ttraining's rmse: 0.886491\tvalid_1's rmse: 0.977324\n",
      "[300]\ttraining's rmse: 0.846713\tvalid_1's rmse: 0.974344\n",
      "[400]\ttraining's rmse: 0.813959\tvalid_1's rmse: 0.973639\n",
      "[500]\ttraining's rmse: 0.785345\tvalid_1's rmse: 0.973923\n",
      "Early stopping, best iteration is:\n",
      "[453]\ttraining's rmse: 0.798514\tvalid_1's rmse: 0.97345\n",
      "Partial score of fold 0 is: 0.6087252656189526\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.941148\tvalid_1's rmse: 0.999101\n",
      "[200]\ttraining's rmse: 0.882491\tvalid_1's rmse: 0.989717\n",
      "[300]\ttraining's rmse: 0.843164\tvalid_1's rmse: 0.989526\n",
      "Early stopping, best iteration is:\n",
      "[225]\ttraining's rmse: 0.871454\tvalid_1's rmse: 0.98897\n",
      "Partial score of fold 1 is: 0.5857667717531403\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.942331\tvalid_1's rmse: 0.998468\n",
      "[200]\ttraining's rmse: 0.883588\tvalid_1's rmse: 0.985589\n",
      "[300]\ttraining's rmse: 0.844077\tvalid_1's rmse: 0.984574\n",
      "Early stopping, best iteration is:\n",
      "[277]\ttraining's rmse: 0.852277\tvalid_1's rmse: 0.984079\n",
      "Partial score of fold 2 is: 0.5882718539110046\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.941681\tvalid_1's rmse: 1.00276\n",
      "[200]\ttraining's rmse: 0.882669\tvalid_1's rmse: 0.989735\n",
      "[300]\ttraining's rmse: 0.842834\tvalid_1's rmse: 0.987055\n",
      "[400]\ttraining's rmse: 0.810375\tvalid_1's rmse: 0.987527\n",
      "Early stopping, best iteration is:\n",
      "[323]\ttraining's rmse: 0.835003\tvalid_1's rmse: 0.986719\n",
      "Partial score of fold 3 is: 0.5893454605500893\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.943264\tvalid_1's rmse: 1.00241\n",
      "[200]\ttraining's rmse: 0.884311\tvalid_1's rmse: 0.986513\n",
      "[300]\ttraining's rmse: 0.844742\tvalid_1's rmse: 0.983019\n",
      "[400]\ttraining's rmse: 0.812775\tvalid_1's rmse: 0.98279\n",
      "[500]\ttraining's rmse: 0.783777\tvalid_1's rmse: 0.982407\n",
      "Early stopping, best iteration is:\n",
      "[469]\ttraining's rmse: 0.792452\tvalid_1's rmse: 0.982191\n",
      "Partial score of fold 4 is: 0.5888102785679687\n",
      "Our oof cohen kappa score is:  0.5917384818279994\n",
      "kappa:  0.5917384818279994\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.5917  \u001b[0m | \u001b[0m 0.6747  \u001b[0m | \u001b[0m 4.935   \u001b[0m | \u001b[0m 0.05846 \u001b[0m | \u001b[0m 0.02945 \u001b[0m | \u001b[0m 13.57   \u001b[0m | \u001b[0m 1.034   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.971828\tvalid_1's rmse: 1.00717\n",
      "[200]\ttraining's rmse: 0.915175\tvalid_1's rmse: 0.983087\n",
      "[300]\ttraining's rmse: 0.880864\tvalid_1's rmse: 0.977758\n",
      "[400]\ttraining's rmse: 0.853118\tvalid_1's rmse: 0.9762\n",
      "[500]\ttraining's rmse: 0.82926\tvalid_1's rmse: 0.976467\n",
      "[600]\ttraining's rmse: 0.807817\tvalid_1's rmse: 0.976333\n",
      "Early stopping, best iteration is:\n",
      "[540]\ttraining's rmse: 0.820541\tvalid_1's rmse: 0.975986\n",
      "Partial score of fold 0 is: 0.6010321638455711\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.968879\tvalid_1's rmse: 1.00929\n",
      "[200]\ttraining's rmse: 0.91133\tvalid_1's rmse: 0.990846\n",
      "[300]\ttraining's rmse: 0.876251\tvalid_1's rmse: 0.987888\n",
      "[400]\ttraining's rmse: 0.848871\tvalid_1's rmse: 0.987305\n",
      "[500]\ttraining's rmse: 0.824544\tvalid_1's rmse: 0.987734\n",
      "Early stopping, best iteration is:\n",
      "[429]\ttraining's rmse: 0.841457\tvalid_1's rmse: 0.987067\n",
      "Partial score of fold 1 is: 0.5886297227906995\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.969447\tvalid_1's rmse: 1.01105\n",
      "[200]\ttraining's rmse: 0.912551\tvalid_1's rmse: 0.990105\n",
      "[300]\ttraining's rmse: 0.878248\tvalid_1's rmse: 0.987033\n",
      "[400]\ttraining's rmse: 0.850797\tvalid_1's rmse: 0.98666\n",
      "[500]\ttraining's rmse: 0.827049\tvalid_1's rmse: 0.987051\n",
      "Early stopping, best iteration is:\n",
      "[412]\ttraining's rmse: 0.847791\tvalid_1's rmse: 0.98646\n",
      "Partial score of fold 2 is: 0.5905980016290215\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.968933\tvalid_1's rmse: 1.013\n",
      "[200]\ttraining's rmse: 0.912431\tvalid_1's rmse: 0.991156\n",
      "[300]\ttraining's rmse: 0.877724\tvalid_1's rmse: 0.987169\n",
      "[400]\ttraining's rmse: 0.849665\tvalid_1's rmse: 0.985978\n",
      "[500]\ttraining's rmse: 0.825683\tvalid_1's rmse: 0.986066\n",
      "Early stopping, best iteration is:\n",
      "[474]\ttraining's rmse: 0.831686\tvalid_1's rmse: 0.985623\n",
      "Partial score of fold 3 is: 0.5929241493470384\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.969888\tvalid_1's rmse: 1.01337\n",
      "[200]\ttraining's rmse: 0.91253\tvalid_1's rmse: 0.988188\n",
      "[300]\ttraining's rmse: 0.878058\tvalid_1's rmse: 0.982578\n",
      "[400]\ttraining's rmse: 0.850538\tvalid_1's rmse: 0.980598\n",
      "[500]\ttraining's rmse: 0.826606\tvalid_1's rmse: 0.979865\n",
      "[600]\ttraining's rmse: 0.80509\tvalid_1's rmse: 0.979749\n",
      "Early stopping, best iteration is:\n",
      "[549]\ttraining's rmse: 0.815758\tvalid_1's rmse: 0.979566\n",
      "Partial score of fold 4 is: 0.5942709971733987\n",
      "Our oof cohen kappa score is:  0.593850117628135\n",
      "kappa:  0.593850117628135\n",
      "| \u001b[95m 21      \u001b[0m | \u001b[95m 0.5939  \u001b[0m | \u001b[95m 0.7511  \u001b[0m | \u001b[95m 4.935   \u001b[0m | \u001b[95m 4.839   \u001b[0m | \u001b[95m 0.02149 \u001b[0m | \u001b[95m 14.08   \u001b[0m | \u001b[95m 1.113   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02162\tvalid_1's rmse: 1.04176\n",
      "[200]\ttraining's rmse: 0.956871\tvalid_1's rmse: 0.99814\n",
      "[300]\ttraining's rmse: 0.922724\tvalid_1's rmse: 0.98457\n",
      "[400]\ttraining's rmse: 0.898046\tvalid_1's rmse: 0.97844\n",
      "[500]\ttraining's rmse: 0.877545\tvalid_1's rmse: 0.976089\n",
      "[600]\ttraining's rmse: 0.85926\tvalid_1's rmse: 0.974945\n",
      "[700]\ttraining's rmse: 0.842397\tvalid_1's rmse: 0.974467\n",
      "[800]\ttraining's rmse: 0.826818\tvalid_1's rmse: 0.973952\n",
      "[900]\ttraining's rmse: 0.812277\tvalid_1's rmse: 0.974007\n",
      "Early stopping, best iteration is:\n",
      "[858]\ttraining's rmse: 0.818155\tvalid_1's rmse: 0.973774\n",
      "Partial score of fold 0 is: 0.6035368946555093\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01942\tvalid_1's rmse: 1.04189\n",
      "[200]\ttraining's rmse: 0.953511\tvalid_1's rmse: 1.00302\n",
      "[300]\ttraining's rmse: 0.919159\tvalid_1's rmse: 0.993866\n",
      "[400]\ttraining's rmse: 0.89374\tvalid_1's rmse: 0.99109\n",
      "[500]\ttraining's rmse: 0.873166\tvalid_1's rmse: 0.989893\n",
      "[600]\ttraining's rmse: 0.855061\tvalid_1's rmse: 0.989304\n",
      "Early stopping, best iteration is:\n",
      "[582]\ttraining's rmse: 0.858259\tvalid_1's rmse: 0.989068\n",
      "Partial score of fold 1 is: 0.584514230674208\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01978\tvalid_1's rmse: 1.0451\n",
      "[200]\ttraining's rmse: 0.954671\tvalid_1's rmse: 1.00439\n",
      "[300]\ttraining's rmse: 0.920754\tvalid_1's rmse: 0.992483\n",
      "[400]\ttraining's rmse: 0.895744\tvalid_1's rmse: 0.988335\n",
      "[500]\ttraining's rmse: 0.875488\tvalid_1's rmse: 0.986176\n",
      "[600]\ttraining's rmse: 0.857344\tvalid_1's rmse: 0.985408\n",
      "[700]\ttraining's rmse: 0.841088\tvalid_1's rmse: 0.985357\n",
      "Early stopping, best iteration is:\n",
      "[668]\ttraining's rmse: 0.846135\tvalid_1's rmse: 0.985094\n",
      "Partial score of fold 2 is: 0.5905980016290215\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02083\tvalid_1's rmse: 1.04657\n",
      "[200]\ttraining's rmse: 0.954786\tvalid_1's rmse: 1.00495\n",
      "[300]\ttraining's rmse: 0.920255\tvalid_1's rmse: 0.992816\n",
      "[400]\ttraining's rmse: 0.89533\tvalid_1's rmse: 0.987867\n",
      "[500]\ttraining's rmse: 0.874904\tvalid_1's rmse: 0.986269\n",
      "[600]\ttraining's rmse: 0.856457\tvalid_1's rmse: 0.985342\n",
      "[700]\ttraining's rmse: 0.840038\tvalid_1's rmse: 0.98494\n",
      "[800]\ttraining's rmse: 0.824899\tvalid_1's rmse: 0.985255\n",
      "Early stopping, best iteration is:\n",
      "[728]\ttraining's rmse: 0.835739\tvalid_1's rmse: 0.984728\n",
      "Partial score of fold 3 is: 0.5920294771478012\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02043\tvalid_1's rmse: 1.04868\n",
      "[200]\ttraining's rmse: 0.955072\tvalid_1's rmse: 1.00681\n",
      "[300]\ttraining's rmse: 0.921082\tvalid_1's rmse: 0.994136\n",
      "[400]\ttraining's rmse: 0.895962\tvalid_1's rmse: 0.988213\n",
      "[500]\ttraining's rmse: 0.875295\tvalid_1's rmse: 0.985107\n",
      "[600]\ttraining's rmse: 0.857405\tvalid_1's rmse: 0.983628\n",
      "[700]\ttraining's rmse: 0.840866\tvalid_1's rmse: 0.982806\n",
      "[800]\ttraining's rmse: 0.825439\tvalid_1's rmse: 0.982874\n",
      "Early stopping, best iteration is:\n",
      "[729]\ttraining's rmse: 0.83635\tvalid_1's rmse: 0.982414\n",
      "Partial score of fold 4 is: 0.5890796708632414\n",
      "Our oof cohen kappa score is:  0.592525871448389\n",
      "kappa:  0.592525871448389\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.5925  \u001b[0m | \u001b[0m 0.9887  \u001b[0m | \u001b[0m 0.06079 \u001b[0m | \u001b[0m 0.1851  \u001b[0m | \u001b[0m 0.01074 \u001b[0m | \u001b[0m 16.83   \u001b[0m | \u001b[0m 2.626   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.00456\tvalid_1's rmse: 1.02885\n",
      "[200]\ttraining's rmse: 0.942084\tvalid_1's rmse: 0.991871\n",
      "[300]\ttraining's rmse: 0.907835\tvalid_1's rmse: 0.981811\n",
      "[400]\ttraining's rmse: 0.882018\tvalid_1's rmse: 0.977705\n",
      "[500]\ttraining's rmse: 0.860045\tvalid_1's rmse: 0.975988\n",
      "[600]\ttraining's rmse: 0.839998\tvalid_1's rmse: 0.974619\n",
      "[700]\ttraining's rmse: 0.821012\tvalid_1's rmse: 0.974286\n",
      "Early stopping, best iteration is:\n",
      "[668]\ttraining's rmse: 0.826793\tvalid_1's rmse: 0.974159\n",
      "Partial score of fold 0 is: 0.6024634385941072\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.00078\tvalid_1's rmse: 1.02989\n",
      "[200]\ttraining's rmse: 0.937434\tvalid_1's rmse: 0.997253\n",
      "[300]\ttraining's rmse: 0.901798\tvalid_1's rmse: 0.989935\n",
      "[400]\ttraining's rmse: 0.876035\tvalid_1's rmse: 0.988443\n",
      "[500]\ttraining's rmse: 0.853631\tvalid_1's rmse: 0.987766\n",
      "Early stopping, best iteration is:\n",
      "[487]\ttraining's rmse: 0.856418\tvalid_1's rmse: 0.987515\n",
      "Partial score of fold 1 is: 0.584872099553903\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.00258\tvalid_1's rmse: 1.03375\n",
      "[200]\ttraining's rmse: 0.939809\tvalid_1's rmse: 1.0006\n",
      "[300]\ttraining's rmse: 0.905136\tvalid_1's rmse: 0.992217\n",
      "[400]\ttraining's rmse: 0.879252\tvalid_1's rmse: 0.988674\n",
      "[500]\ttraining's rmse: 0.857097\tvalid_1's rmse: 0.987156\n",
      "[600]\ttraining's rmse: 0.837633\tvalid_1's rmse: 0.986102\n",
      "Early stopping, best iteration is:\n",
      "[578]\ttraining's rmse: 0.841598\tvalid_1's rmse: 0.985861\n",
      "Partial score of fold 2 is: 0.5875561161516147\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.00288\tvalid_1's rmse: 1.0353\n",
      "[200]\ttraining's rmse: 0.938422\tvalid_1's rmse: 0.999683\n",
      "[300]\ttraining's rmse: 0.903854\tvalid_1's rmse: 0.990167\n",
      "[400]\ttraining's rmse: 0.878574\tvalid_1's rmse: 0.987158\n",
      "[500]\ttraining's rmse: 0.856956\tvalid_1's rmse: 0.985109\n",
      "[600]\ttraining's rmse: 0.837303\tvalid_1's rmse: 0.984451\n",
      "Early stopping, best iteration is:\n",
      "[594]\ttraining's rmse: 0.838247\tvalid_1's rmse: 0.984377\n",
      "Partial score of fold 3 is: 0.5954292315049028\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.00375\tvalid_1's rmse: 1.03722\n",
      "[200]\ttraining's rmse: 0.938425\tvalid_1's rmse: 0.999982\n",
      "[300]\ttraining's rmse: 0.904357\tvalid_1's rmse: 0.989916\n",
      "[400]\ttraining's rmse: 0.878708\tvalid_1's rmse: 0.985413\n",
      "[500]\ttraining's rmse: 0.856532\tvalid_1's rmse: 0.983683\n",
      "[600]\ttraining's rmse: 0.837119\tvalid_1's rmse: 0.98182\n",
      "[700]\ttraining's rmse: 0.818789\tvalid_1's rmse: 0.981514\n",
      "Early stopping, best iteration is:\n",
      "[614]\ttraining's rmse: 0.834401\tvalid_1's rmse: 0.981417\n",
      "Partial score of fold 4 is: 0.5901537383756877\n",
      "Our oof cohen kappa score is:  0.5918100627025802\n",
      "kappa:  0.5918100627025802\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.5918  \u001b[0m | \u001b[0m 0.8491  \u001b[0m | \u001b[0m 0.2743  \u001b[0m | \u001b[0m 0.5687  \u001b[0m | \u001b[0m 0.01346 \u001b[0m | \u001b[0m 16.5    \u001b[0m | \u001b[0m 9.654   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.979609\tvalid_1's rmse: 1.0187\n",
      "[200]\ttraining's rmse: 0.916412\tvalid_1's rmse: 0.986772\n",
      "[300]\ttraining's rmse: 0.88014\tvalid_1's rmse: 0.979918\n",
      "[400]\ttraining's rmse: 0.851361\tvalid_1's rmse: 0.978266\n",
      "[500]\ttraining's rmse: 0.82673\tvalid_1's rmse: 0.977738\n",
      "[600]\ttraining's rmse: 0.804852\tvalid_1's rmse: 0.977409\n",
      "[700]\ttraining's rmse: 0.784393\tvalid_1's rmse: 0.977561\n",
      "Early stopping, best iteration is:\n",
      "[633]\ttraining's rmse: 0.797899\tvalid_1's rmse: 0.977002\n",
      "Partial score of fold 0 is: 0.6003165264713031\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.977897\tvalid_1's rmse: 1.02006\n",
      "[200]\ttraining's rmse: 0.913779\tvalid_1's rmse: 0.99298\n",
      "[300]\ttraining's rmse: 0.87691\tvalid_1's rmse: 0.989147\n",
      "[400]\ttraining's rmse: 0.848557\tvalid_1's rmse: 0.987958\n",
      "[500]\ttraining's rmse: 0.823811\tvalid_1's rmse: 0.987972\n",
      "Early stopping, best iteration is:\n",
      "[458]\ttraining's rmse: 0.833876\tvalid_1's rmse: 0.987385\n",
      "Partial score of fold 1 is: 0.5861246406328351\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.978773\tvalid_1's rmse: 1.01917\n",
      "[200]\ttraining's rmse: 0.91468\tvalid_1's rmse: 0.991825\n",
      "[300]\ttraining's rmse: 0.877803\tvalid_1's rmse: 0.987595\n",
      "[400]\ttraining's rmse: 0.849399\tvalid_1's rmse: 0.986626\n",
      "[500]\ttraining's rmse: 0.825007\tvalid_1's rmse: 0.986843\n",
      "Early stopping, best iteration is:\n",
      "[437]\ttraining's rmse: 0.839891\tvalid_1's rmse: 0.98627\n",
      "Partial score of fold 2 is: 0.5823670173960386\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.972822\tvalid_1's rmse: 1.01891\n",
      "[200]\ttraining's rmse: 0.911901\tvalid_1's rmse: 0.993348\n",
      "[300]\ttraining's rmse: 0.877199\tvalid_1's rmse: 0.988204\n",
      "[400]\ttraining's rmse: 0.849537\tvalid_1's rmse: 0.987254\n",
      "[500]\ttraining's rmse: 0.825323\tvalid_1's rmse: 0.986573\n",
      "Early stopping, best iteration is:\n",
      "[450]\ttraining's rmse: 0.836978\tvalid_1's rmse: 0.986162\n",
      "Partial score of fold 3 is: 0.5939977559861231\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.975497\tvalid_1's rmse: 1.02\n",
      "[200]\ttraining's rmse: 0.913223\tvalid_1's rmse: 0.991158\n",
      "[300]\ttraining's rmse: 0.877729\tvalid_1's rmse: 0.985526\n",
      "[400]\ttraining's rmse: 0.850044\tvalid_1's rmse: 0.98423\n",
      "[500]\ttraining's rmse: 0.825565\tvalid_1's rmse: 0.983468\n",
      "Early stopping, best iteration is:\n",
      "[482]\ttraining's rmse: 0.829842\tvalid_1's rmse: 0.983306\n",
      "Partial score of fold 4 is: 0.5851414233176049\n",
      "Our oof cohen kappa score is:  0.589662636465154\n",
      "kappa:  0.589662636465154\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.5897  \u001b[0m | \u001b[0m 0.5152  \u001b[0m | \u001b[0m 4.685   \u001b[0m | \u001b[0m 4.958   \u001b[0m | \u001b[0m 0.0232  \u001b[0m | \u001b[0m 16.59   \u001b[0m | \u001b[0m 9.71    \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.944446\tvalid_1's rmse: 0.995224\n",
      "[200]\ttraining's rmse: 0.885652\tvalid_1's rmse: 0.97961\n",
      "[300]\ttraining's rmse: 0.844812\tvalid_1's rmse: 0.975607\n",
      "[400]\ttraining's rmse: 0.811206\tvalid_1's rmse: 0.975437\n",
      "Early stopping, best iteration is:\n",
      "[350]\ttraining's rmse: 0.827432\tvalid_1's rmse: 0.975226\n",
      "Partial score of fold 0 is: 0.5988852517227671\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.941457\tvalid_1's rmse: 0.999862\n",
      "[200]\ttraining's rmse: 0.882056\tvalid_1's rmse: 0.989113\n",
      "[300]\ttraining's rmse: 0.842189\tvalid_1's rmse: 0.989949\n",
      "Early stopping, best iteration is:\n",
      "[219]\ttraining's rmse: 0.873598\tvalid_1's rmse: 0.988535\n",
      "Partial score of fold 1 is: 0.5814723451968014\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.942392\tvalid_1's rmse: 0.998396\n",
      "[200]\ttraining's rmse: 0.882828\tvalid_1's rmse: 0.98634\n",
      "[300]\ttraining's rmse: 0.842876\tvalid_1's rmse: 0.986629\n",
      "Early stopping, best iteration is:\n",
      "[231]\ttraining's rmse: 0.869302\tvalid_1's rmse: 0.985952\n",
      "Partial score of fold 2 is: 0.5902401327493265\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.941504\tvalid_1's rmse: 1.00325\n",
      "[200]\ttraining's rmse: 0.882563\tvalid_1's rmse: 0.989202\n",
      "[300]\ttraining's rmse: 0.842358\tvalid_1's rmse: 0.985847\n",
      "[400]\ttraining's rmse: 0.809803\tvalid_1's rmse: 0.986277\n",
      "Early stopping, best iteration is:\n",
      "[322]\ttraining's rmse: 0.834782\tvalid_1's rmse: 0.985515\n",
      "Partial score of fold 3 is: 0.5945345593056655\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.942979\tvalid_1's rmse: 1.00098\n",
      "[200]\ttraining's rmse: 0.882489\tvalid_1's rmse: 0.984941\n",
      "[300]\ttraining's rmse: 0.842795\tvalid_1's rmse: 0.982221\n",
      "Early stopping, best iteration is:\n",
      "[293]\ttraining's rmse: 0.845233\tvalid_1's rmse: 0.982013\n",
      "Partial score of fold 4 is: 0.5885426371070183\n",
      "Our oof cohen kappa score is:  0.5908795113330289\n",
      "kappa:  0.5908795113330289\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.5909  \u001b[0m | \u001b[0m 0.5568  \u001b[0m | \u001b[0m 4.086   \u001b[0m | \u001b[0m 4.287   \u001b[0m | \u001b[0m 0.03151 \u001b[0m | \u001b[0m 16.82   \u001b[0m | \u001b[0m 1.101   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.940235\tvalid_1's rmse: 0.994586\n",
      "[200]\ttraining's rmse: 0.877765\tvalid_1's rmse: 0.978288\n",
      "[300]\ttraining's rmse: 0.834596\tvalid_1's rmse: 0.975098\n",
      "[400]\ttraining's rmse: 0.79841\tvalid_1's rmse: 0.974426\n",
      "[500]\ttraining's rmse: 0.767516\tvalid_1's rmse: 0.975555\n",
      "Early stopping, best iteration is:\n",
      "[404]\ttraining's rmse: 0.796996\tvalid_1's rmse: 0.974225\n",
      "Partial score of fold 0 is: 0.6030001666248083\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.936334\tvalid_1's rmse: 0.998496\n",
      "[200]\ttraining's rmse: 0.873469\tvalid_1's rmse: 0.991383\n",
      "[300]\ttraining's rmse: 0.830638\tvalid_1's rmse: 0.991043\n",
      "Early stopping, best iteration is:\n",
      "[250]\ttraining's rmse: 0.851156\tvalid_1's rmse: 0.990422\n",
      "Partial score of fold 1 is: 0.5859457061929877\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.938165\tvalid_1's rmse: 0.998607\n",
      "[200]\ttraining's rmse: 0.875728\tvalid_1's rmse: 0.987444\n",
      "[300]\ttraining's rmse: 0.833013\tvalid_1's rmse: 0.988565\n",
      "Early stopping, best iteration is:\n",
      "[232]\ttraining's rmse: 0.860389\tvalid_1's rmse: 0.987235\n",
      "Partial score of fold 2 is: 0.5893454605500893\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.938459\tvalid_1's rmse: 1.00235\n",
      "[200]\ttraining's rmse: 0.875336\tvalid_1's rmse: 0.987044\n",
      "[300]\ttraining's rmse: 0.832734\tvalid_1's rmse: 0.984017\n",
      "[400]\ttraining's rmse: 0.796652\tvalid_1's rmse: 0.984707\n",
      "Early stopping, best iteration is:\n",
      "[354]\ttraining's rmse: 0.813163\tvalid_1's rmse: 0.983822\n",
      "Partial score of fold 3 is: 0.5932820182267333\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.939957\tvalid_1's rmse: 1.00312\n",
      "[200]\ttraining's rmse: 0.875216\tvalid_1's rmse: 0.987116\n",
      "[300]\ttraining's rmse: 0.833433\tvalid_1's rmse: 0.984845\n",
      "[400]\ttraining's rmse: 0.797694\tvalid_1's rmse: 0.984509\n",
      "[500]\ttraining's rmse: 0.765995\tvalid_1's rmse: 0.984502\n",
      "Early stopping, best iteration is:\n",
      "[440]\ttraining's rmse: 0.78469\tvalid_1's rmse: 0.984026\n",
      "Partial score of fold 4 is: 0.5872895583424975\n",
      "Our oof cohen kappa score is:  0.5905304978712063\n",
      "kappa:  0.5905304978712063\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.5905  \u001b[0m | \u001b[0m 0.6929  \u001b[0m | \u001b[0m 0.362   \u001b[0m | \u001b[0m 0.1636  \u001b[0m | \u001b[0m 0.02829 \u001b[0m | \u001b[0m 16.44   \u001b[0m | \u001b[0m 1.152   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.945019\tvalid_1's rmse: 0.994318\n",
      "[200]\ttraining's rmse: 0.886734\tvalid_1's rmse: 0.978757\n",
      "[300]\ttraining's rmse: 0.847071\tvalid_1's rmse: 0.975209\n",
      "[400]\ttraining's rmse: 0.814837\tvalid_1's rmse: 0.975059\n",
      "Early stopping, best iteration is:\n",
      "[364]\ttraining's rmse: 0.826091\tvalid_1's rmse: 0.974481\n",
      "Partial score of fold 0 is: 0.6008532545020042\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.942017\tvalid_1's rmse: 0.999605\n",
      "[200]\ttraining's rmse: 0.883886\tvalid_1's rmse: 0.990156\n",
      "Early stopping, best iteration is:\n",
      "[198]\ttraining's rmse: 0.884766\tvalid_1's rmse: 0.990071\n",
      "Partial score of fold 1 is: 0.5846931651140554\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.94317\tvalid_1's rmse: 0.999366\n",
      "[200]\ttraining's rmse: 0.884951\tvalid_1's rmse: 0.987271\n",
      "[300]\ttraining's rmse: 0.845823\tvalid_1's rmse: 0.986057\n",
      "Early stopping, best iteration is:\n",
      "[272]\ttraining's rmse: 0.855693\tvalid_1's rmse: 0.985753\n",
      "Partial score of fold 2 is: 0.5889875916703944\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.941634\tvalid_1's rmse: 1.00218\n",
      "[200]\ttraining's rmse: 0.883437\tvalid_1's rmse: 0.98913\n",
      "[300]\ttraining's rmse: 0.843969\tvalid_1's rmse: 0.987294\n",
      "[400]\ttraining's rmse: 0.811972\tvalid_1's rmse: 0.987992\n",
      "Early stopping, best iteration is:\n",
      "[314]\ttraining's rmse: 0.839258\tvalid_1's rmse: 0.987015\n",
      "Partial score of fold 3 is: 0.5871982472719199\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.943368\tvalid_1's rmse: 1.00024\n",
      "[200]\ttraining's rmse: 0.8851\tvalid_1's rmse: 0.985376\n",
      "[300]\ttraining's rmse: 0.846553\tvalid_1's rmse: 0.982356\n",
      "[400]\ttraining's rmse: 0.81485\tvalid_1's rmse: 0.98187\n",
      "Early stopping, best iteration is:\n",
      "[348]\ttraining's rmse: 0.830705\tvalid_1's rmse: 0.981351\n",
      "Partial score of fold 4 is: 0.5846043895613817\n",
      "Our oof cohen kappa score is:  0.5890184085939262\n",
      "kappa:  0.5890184085939262\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.589   \u001b[0m | \u001b[0m 0.6156  \u001b[0m | \u001b[0m 4.883   \u001b[0m | \u001b[0m 4.821   \u001b[0m | \u001b[0m 0.03112 \u001b[0m | \u001b[0m 13.75   \u001b[0m | \u001b[0m 1.155   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.951706\tvalid_1's rmse: 0.993321\n",
      "[200]\ttraining's rmse: 0.896708\tvalid_1's rmse: 0.978189\n",
      "[300]\ttraining's rmse: 0.860194\tvalid_1's rmse: 0.9763\n",
      "[400]\ttraining's rmse: 0.828698\tvalid_1's rmse: 0.974652\n",
      "Early stopping, best iteration is:\n",
      "[397]\ttraining's rmse: 0.829631\tvalid_1's rmse: 0.974633\n",
      "Partial score of fold 0 is: 0.5996917470768636\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.948347\tvalid_1's rmse: 1.00082\n",
      "[200]\ttraining's rmse: 0.892626\tvalid_1's rmse: 0.991839\n",
      "[300]\ttraining's rmse: 0.854816\tvalid_1's rmse: 0.992178\n",
      "Early stopping, best iteration is:\n",
      "[252]\ttraining's rmse: 0.871926\tvalid_1's rmse: 0.991574\n",
      "Partial score of fold 1 is: 0.582545951835886\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.949135\tvalid_1's rmse: 1.00178\n",
      "[200]\ttraining's rmse: 0.893885\tvalid_1's rmse: 0.990284\n",
      "[300]\ttraining's rmse: 0.856979\tvalid_1's rmse: 0.989764\n",
      "Early stopping, best iteration is:\n",
      "[247]\ttraining's rmse: 0.875472\tvalid_1's rmse: 0.989089\n",
      "Partial score of fold 2 is: 0.5861246406328351\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.947765\tvalid_1's rmse: 1.00184\n",
      "[200]\ttraining's rmse: 0.892387\tvalid_1's rmse: 0.988084\n",
      "[300]\ttraining's rmse: 0.854953\tvalid_1's rmse: 0.98597\n",
      "[400]\ttraining's rmse: 0.824484\tvalid_1's rmse: 0.98737\n",
      "Early stopping, best iteration is:\n",
      "[304]\ttraining's rmse: 0.853711\tvalid_1's rmse: 0.985865\n",
      "Partial score of fold 3 is: 0.5961449692642926\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.94765\tvalid_1's rmse: 1.00309\n",
      "[200]\ttraining's rmse: 0.892623\tvalid_1's rmse: 0.98777\n",
      "[300]\ttraining's rmse: 0.856201\tvalid_1's rmse: 0.984752\n",
      "[400]\ttraining's rmse: 0.825275\tvalid_1's rmse: 0.983801\n",
      "[500]\ttraining's rmse: 0.797945\tvalid_1's rmse: 0.983544\n",
      "Early stopping, best iteration is:\n",
      "[423]\ttraining's rmse: 0.818834\tvalid_1's rmse: 0.983326\n",
      "Partial score of fold 4 is: 0.5840673558051586\n",
      "Our oof cohen kappa score is:  0.5907363495838671\n",
      "kappa:  0.5907363495838671\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.5907  \u001b[0m | \u001b[0m 0.9502  \u001b[0m | \u001b[0m 4.621   \u001b[0m | \u001b[0m 0.4824  \u001b[0m | \u001b[0m 0.02569 \u001b[0m | \u001b[0m 13.69   \u001b[0m | \u001b[0m 9.776   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02191\tvalid_1's rmse: 1.04182\n",
      "[200]\ttraining's rmse: 0.958424\tvalid_1's rmse: 0.998745\n",
      "[300]\ttraining's rmse: 0.92474\tvalid_1's rmse: 0.984092\n",
      "[400]\ttraining's rmse: 0.900653\tvalid_1's rmse: 0.978266\n",
      "[500]\ttraining's rmse: 0.880903\tvalid_1's rmse: 0.976024\n",
      "[600]\ttraining's rmse: 0.863276\tvalid_1's rmse: 0.97442\n",
      "[700]\ttraining's rmse: 0.847125\tvalid_1's rmse: 0.973293\n",
      "[800]\ttraining's rmse: 0.831944\tvalid_1's rmse: 0.973206\n",
      "Early stopping, best iteration is:\n",
      "[709]\ttraining's rmse: 0.845672\tvalid_1's rmse: 0.973082\n",
      "Partial score of fold 0 is: 0.6004954358148702\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01989\tvalid_1's rmse: 1.04122\n",
      "[200]\ttraining's rmse: 0.955022\tvalid_1's rmse: 1.00307\n",
      "[300]\ttraining's rmse: 0.92111\tvalid_1's rmse: 0.992543\n",
      "[400]\ttraining's rmse: 0.896697\tvalid_1's rmse: 0.98934\n",
      "[500]\ttraining's rmse: 0.876695\tvalid_1's rmse: 0.988146\n",
      "[600]\ttraining's rmse: 0.858891\tvalid_1's rmse: 0.988518\n",
      "Early stopping, best iteration is:\n",
      "[560]\ttraining's rmse: 0.865841\tvalid_1's rmse: 0.988115\n",
      "Partial score of fold 1 is: 0.5870193128320724\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02041\tvalid_1's rmse: 1.04565\n",
      "[200]\ttraining's rmse: 0.956522\tvalid_1's rmse: 1.0047\n",
      "[300]\ttraining's rmse: 0.922775\tvalid_1's rmse: 0.992593\n",
      "[400]\ttraining's rmse: 0.898828\tvalid_1's rmse: 0.987985\n",
      "[500]\ttraining's rmse: 0.87891\tvalid_1's rmse: 0.9856\n",
      "[600]\ttraining's rmse: 0.861124\tvalid_1's rmse: 0.984649\n",
      "[700]\ttraining's rmse: 0.845271\tvalid_1's rmse: 0.984447\n",
      "Early stopping, best iteration is:\n",
      "[668]\ttraining's rmse: 0.850223\tvalid_1's rmse: 0.984268\n",
      "Partial score of fold 2 is: 0.5932820182267333\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02136\tvalid_1's rmse: 1.0467\n",
      "[200]\ttraining's rmse: 0.955374\tvalid_1's rmse: 1.00501\n",
      "[300]\ttraining's rmse: 0.922304\tvalid_1's rmse: 0.99336\n",
      "[400]\ttraining's rmse: 0.898493\tvalid_1's rmse: 0.989836\n",
      "[500]\ttraining's rmse: 0.878483\tvalid_1's rmse: 0.988055\n",
      "[600]\ttraining's rmse: 0.860569\tvalid_1's rmse: 0.987005\n",
      "[700]\ttraining's rmse: 0.844785\tvalid_1's rmse: 0.986413\n",
      "[800]\ttraining's rmse: 0.830032\tvalid_1's rmse: 0.986094\n",
      "[900]\ttraining's rmse: 0.81617\tvalid_1's rmse: 0.985771\n",
      "[1000]\ttraining's rmse: 0.803111\tvalid_1's rmse: 0.985996\n",
      "Early stopping, best iteration is:\n",
      "[977]\ttraining's rmse: 0.80601\tvalid_1's rmse: 0.985645\n",
      "Partial score of fold 3 is: 0.5945345593056655\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02125\tvalid_1's rmse: 1.04851\n",
      "[200]\ttraining's rmse: 0.955685\tvalid_1's rmse: 1.00574\n",
      "[300]\ttraining's rmse: 0.922606\tvalid_1's rmse: 0.992957\n",
      "[400]\ttraining's rmse: 0.898503\tvalid_1's rmse: 0.987095\n",
      "[500]\ttraining's rmse: 0.878594\tvalid_1's rmse: 0.984434\n",
      "[600]\ttraining's rmse: 0.861072\tvalid_1's rmse: 0.982335\n",
      "[700]\ttraining's rmse: 0.845154\tvalid_1's rmse: 0.981625\n",
      "[800]\ttraining's rmse: 0.830531\tvalid_1's rmse: 0.981126\n",
      "[900]\ttraining's rmse: 0.816816\tvalid_1's rmse: 0.981074\n",
      "[1000]\ttraining's rmse: 0.80374\tvalid_1's rmse: 0.981035\n",
      "Early stopping, best iteration is:\n",
      "[942]\ttraining's rmse: 0.811109\tvalid_1's rmse: 0.980833\n",
      "Partial score of fold 4 is: 0.5863945020821256\n",
      "Our oof cohen kappa score is:  0.5922395479500654\n",
      "kappa:  0.5922395479500654\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.5922  \u001b[0m | \u001b[0m 0.9126  \u001b[0m | \u001b[0m 2.809   \u001b[0m | \u001b[0m 0.3507  \u001b[0m | \u001b[0m 0.01117 \u001b[0m | \u001b[0m 16.85   \u001b[0m | \u001b[0m 1.18    \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03445\tvalid_1's rmse: 1.05592\n",
      "[200]\ttraining's rmse: 0.965588\tvalid_1's rmse: 1.00563\n",
      "[300]\ttraining's rmse: 0.93002\tvalid_1's rmse: 0.98803\n",
      "[400]\ttraining's rmse: 0.905654\tvalid_1's rmse: 0.980971\n",
      "[500]\ttraining's rmse: 0.885669\tvalid_1's rmse: 0.977262\n",
      "[600]\ttraining's rmse: 0.868565\tvalid_1's rmse: 0.975535\n",
      "[700]\ttraining's rmse: 0.85279\tvalid_1's rmse: 0.97407\n",
      "[800]\ttraining's rmse: 0.838361\tvalid_1's rmse: 0.973183\n",
      "[900]\ttraining's rmse: 0.824857\tvalid_1's rmse: 0.972819\n",
      "[1000]\ttraining's rmse: 0.812381\tvalid_1's rmse: 0.972815\n",
      "Early stopping, best iteration is:\n",
      "[950]\ttraining's rmse: 0.818408\tvalid_1's rmse: 0.97269\n",
      "Partial score of fold 0 is: 0.6042525320297774\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03313\tvalid_1's rmse: 1.05426\n",
      "[200]\ttraining's rmse: 0.962683\tvalid_1's rmse: 1.00934\n",
      "[300]\ttraining's rmse: 0.926909\tvalid_1's rmse: 0.995086\n",
      "[400]\ttraining's rmse: 0.901854\tvalid_1's rmse: 0.989913\n",
      "[500]\ttraining's rmse: 0.882043\tvalid_1's rmse: 0.988166\n",
      "[600]\ttraining's rmse: 0.864894\tvalid_1's rmse: 0.987377\n",
      "Early stopping, best iteration is:\n",
      "[587]\ttraining's rmse: 0.867088\tvalid_1's rmse: 0.987287\n",
      "Partial score of fold 1 is: 0.5850510339937505\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03288\tvalid_1's rmse: 1.0566\n",
      "[200]\ttraining's rmse: 0.96256\tvalid_1's rmse: 1.0082\n",
      "[300]\ttraining's rmse: 0.92678\tvalid_1's rmse: 0.992927\n",
      "[400]\ttraining's rmse: 0.902645\tvalid_1's rmse: 0.988141\n",
      "[500]\ttraining's rmse: 0.883062\tvalid_1's rmse: 0.985725\n",
      "[600]\ttraining's rmse: 0.865798\tvalid_1's rmse: 0.985073\n",
      "[700]\ttraining's rmse: 0.850363\tvalid_1's rmse: 0.984761\n",
      "[800]\ttraining's rmse: 0.836187\tvalid_1's rmse: 0.984792\n",
      "Early stopping, best iteration is:\n",
      "[721]\ttraining's rmse: 0.847282\tvalid_1's rmse: 0.984606\n",
      "Partial score of fold 2 is: 0.5913137393884114\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03557\tvalid_1's rmse: 1.06121\n",
      "[200]\ttraining's rmse: 0.962737\tvalid_1's rmse: 1.01157\n",
      "[300]\ttraining's rmse: 0.926592\tvalid_1's rmse: 0.995858\n",
      "[400]\ttraining's rmse: 0.902364\tvalid_1's rmse: 0.990097\n",
      "[500]\ttraining's rmse: 0.882705\tvalid_1's rmse: 0.987301\n",
      "[600]\ttraining's rmse: 0.86536\tvalid_1's rmse: 0.986115\n",
      "[700]\ttraining's rmse: 0.850086\tvalid_1's rmse: 0.985413\n",
      "[800]\ttraining's rmse: 0.835732\tvalid_1's rmse: 0.985058\n",
      "[900]\ttraining's rmse: 0.822395\tvalid_1's rmse: 0.985099\n",
      "Early stopping, best iteration is:\n",
      "[893]\ttraining's rmse: 0.823317\tvalid_1's rmse: 0.984995\n",
      "Partial score of fold 3 is: 0.5945345593056655\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03647\tvalid_1's rmse: 1.0618\n",
      "[200]\ttraining's rmse: 0.963605\tvalid_1's rmse: 1.01141\n",
      "[300]\ttraining's rmse: 0.927317\tvalid_1's rmse: 0.994484\n",
      "[400]\ttraining's rmse: 0.903152\tvalid_1's rmse: 0.988427\n",
      "[500]\ttraining's rmse: 0.883522\tvalid_1's rmse: 0.985587\n",
      "[600]\ttraining's rmse: 0.866448\tvalid_1's rmse: 0.98337\n",
      "[700]\ttraining's rmse: 0.851201\tvalid_1's rmse: 0.982762\n",
      "[800]\ttraining's rmse: 0.837482\tvalid_1's rmse: 0.981969\n",
      "Early stopping, best iteration is:\n",
      "[794]\ttraining's rmse: 0.838266\tvalid_1's rmse: 0.981923\n",
      "Partial score of fold 4 is: 0.5881846146028695\n",
      "Our oof cohen kappa score is:  0.5927764045094219\n",
      "kappa:  0.5927764045094219\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.5928  \u001b[0m | \u001b[0m 0.6964  \u001b[0m | \u001b[0m 4.569   \u001b[0m | \u001b[0m 0.4705  \u001b[0m | \u001b[0m 0.01162 \u001b[0m | \u001b[0m 13.1    \u001b[0m | \u001b[0m 1.139   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.0244\tvalid_1's rmse: 1.04289\n",
      "[200]\ttraining's rmse: 0.961928\tvalid_1's rmse: 0.999069\n",
      "[300]\ttraining's rmse: 0.929318\tvalid_1's rmse: 0.983717\n",
      "[400]\ttraining's rmse: 0.906335\tvalid_1's rmse: 0.977973\n",
      "[500]\ttraining's rmse: 0.88761\tvalid_1's rmse: 0.975445\n",
      "[600]\ttraining's rmse: 0.870775\tvalid_1's rmse: 0.974265\n",
      "[700]\ttraining's rmse: 0.855608\tvalid_1's rmse: 0.973122\n",
      "[800]\ttraining's rmse: 0.841588\tvalid_1's rmse: 0.972825\n",
      "[900]\ttraining's rmse: 0.828242\tvalid_1's rmse: 0.972536\n",
      "[1000]\ttraining's rmse: 0.815845\tvalid_1's rmse: 0.972662\n",
      "Early stopping, best iteration is:\n",
      "[967]\ttraining's rmse: 0.81975\tvalid_1's rmse: 0.972375\n",
      "Partial score of fold 0 is: 0.6024634385941072\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02236\tvalid_1's rmse: 1.04269\n",
      "[200]\ttraining's rmse: 0.958442\tvalid_1's rmse: 1.00406\n",
      "[300]\ttraining's rmse: 0.925508\tvalid_1's rmse: 0.992929\n",
      "[400]\ttraining's rmse: 0.902318\tvalid_1's rmse: 0.99003\n",
      "[500]\ttraining's rmse: 0.883398\tvalid_1's rmse: 0.988625\n",
      "[600]\ttraining's rmse: 0.866582\tvalid_1's rmse: 0.988196\n",
      "Early stopping, best iteration is:\n",
      "[598]\ttraining's rmse: 0.866925\tvalid_1's rmse: 0.988152\n",
      "Partial score of fold 1 is: 0.5877350505914622\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02291\tvalid_1's rmse: 1.04702\n",
      "[200]\ttraining's rmse: 0.959749\tvalid_1's rmse: 1.00542\n",
      "[300]\ttraining's rmse: 0.926948\tvalid_1's rmse: 0.993087\n",
      "[400]\ttraining's rmse: 0.903965\tvalid_1's rmse: 0.988519\n",
      "[500]\ttraining's rmse: 0.88499\tvalid_1's rmse: 0.986567\n",
      "[600]\ttraining's rmse: 0.868276\tvalid_1's rmse: 0.985952\n",
      "[700]\ttraining's rmse: 0.853341\tvalid_1's rmse: 0.985477\n",
      "[800]\ttraining's rmse: 0.839385\tvalid_1's rmse: 0.985099\n",
      "Early stopping, best iteration is:\n",
      "[774]\ttraining's rmse: 0.8429\tvalid_1's rmse: 0.985009\n",
      "Partial score of fold 2 is: 0.5893454605500893\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.0231\tvalid_1's rmse: 1.04703\n",
      "[200]\ttraining's rmse: 0.958858\tvalid_1's rmse: 1.00533\n",
      "[300]\ttraining's rmse: 0.926867\tvalid_1's rmse: 0.99323\n",
      "[400]\ttraining's rmse: 0.903813\tvalid_1's rmse: 0.988754\n",
      "[500]\ttraining's rmse: 0.884889\tvalid_1's rmse: 0.986593\n",
      "[600]\ttraining's rmse: 0.86814\tvalid_1's rmse: 0.985102\n",
      "[700]\ttraining's rmse: 0.853208\tvalid_1's rmse: 0.984902\n",
      "[800]\ttraining's rmse: 0.839324\tvalid_1's rmse: 0.984657\n",
      "[900]\ttraining's rmse: 0.826221\tvalid_1's rmse: 0.98468\n",
      "Early stopping, best iteration is:\n",
      "[878]\ttraining's rmse: 0.829069\tvalid_1's rmse: 0.984447\n",
      "Partial score of fold 3 is: 0.5972185759033773\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02306\tvalid_1's rmse: 1.04926\n",
      "[200]\ttraining's rmse: 0.959279\tvalid_1's rmse: 1.00681\n",
      "[300]\ttraining's rmse: 0.92725\tvalid_1's rmse: 0.993111\n",
      "[400]\ttraining's rmse: 0.904432\tvalid_1's rmse: 0.987231\n",
      "[500]\ttraining's rmse: 0.885482\tvalid_1's rmse: 0.984102\n",
      "[600]\ttraining's rmse: 0.869036\tvalid_1's rmse: 0.982659\n",
      "[700]\ttraining's rmse: 0.853982\tvalid_1's rmse: 0.981634\n",
      "[800]\ttraining's rmse: 0.840038\tvalid_1's rmse: 0.981592\n",
      "[900]\ttraining's rmse: 0.827126\tvalid_1's rmse: 0.981413\n",
      "[1000]\ttraining's rmse: 0.814741\tvalid_1's rmse: 0.981539\n",
      "Early stopping, best iteration is:\n",
      "[902]\ttraining's rmse: 0.826876\tvalid_1's rmse: 0.981363\n",
      "Partial score of fold 4 is: 0.5881846146028695\n",
      "Our oof cohen kappa score is:  0.5934564228179402\n",
      "kappa:  0.5934564228179402\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 0.5935  \u001b[0m | \u001b[0m 0.9359  \u001b[0m | \u001b[0m 4.689   \u001b[0m | \u001b[0m 2.152   \u001b[0m | \u001b[0m 0.01116 \u001b[0m | \u001b[0m 15.21   \u001b[0m | \u001b[0m 1.024   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.89701\tvalid_1's rmse: 0.979438\n",
      "[200]\ttraining's rmse: 0.829126\tvalid_1's rmse: 0.974739\n",
      "[300]\ttraining's rmse: 0.778549\tvalid_1's rmse: 0.977402\n",
      "Early stopping, best iteration is:\n",
      "[202]\ttraining's rmse: 0.827947\tvalid_1's rmse: 0.974529\n",
      "Partial score of fold 0 is: 0.598527433035633\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.894873\tvalid_1's rmse: 0.98958\n",
      "[200]\ttraining's rmse: 0.827992\tvalid_1's rmse: 0.990123\n",
      "Early stopping, best iteration is:\n",
      "[109]\ttraining's rmse: 0.887481\tvalid_1's rmse: 0.988996\n",
      "Partial score of fold 1 is: 0.5836195584749708\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.896387\tvalid_1's rmse: 0.986945\n",
      "[200]\ttraining's rmse: 0.829457\tvalid_1's rmse: 0.984413\n",
      "Early stopping, best iteration is:\n",
      "[151]\ttraining's rmse: 0.858924\tvalid_1's rmse: 0.983608\n",
      "Partial score of fold 2 is: 0.5909558705087163\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.895545\tvalid_1's rmse: 0.988888\n",
      "[200]\ttraining's rmse: 0.828122\tvalid_1's rmse: 0.986785\n",
      "Early stopping, best iteration is:\n",
      "[142]\ttraining's rmse: 0.863766\tvalid_1's rmse: 0.986557\n",
      "Partial score of fold 3 is: 0.5925662804673435\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.89672\tvalid_1's rmse: 0.988752\n",
      "[200]\ttraining's rmse: 0.829217\tvalid_1's rmse: 0.984322\n",
      "Early stopping, best iteration is:\n",
      "[178]\ttraining's rmse: 0.841873\tvalid_1's rmse: 0.983981\n",
      "Partial score of fold 4 is: 0.585678457073828\n",
      "Our oof cohen kappa score is:  0.5907721400211575\n",
      "kappa:  0.5907721400211575\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 0.5908  \u001b[0m | \u001b[0m 0.7439  \u001b[0m | \u001b[0m 4.773   \u001b[0m | \u001b[0m 0.05874 \u001b[0m | \u001b[0m 0.05045 \u001b[0m | \u001b[0m 16.93   \u001b[0m | \u001b[0m 1.116   \u001b[0m |\n",
      "=================================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEVdJREFUeJzt3X2MXFd5x/Hvg50XwBAbQl1ju3WqWGkDKhBWxhQJbQjKWys2UhPJqAKDgiy1KaRVJd4kagcSFSRKgLaA3DqSg1Cc1ES1G0Kpm2Ra8UcMcQgvwaTZBjUxcQlgx7AEQk2f/jFnw7Lseu7szs7s7Pl+pJXvPffcO+eZY89v770z48hMJEn1edagByBJGgwDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklSp5YMewKmcffbZuWHDhjnv/+Mf/5jnPve5vRvQgCyVOsBaFqOlUgdYy6RDhw59PzNf1Knfog6ADRs2cN999815/1arxejoaO8GNCBLpQ6wlsVoqdQB1jIpIv67ST8vAUlSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUW9SeBJWmgdpw1uMce3bfgD+EZgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZVqFAAR8ecR8WBEfCMibomIMyPinIg4GBEPR8StEXF66XtGWR8v2zdMOc57SvtDEXHJwpQkSWqiYwBExFrgHcBIZr4UWAZsAT4E3JiZG4HjwNVll6uB45l5LnBj6UdEnF/2ewlwKfCJiFjW23IkSU01vQS0HHh2RCwHngMcBV4H7C3bdwNXlOWxsk7ZflFERGnfk5lPZ+a3gXFg0/xLkCTNRcf/EjIzvxMRHwYeBX4C/CtwCHgyM0+WbkeAtWV5LfBY2fdkRJwAXlja751y6Kn7PCMitgHbAFavXk2r1eq+qmJiYmJe+y8WS6UOsJbFaKnUAQtQy3nX9e5YXerHvHQMgIhYRfu393OAJ4F/BC6boWtO7jLLttnaf7khcyewE2BkZCRHR0c7DXFWrVaL+ey/WCyVOsBaFqOlUgcsQC07xnp3rC61Rvct+Lw0uQT0euDbmfm9zPxf4Hbg94CV5ZIQwDrg8bJ8BFgPULafBRyb2j7DPpKkPmsSAI8CmyPiOeVa/kXAN4F7gCtLn63A5H9hv7+sU7bfnZlZ2reUdwmdA2wEvtSbMiRJ3WpyD+BgROwF7gdOAl+hfYnmc8CeiLi+tO0qu+wCPh0R47R/899SjvNgRNxGOzxOAtdk5s97XI8kqaGOAQCQmduB7dOaH2GGd/Fk5k+Bq2Y5zg3ADV2OUZK0APwksCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqVKMAiIiVEbE3Ir4VEYcj4tUR8YKIOBARD5c/V5W+EREfj4jxiPhaRFww5ThbS/+HI2LrQhUlSeqs6RnAx4B/yczfBl4GHAbeDdyVmRuBu8o6wGXAxvKzDfgkQES8ANgOvArYBGyfDA1JUv91DICIeD7wWmAXQGb+LDOfBMaA3aXbbuCKsjwG3Jxt9wIrI2INcAlwIDOPZeZx4ABwaU+rkSQ1Fpl56g4RLwd2At+k/dv/IeBa4DuZuXJKv+OZuSoi7gA+mJlfLO13Ae8CRoEzM/P60v4+4CeZ+eFpj7eN9pkDq1evfuWePXvmXNzExAQrVqyY8/6LxVKpA6xlMVoqdcAC1HL0gd4dq0sTzzt3zrVceOGFhzJzpFO/5Q2OtRy4AHh7Zh6MiI/xi8s9M4kZ2vIU7b/ckLmTduAwMjKSo6OjDYY4s1arxXz2XyyWSh1gLYvRUqkDFqCWHWO9O1aXWqP7FnxemtwDOAIcycyDZX0v7UD4brm0Q/nziSn910/Zfx3w+CnaJUkD0DEAMvN/gMci4rzSdBHty0H7gcl38mwF9pXl/cCby7uBNgMnMvMo8AXg4ohYVW7+XlzaJEkD0OQSEMDbgc9ExOnAI8BbaYfHbRFxNfAocFXpeydwOTAOPFX6kpnHIuIDwJdLv/dn5rGeVCFJ6lqjAMjMB4CZbihcNEPfBK6Z5Tg3ATd1M0BJ0sLwk8CSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqtTyQQ9gQR19AHaM9f9xd5zo/2NKUpcanwFExLKI+EpE3FHWz4mIgxHxcETcGhGnl/Yzyvp42b5hyjHeU9ofiohLel2MJKm5bi4BXQscnrL+IeDGzNwIHAeuLu1XA8cz81zgxtKPiDgf2AK8BLgU+ERELJvf8CVJc9UoACJiHfD7wD+U9QBeB+wtXXYDV5TlsbJO2X5R6T8G7MnMpzPz28A4sKkXRUiSutf0DOCjwDuB/yvrLwSezMyTZf0IsLYsrwUeAyjbT5T+z7TPsI8kqc863gSOiD8AnsjMQxExOtk8Q9fssO1U+0x9vG3ANoDVq1fTarU6DXFWE2e8mNZ51815/zmbx5hnMjExMa/nYTGxlsVnqdQBC1DLIF4/in7MS5N3Ab0GeENEXA6cCTyf9hnByohYXn7LXwc8XvofAdYDRyJiOXAWcGxK+6Sp+zwjM3cCOwFGRkZydHR0DmW1tW75KKMPbZ/z/nP2xt6+C6jVajGf52ExsZbFZ6nUAQtQyyDeRVi0Rvct+Lx0vASUme/JzHWZuYH2Tdy7M/OPgHuAK0u3rcC+sry/rFO2352ZWdq3lHcJnQNsBL7Us0okSV2Zz+cA3gXsiYjrga8Au0r7LuDTETFO+zf/LQCZ+WBE3AZ8EzgJXJOZP5/H40uS5qGrAMjMFtAqy48ww7t4MvOnwFWz7H8DcEO3g5Qk9Z5fBSFJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmq1PJBD0BLxI6zmvU77zrYMdbDxz3Ru2NJlfEMQJIqZQBIUqUMAEmqlAEgSZXyJrA0bJrecG+qmxvz3nRfUjqeAUTE+oi4JyIOR8SDEXFtaX9BRByIiIfLn6tKe0TExyNiPCK+FhEXTDnW1tL/4YjYunBlSZI6aXIJ6CTwF5n5O8Bm4JqIOB94N3BXZm4E7irrAJcBG8vPNuCT0A4MYDvwKmATsH0yNCRJ/dcxADLzaGbeX5Z/BBwG1gJjwO7SbTdwRVkeA27OtnuBlRGxBrgEOJCZxzLzOHAAuLSn1UiSGuvqJnBEbABeARwEVmfmUWiHBPBrpdta4LEpux0pbbO1S5IGIDKzWceIFcC/Azdk5u0R8WRmrpyy/XhmroqIzwF/lZlfLO13Ae8EXgeckZnXl/b3AU9l5l9Pe5xttC8dsXr16lfu2bNnzsVNHHuCFU8/Puf952zNy3t6uImJCVasWNHTY/bc0QcadZs448W9nZMeP9fdGNi8NHyum+pqTgb4fDfR8znp8XPdjYnnnTvnWi688MJDmTnSqV+jdwFFxGnAZ4HPZObtpfm7EbEmM4+WSzxPlPYjwPopu68DHi/to9PaW9MfKzN3AjsBRkZGcnR0dHqXxlq3fJTRh7bPef85e2Nv3ynRarWYz/PQFw3fRdI677rezkmPn+tuDGxeevlVGnQ5JwN8vpvo+Zz0+LnuRmt034L//WryLqAAdgGHM/MjUzbtBybfybMV2Del/c3l3UCbgRPlEtEXgIsjYlW5+XtxaZMkDUCTM4DXAG8Cvh4Rk+dD7wU+CNwWEVcDjwJXlW13ApcD48BTwFsBMvNYRHwA+HLp9/7MPNaTKiRJXesYAOVafsyy+aIZ+idwzSzHugm4qZsBSpIWhl8FIUmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIq1fcAiIhLI+KhiBiPiHf3+/ElSW19DYCIWAb8HXAZcD7wxog4v59jkCS19fsMYBMwnpmPZObPgD3AWJ/HIEmi/wGwFnhsyvqR0iZJ6rPIzP49WMRVwCWZ+bay/iZgU2a+fUqfbcC2snoe8NA8HvJs4Pvz2H+xWCp1gLUsRkulDrCWSb+ZmS/q1Gn5HA8+V0eA9VPW1wGPT+2QmTuBnb14sIi4LzNHenGsQVoqdYC1LEZLpQ6wlm71+xLQl4GNEXFORJwObAH293kMkiT6fAaQmScj4k+BLwDLgJsy88F+jkGS1NbvS0Bk5p3AnX16uJ5cSloElkodYC2L0VKpA6ylK329CSxJWjz8KghJqtTQB0Cnr5aIiDMi4tay/WBEbOj/KJtpUMtbIuJ7EfFA+XnbIMbZSUTcFBFPRMQ3ZtkeEfHxUufXIuKCfo+xqQa1jEbEiSlz8pf9HmMTEbE+Iu6JiMMR8WBEXDtDn6GYl4a1DMu8nBkRX4qIr5Zarpuhz8K9hmXm0P7QvpH8X8BvAacDXwXOn9bnT4BPleUtwK2DHvc8ankL8LeDHmuDWl4LXAB8Y5btlwOfBwLYDBwc9JjnUcsocMegx9mgjjXABWX5ecB/zvD3ayjmpWEtwzIvAawoy6cBB4HN0/os2GvYsJ8BNPlqiTFgd1neC1wUEdHHMTa1ZL4mIzP/Azh2ii5jwM3Zdi+wMiLW9Gd03WlQy1DIzKOZeX9Z/hFwmF/9FP5QzEvDWoZCea4nyupp5Wf6jdkFew0b9gBo8tUSz/TJzJPACeCFfRldd5p+TcYfltPzvRGxfobtw2CpfSXIq8sp/Ocj4iWDHkwn5RLCK2j/tjnV0M3LKWqBIZmXiFgWEQ8ATwAHMnPWeen1a9iwB8BMKTg9PZv0WQyajPOfgQ2Z+bvAv/GL3wqGzbDMSRP30/7Y/cuAvwH+acDjOaWIWAF8FvizzPzh9M0z7LJo56VDLUMzL5n588x8Oe1vRtgUES+d1mXB5mXYA6DjV0tM7RMRy4GzWJyn9E2+JuMHmfl0Wf174JV9GluvNZm3oZCZP5w8hc/2Z1xOi4izBzysGUXEabRfMD+TmbfP0GVo5qVTLcM0L5My80mgBVw6bdOCvYYNewA0+WqJ/cDWsnwlcHeWuymLTMdapl2PfQPta5/DaD/w5vKuk83Aicw8OuhBzUVE/Prk9diI2ET739QPBjuqX1XGuAs4nJkfmaXbUMxLk1qGaF5eFBEry/KzgdcD35rWbcFew/r+SeBeylm+WiIi3g/cl5n7af9F+XREjNNOzS2DG/HsGtbyjoh4A3CSdi1vGdiATyEibqH9LoyzI+IIsJ32zS0y81O0Pwl+OTAOPAW8dTAj7axBLVcCfxwRJ4GfAFsW6S8YrwHeBHy9XG8GeC/wGzB089KklmGZlzXA7mj/Z1nPAm7LzDv69RrmJ4ElqVLDfglIkjRHBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZX6fyziZk0dQpgsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def LGB_Beyes(subsample_freq,\n",
    "                    learning_rate,\n",
    "                    feature_fraction,\n",
    "                    max_depth,\n",
    "                    lambda_l1,\n",
    "                    lambda_l2):\n",
    "    params={}\n",
    "    params['subsample_freq']=subsample_freq\n",
    "    params['learning_rate']=learning_rate\n",
    "    params['feature_fraction']=feature_fraction\n",
    "    params['lambda_l1']=lambda_l1\n",
    "    params['lambda_l2']=lambda_l2\n",
    "    params['max_depth']=max_depth\n",
    "    lgb_model= Lgb_Model(reduce_train, ajusted_test, features, categoricals=categoricals,ps=params)\n",
    "    print('kappa: ',lgb_model.score)\n",
    "    return lgb_model.score\n",
    "\n",
    "bounds_LGB = {\n",
    "    'subsample_freq': (1, 10),\n",
    "    'learning_rate': (0.01, 0.1),\n",
    "    'feature_fraction': (0.5, 1),\n",
    "    'lambda_l1': (0, 5),\n",
    "    'lambda_l2': (0, 5),\n",
    "    'max_depth': (13, 17),\n",
    "}\n",
    "\n",
    "LGB_BO = BayesianOptimization(LGB_Beyes, bounds_LGB, random_state=1029)\n",
    "import warnings\n",
    "init_points = 16\n",
    "n_iter = 16\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found `n_estimators` in params. Will use it instead of argument\n",
      "Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "Using categorical_feature in Dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.971828\tvalid_1's rmse: 1.00717\n",
      "[200]\ttraining's rmse: 0.915175\tvalid_1's rmse: 0.983087\n",
      "[300]\ttraining's rmse: 0.880864\tvalid_1's rmse: 0.977758\n",
      "[400]\ttraining's rmse: 0.853118\tvalid_1's rmse: 0.9762\n",
      "[500]\ttraining's rmse: 0.82926\tvalid_1's rmse: 0.976467\n",
      "[600]\ttraining's rmse: 0.807817\tvalid_1's rmse: 0.976333\n",
      "Early stopping, best iteration is:\n",
      "[540]\ttraining's rmse: 0.820541\tvalid_1's rmse: 0.975986\n",
      "Partial score of fold 0 is: 0.6010321638455711\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.968879\tvalid_1's rmse: 1.00929\n",
      "[200]\ttraining's rmse: 0.91133\tvalid_1's rmse: 0.990846\n",
      "[300]\ttraining's rmse: 0.876251\tvalid_1's rmse: 0.987888\n",
      "[400]\ttraining's rmse: 0.848871\tvalid_1's rmse: 0.987305\n",
      "[500]\ttraining's rmse: 0.824544\tvalid_1's rmse: 0.987734\n",
      "Early stopping, best iteration is:\n",
      "[429]\ttraining's rmse: 0.841457\tvalid_1's rmse: 0.987067\n",
      "Partial score of fold 1 is: 0.5886297227906995\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.969447\tvalid_1's rmse: 1.01105\n",
      "[200]\ttraining's rmse: 0.912551\tvalid_1's rmse: 0.990105\n",
      "[300]\ttraining's rmse: 0.878248\tvalid_1's rmse: 0.987033\n",
      "[400]\ttraining's rmse: 0.850797\tvalid_1's rmse: 0.98666\n",
      "[500]\ttraining's rmse: 0.827049\tvalid_1's rmse: 0.987051\n",
      "Early stopping, best iteration is:\n",
      "[412]\ttraining's rmse: 0.847791\tvalid_1's rmse: 0.98646\n",
      "Partial score of fold 2 is: 0.5905980016290215\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.968933\tvalid_1's rmse: 1.013\n",
      "[200]\ttraining's rmse: 0.912431\tvalid_1's rmse: 0.991156\n",
      "[300]\ttraining's rmse: 0.877724\tvalid_1's rmse: 0.987169\n",
      "[400]\ttraining's rmse: 0.849665\tvalid_1's rmse: 0.985978\n",
      "[500]\ttraining's rmse: 0.825683\tvalid_1's rmse: 0.986066\n",
      "Early stopping, best iteration is:\n",
      "[474]\ttraining's rmse: 0.831686\tvalid_1's rmse: 0.985623\n",
      "Partial score of fold 3 is: 0.5929241493470384\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.969888\tvalid_1's rmse: 1.01337\n",
      "[200]\ttraining's rmse: 0.91253\tvalid_1's rmse: 0.988188\n",
      "[300]\ttraining's rmse: 0.878058\tvalid_1's rmse: 0.982578\n",
      "[400]\ttraining's rmse: 0.850538\tvalid_1's rmse: 0.980598\n",
      "[500]\ttraining's rmse: 0.826606\tvalid_1's rmse: 0.979865\n",
      "[600]\ttraining's rmse: 0.80509\tvalid_1's rmse: 0.979749\n",
      "Early stopping, best iteration is:\n",
      "[549]\ttraining's rmse: 0.815758\tvalid_1's rmse: 0.979566\n",
      "Partial score of fold 4 is: 0.5942709971733987\n",
      "Our oof cohen kappa score is:  0.593850117628135\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lgb_model = Lgb_Model(reduce_train, ajusted_test, features, categoricals=categoricals, ps=LGB_BO.max['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Series.base is deprecated and will be removed in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:1.85623\tval-rmse:1.85705\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:1.0178\tval-rmse:1.15218\n",
      "[200]\ttrain-rmse:0.748606\tval-rmse:1.01058\n",
      "[300]\ttrain-rmse:0.640051\tval-rmse:0.98722\n",
      "[400]\ttrain-rmse:0.574536\tval-rmse:0.983104\n",
      "[500]\ttrain-rmse:0.535421\tval-rmse:0.982939\n",
      "Stopping. Best iteration:\n",
      "[454]\ttrain-rmse:0.550558\tval-rmse:0.982736\n",
      "\n",
      "Partial score of fold 0 is: 0.5902976032315507\n",
      "[0]\ttrain-rmse:1.85615\tval-rmse:1.85764\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:1.01309\tval-rmse:1.16798\n",
      "[200]\ttrain-rmse:0.743602\tval-rmse:1.02575\n",
      "[300]\ttrain-rmse:0.635774\tval-rmse:1.00201\n",
      "[400]\ttrain-rmse:0.577681\tval-rmse:0.997401\n",
      "[500]\ttrain-rmse:0.539739\tval-rmse:0.997492\n",
      "Stopping. Best iteration:\n",
      "[446]\ttrain-rmse:0.557437\tval-rmse:0.997015\n",
      "\n",
      "Partial score of fold 1 is: 0.575030705362293\n",
      "[0]\ttrain-rmse:1.85616\tval-rmse:1.85739\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:1.01521\tval-rmse:1.15934\n",
      "[200]\ttrain-rmse:0.748236\tval-rmse:1.02023\n",
      "[300]\ttrain-rmse:0.641359\tval-rmse:0.999563\n",
      "[400]\ttrain-rmse:0.577266\tval-rmse:0.996335\n",
      "[500]\ttrain-rmse:0.540978\tval-rmse:0.996035\n",
      "Stopping. Best iteration:\n",
      "[462]\ttrain-rmse:0.552266\tval-rmse:0.995614\n",
      "\n",
      "Partial score of fold 2 is: 0.5771779186404624\n",
      "[0]\ttrain-rmse:1.85623\tval-rmse:1.85738\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:1.0121\tval-rmse:1.15983\n",
      "[200]\ttrain-rmse:0.745398\tval-rmse:1.02086\n",
      "[300]\ttrain-rmse:0.640669\tval-rmse:1.00001\n",
      "[400]\ttrain-rmse:0.582545\tval-rmse:0.997271\n",
      "[500]\ttrain-rmse:0.542318\tval-rmse:0.997567\n",
      "Stopping. Best iteration:\n",
      "[437]\ttrain-rmse:0.563876\tval-rmse:0.9971\n",
      "\n",
      "Partial score of fold 3 is: 0.5820091485163437\n",
      "[0]\ttrain-rmse:1.85624\tval-rmse:1.85763\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:1.01767\tval-rmse:1.15972\n",
      "[200]\ttrain-rmse:0.748512\tval-rmse:1.01982\n",
      "[300]\ttrain-rmse:0.64109\tval-rmse:0.997415\n",
      "[400]\ttrain-rmse:0.581036\tval-rmse:0.993785\n",
      "[500]\ttrain-rmse:0.538311\tval-rmse:0.993028\n",
      "Stopping. Best iteration:\n",
      "[453]\ttrain-rmse:0.556167\tval-rmse:0.992871\n",
      "\n",
      "Partial score of fold 4 is: 0.5663452418497941\n",
      "Our oof cohen kappa score is:  0.5781023252203432\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEW9JREFUeJzt3X2MXFd5x/Hvgx0nEJfYEFgi262DsGgDKhCvjCkS2mAUJynClpooriowKMhSmkJaEfEmUauQqCDFpNCWIIMtGYSwUxMRN4SmbpJtxR8xxCG8BJNma9TExE0Av4AJLzV9+secDcuy67m7Ozuzs+f7kVa+99xz75xnjj2/nXvvjCMzkSTV51m9HoAkqTcMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlFvZ6AGdy/vnn58qVK6e9/09/+lPOPffczg2oR+ZLHWAtc9F8qQOsZdTBgwd/mJkvaNdvTgfAypUreeCBB6a9//DwMENDQ50bUI/MlzrAWuai+VIHWMuoiPjvJv08BSRJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZWa058ElqRe2nb1G3v22KuvvWHWH8N3AJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFWqUQBExF9FxMMR8e2I+HxEnBMRF0bEgYh4NCL2RMSi0vfssj5Stq8cc5z3lfZHImL97JQkSWqibQBExDLgncBgZr4cWABsAj4C3JKZq4DjwDVll2uA45n5EuCW0o+IuKjs9zLgMuATEbGgs+VIkppqegpoIfDsiFgIPAc4Crwe2Fu27wI2luUNZZ2yfV1ERGnfnZm/yMzvASPAmpmXIEmajrb/JWRmfj8ibgYeA34G/CtwEDiRmadLtyPAsrK8DHi87Hs6Ik4Czy/t94859Nh9nhERW4AtAAMDAwwPD0+9quLUqVMz2n+umC91gLXMRfOlDuh8LcvXb2zfaZZ0Y17aBkBELKX12/uFwAngn4DLJ+iao7tMsm2y9t9syNwObAcYHBzMoaGhdkOc1PDwMDPZf66YL3WAtcxF86UO6Hwt2269uWPHmqrV194w6/PS5BTQG4DvZeYPMvN/gduBPwKWlFNCAMuBJ8ryEWAFQNl+HnBsbPsE+0iSuqxJADwGrI2I55Rz+euA7wD3AVeWPpuBO8ryvrJO2X5vZmZp31TuEroQWAV8tTNlSJKmqsk1gAMRsRd4EDgNfJ3WKZovAbsj4sbStqPssgP4bESM0PrNf1M5zsMRcRut8DgNXJeZv+pwPZKkhtoGAEBmbgW2jms+zAR38WTmz4GrJjnOTcBNUxyjJGkW+ElgSaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklSpRgEQEUsiYm9EfDciDkXEayLieRGxPyIeLX8uLX0jIj4eESMR8c2IuHjMcTaX/o9GxObZKkqS1F7TdwAfA/4lM38feAVwCHgvcE9mrgLuKesAlwOrys8W4FaAiHgesBV4NbAG2DoaGpKk7msbABHxXOB1wA6AzPxlZp4ANgC7SrddwMayvAH4TLbcDyyJiAuA9cD+zDyWmceB/cBlHa1GktRYZOaZO0S8EtgOfIfWb/8HgeuB72fmkjH9jmfm0oi4E/hwZn6ltN8DvAcYAs7JzBtL+weAn2XmzeMebwutdw4MDAys3r1797SLO3XqFIsXL572/nPFfKkDrGUumi91QOdrefLwSMeONVXnvvBF067lkksuOZiZg+36LWxwrIXAxcA7MvNARHyMX5/umUhM0JZnaP/NhszttAKHwcHBHBoaajDEiQ0PDzOT/eeK+VIHWMtcNF/qgM7Xsu3Wm9t3miWrr71h1uelyTWAI8CRzDxQ1vfSCoQny6kdyp9Pjem/Ysz+y4EnztAuSeqBtgGQmf8DPB4RLy1N62idDtoHjN7Jsxm4oyzvA95S7gZaC5zMzKPA3cClEbG0XPy9tLRJknqgySkggHcAn4uIRcBh4G20wuO2iLgGeAy4qvS9C7gCGAGeLn3JzGMR8SHga6XfBzPzWEeqkCRNWaMAyMyHgIkuKKyboG8C101ynJ3AzqkMUJI0O/wksCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqtbDXA5hNTx4eYdutN3f9cd+1586uP6YkTVXjdwARsSAivh4Rd5b1CyPiQEQ8GhF7ImJRaT+7rI+U7SvHHON9pf2RiFjf6WIkSc1N5RTQ9cChMesfAW7JzFXAceCa0n4NcDwzXwLcUvoRERcBm4CXAZcBn4iIBTMbviRpuhoFQEQsB/4Y+HRZD+D1wN7SZRewsSxvKOuU7etK/w3A7sz8RWZ+DxgB1nSiCEnS1DV9B/B3wLuB/yvrzwdOZObpsn4EWFaWlwGPA5TtJ0v/Z9on2EeS1GVtLwJHxBuBpzLzYEQMjTZP0DXbbDvTPmMfbwuwBWBgYIDh4eF2Q5zUovOWsHz9xvYdO2wmY57IqVOnOn7MXrGWuWe+1AGdr6UXrx+jujEvTe4Cei3wpoi4AjgHeC6tdwRLImJh+S1/OfBE6X8EWAEciYiFwHnAsTHto8bu84zM3A5sBxgcHMyhoaFplNWyZ+enOXL3F6e9/3Rd3eG7gIaHh5nJ8zCXWMvcM1/qgM7X0ou7CEetvvaGWZ+XtqeAMvN9mbk8M1fSuoh7b2b+GXAfcGXpthm4oyzvK+uU7fdmZpb2TeUuoQuBVcBXO1aJJGlKZvI5gPcAuyPiRuDrwI7SvgP4bESM0PrNfxNAZj4cEbcB3wFOA9dl5q9m8PiSpBmYUgBk5jAwXJYPM8FdPJn5c+CqSfa/CbhpqoOUJHWeXwUhSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqtTCXg9A88O2q9/YqN/y9RvZduvNHXvcd+25s2PHkmrjOwBJqpQBIEmVMgAkqVIGgCRVyovAUp9pesG9qalcmPei+/zS9h1ARKyIiPsi4lBEPBwR15f250XE/oh4tPy5tLRHRHw8IkYi4psRcfGYY20u/R+NiM2zV5YkqZ0mp4BOA+/KzD8A1gLXRcRFwHuBezJzFXBPWQe4HFhVfrYAt0IrMICtwKuBNcDW0dCQJHVf2wDIzKOZ+WBZ/glwCFgGbAB2lW67gI1leQPwmWy5H1gSERcA64H9mXksM48D+4HLOlqNJKmxKV0EjoiVwKuAA8BAZh6FVkgALyzdlgGPj9ntSGmbrF2S1AORmc06RiwG/h24KTNvj4gTmblkzPbjmbk0Ir4E/G1mfqW03wO8G3g9cHZm3ljaPwA8nZnbxj3OFlqnjhgYGFi9e/fuaRd3/Ec/5JcnT0x7/+kaePFLOnq8U6dOsXjx4o4es9OePDzSqN+i85Z0dE46/VxPRa/mpelz3dRU5qSXz3cTnZ6TTj/XU3HuC1807VouueSSg5k52K5fo7uAIuIs4AvA5zLz9tL8ZERckJlHyymep0r7EWDFmN2XA0+U9qFx7cPjHysztwPbAQYHB3NoaGh8l8b27Pw0R+7+4rT3n66rO3ynxPDwMDN5Hrqh6V0ky9dv7OicdPq5nopezUsnv0oDpjYnvXy+m+j0nHT6uZ6K1dfeMOt/v5rcBRTADuBQZn50zKZ9wOidPJuBO8a0v6XcDbQWOFlOEd0NXBoRS8vF30tLmySpB5q8A3gt8GbgWxHxUGl7P/Bh4LaIuAZ4DLiqbLsLuAIYAZ4G3gaQmcci4kPA10q/D2bmsY5UIUmasrYBUM7lxySb103QP4HrJjnWTmDnVAYoSZodfhWEJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASapU1wMgIi6LiEciYiQi3tvtx5cktXQ1ACJiAfCPwOXARcCfRsRF3RyDJKml2+8A1gAjmXk4M38J7AY2dHkMkiS6HwDLgMfHrB8pbZKkLovM7N6DRVwFrM/Mt5f1NwNrMvMdY/psAbaU1ZcCj8zgIc8HfjiD/eeK+VIHWMtcNF/qAGsZ9XuZ+YJ2nRZO8+DTdQRYMWZ9OfDE2A6ZuR3Y3okHi4gHMnOwE8fqpflSB1jLXDRf6gBrmapunwL6GrAqIi6MiEXAJmBfl8cgSaLL7wAy83RE/AVwN7AA2JmZD3dzDJKklm6fAiIz7wLu6tLDdeRU0hwwX+oAa5mL5ksdYC1T0tWLwJKkucOvgpCkSvV9ALT7aomIODsi9pTtByJiZfdH2UyDWt4aET+IiIfKz9t7Mc52ImJnRDwVEd+eZHtExMdLnd+MiIu7PcamGtQyFBEnx8zJX3d7jE1ExIqIuC8iDkXEwxFx/QR9+mJeGtbSL/NyTkR8NSK+UWr5mwn6zN5rWGb27Q+tC8n/BbwYWAR8A7hoXJ8/Bz5ZljcBe3o97hnU8lbgH3o91ga1vA64GPj2JNuvAL4MBLAWONDrMc+gliHgzl6Ps0EdFwAXl+XfAf5zgr9ffTEvDWvpl3kJYHFZPgs4AKwd12fWXsP6/R1Ak6+W2ADsKst7gXUREV0cY1Pz5msyMvM/gGNn6LIB+Ey23A8siYgLujO6qWlQS1/IzKOZ+WBZ/glwiN/+FH5fzEvDWvpCea5PldWzys/4C7Oz9hrW7wHQ5KslnumTmaeBk8DzuzK6qWn6NRl/Ut6e742IFRNs7wfz7StBXlPewn85Il7W68G0U04hvIrWb5tj9d28nKEW6JN5iYgFEfEQ8BSwPzMnnZdOv4b1ewBMlILj07NJn7mgyTj/GViZmX8I/Bu//q2g3/TLnDTxIK2P3b8C+Hvgiz0ezxlFxGLgC8BfZuaPx2+eYJc5Oy9taumbecnMX2XmK2l9M8KaiHj5uC6zNi/9HgBtv1pibJ+IWAicx9x8S9/kazJ+lJm/KKufAlZ3aWyd1mTe+kJm/nj0LXy2PuNyVkSc3+NhTSgizqL1gvm5zLx9gi59My/taumneRmVmSeAYeCycZtm7TWs3wOgyVdL7AM2l+UrgXuzXE2ZY9rWMu587JtonfvsR/uAt5S7TtYCJzPzaK8HNR0R8aLR87ERsYbWv6kf9XZUv62McQdwKDM/Okm3vpiXJrX00by8ICKWlOVnA28Avjuu26y9hnX9k8CdlJN8tUREfBB4IDP30fqL8tmIGKGVmpt6N+LJNazlnRHxJuA0rVre2rMBn0FEfJ7WXRjnR8QRYCuti1tk5idpfRL8CmAEeBp4W29G2l6DWq4Ero2I08DPgE1z9BeM1wJvBr5VzjcDvB/4Xei7eWlSS7/MywXArmj9Z1nPAm7LzDu79RrmJ4ElqVL9fgpIkjRNBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZX6f4qldO5P+Q0ZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb_model = Xgb_Model(reduce_train, ajusted_test, features, categoricals=categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.2472272\ttest: 1.2475221\tbest: 1.2475221 (0)\ttotal: 361ms\tremaining: 30m 4s\n",
      "100:\tlearn: 0.9949520\ttest: 1.0193874\tbest: 1.0193874 (100)\ttotal: 16.4s\tremaining: 13m 17s\n",
      "200:\tlearn: 0.9516122\ttest: 0.9988753\tbest: 0.9988753 (200)\ttotal: 32.5s\tremaining: 12m 56s\n",
      "300:\tlearn: 0.9208086\ttest: 0.9905148\tbest: 0.9905148 (300)\ttotal: 49.3s\tremaining: 12m 49s\n",
      "400:\tlearn: 0.8871462\ttest: 0.9848748\tbest: 0.9848748 (400)\ttotal: 1m 5s\tremaining: 12m 33s\n",
      "500:\tlearn: 0.8605635\ttest: 0.9825447\tbest: 0.9825447 (500)\ttotal: 1m 21s\tremaining: 12m 15s\n",
      "600:\tlearn: 0.8374680\ttest: 0.9809891\tbest: 0.9809891 (600)\ttotal: 1m 38s\tremaining: 11m 58s\n",
      "700:\tlearn: 0.8169417\ttest: 0.9800077\tbest: 0.9799860 (699)\ttotal: 1m 55s\tremaining: 11m 46s\n",
      "800:\tlearn: 0.7981587\ttest: 0.9798398\tbest: 0.9797768 (766)\ttotal: 2m 11s\tremaining: 11m 30s\n",
      "900:\tlearn: 0.7803339\ttest: 0.9795674\tbest: 0.9794455 (862)\ttotal: 2m 28s\tremaining: 11m 13s\n",
      "1000:\tlearn: 0.7643023\ttest: 0.9793895\tbest: 0.9793895 (1000)\ttotal: 2m 44s\tremaining: 10m 58s\n",
      "1100:\tlearn: 0.7498930\ttest: 0.9794386\tbest: 0.9793227 (1040)\ttotal: 3m 1s\tremaining: 10m 42s\n",
      "1200:\tlearn: 0.7354883\ttest: 0.9797107\tbest: 0.9793227 (1040)\ttotal: 3m 17s\tremaining: 10m 25s\n",
      "1300:\tlearn: 0.7222090\ttest: 0.9798768\tbest: 0.9793227 (1040)\ttotal: 3m 33s\tremaining: 10m 7s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.9793226877\n",
      "bestIteration = 1040\n",
      "\n",
      "Shrink model to first 1041 iterations.\n",
      "Partial score of fold 0 is: 0.5973202358030704\n",
      "0:\tlearn: 1.2478955\ttest: 1.2476537\tbest: 1.2476537 (0)\ttotal: 227ms\tremaining: 18m 54s\n",
      "100:\tlearn: 0.9939331\ttest: 1.0167142\tbest: 1.0167142 (100)\ttotal: 16.6s\tremaining: 13m 27s\n",
      "200:\tlearn: 0.9466335\ttest: 1.0014380\tbest: 1.0014380 (200)\ttotal: 32.8s\tremaining: 13m 3s\n",
      "300:\tlearn: 0.9151877\ttest: 0.9963593\tbest: 0.9963593 (300)\ttotal: 49s\tremaining: 12m 44s\n",
      "400:\tlearn: 0.8853955\ttest: 0.9943364\tbest: 0.9943269 (398)\ttotal: 1m 5s\tremaining: 12m 34s\n",
      "500:\tlearn: 0.8583363\ttest: 0.9929516\tbest: 0.9928961 (497)\ttotal: 1m 22s\tremaining: 12m 21s\n",
      "600:\tlearn: 0.8378431\ttest: 0.9924360\tbest: 0.9923886 (592)\ttotal: 1m 39s\tremaining: 12m 5s\n",
      "700:\tlearn: 0.8172916\ttest: 0.9922508\tbest: 0.9922014 (647)\ttotal: 1m 55s\tremaining: 11m 49s\n",
      "800:\tlearn: 0.7987380\ttest: 0.9924283\tbest: 0.9921695 (730)\ttotal: 2m 12s\tremaining: 11m 35s\n",
      "900:\tlearn: 0.7809272\ttest: 0.9924112\tbest: 0.9921695 (730)\ttotal: 2m 28s\tremaining: 11m 17s\n",
      "1000:\tlearn: 0.7650485\ttest: 0.9929993\tbest: 0.9921695 (730)\ttotal: 2m 45s\tremaining: 10m 59s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.9921695159\n",
      "bestIteration = 730\n",
      "\n",
      "Shrink model to first 731 iterations.\n",
      "Partial score of fold 1 is: 0.5864825095125301\n",
      "0:\tlearn: 1.2474534\ttest: 1.2479049\tbest: 1.2479049 (0)\ttotal: 175ms\tremaining: 14m 35s\n",
      "100:\tlearn: 0.9925095\ttest: 1.0238043\tbest: 1.0238043 (100)\ttotal: 17.2s\tremaining: 13m 53s\n",
      "200:\tlearn: 0.9467827\ttest: 1.0038993\tbest: 1.0038993 (200)\ttotal: 33.5s\tremaining: 13m 18s\n",
      "300:\tlearn: 0.9167288\ttest: 0.9979108\tbest: 0.9979108 (300)\ttotal: 49.7s\tremaining: 12m 56s\n",
      "400:\tlearn: 0.8861818\ttest: 0.9935905\tbest: 0.9935631 (399)\ttotal: 1m 5s\tremaining: 12m 36s\n",
      "500:\tlearn: 0.8608864\ttest: 0.9914045\tbest: 0.9914045 (500)\ttotal: 1m 22s\tremaining: 12m 23s\n",
      "600:\tlearn: 0.8414330\ttest: 0.9904955\tbest: 0.9904844 (597)\ttotal: 1m 39s\tremaining: 12m 5s\n",
      "700:\tlearn: 0.8230386\ttest: 0.9904828\tbest: 0.9903552 (650)\ttotal: 1m 55s\tremaining: 11m 48s\n",
      "800:\tlearn: 0.8059494\ttest: 0.9902409\tbest: 0.9901681 (797)\ttotal: 2m 12s\tremaining: 11m 32s\n",
      "900:\tlearn: 0.7868703\ttest: 0.9900063\tbest: 0.9899340 (892)\ttotal: 2m 30s\tremaining: 11m 25s\n",
      "1000:\tlearn: 0.7703347\ttest: 0.9895937\tbest: 0.9895474 (991)\ttotal: 2m 46s\tremaining: 11m 6s\n",
      "1100:\tlearn: 0.7573451\ttest: 0.9895978\tbest: 0.9894254 (1043)\ttotal: 3m 2s\tremaining: 10m 47s\n",
      "1200:\tlearn: 0.7419640\ttest: 0.9893773\tbest: 0.9892335 (1152)\ttotal: 3m 19s\tremaining: 10m 31s\n",
      "1300:\tlearn: 0.7289211\ttest: 0.9894865\tbest: 0.9892335 (1152)\ttotal: 3m 35s\tremaining: 10m 13s\n",
      "1400:\tlearn: 0.7155992\ttest: 0.9897789\tbest: 0.9892335 (1152)\ttotal: 3m 51s\tremaining: 9m 55s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.9892335406\n",
      "bestIteration = 1152\n",
      "\n",
      "Shrink model to first 1153 iterations.\n",
      "Partial score of fold 2 is: 0.590419067189174\n",
      "0:\tlearn: 1.2468212\ttest: 1.2471249\tbest: 1.2471249 (0)\ttotal: 168ms\tremaining: 13m 59s\n",
      "100:\tlearn: 0.9911209\ttest: 1.0208654\tbest: 1.0208654 (100)\ttotal: 16.9s\tremaining: 13m 38s\n",
      "200:\tlearn: 0.9449828\ttest: 1.0036637\tbest: 1.0036637 (200)\ttotal: 33s\tremaining: 13m 8s\n",
      "300:\tlearn: 0.9137381\ttest: 0.9976827\tbest: 0.9976827 (300)\ttotal: 49s\tremaining: 12m 44s\n",
      "400:\tlearn: 0.8838481\ttest: 0.9938887\tbest: 0.9938887 (400)\ttotal: 1m 5s\tremaining: 12m 30s\n",
      "500:\tlearn: 0.8601933\ttest: 0.9919115\tbest: 0.9918101 (498)\ttotal: 1m 21s\tremaining: 12m 16s\n",
      "600:\tlearn: 0.8389420\ttest: 0.9908528\tbest: 0.9908178 (597)\ttotal: 1m 38s\tremaining: 11m 57s\n",
      "700:\tlearn: 0.8202490\ttest: 0.9905391\tbest: 0.9905391 (700)\ttotal: 1m 54s\tremaining: 11m 40s\n",
      "800:\tlearn: 0.8036969\ttest: 0.9904774\tbest: 0.9903364 (742)\ttotal: 2m 10s\tremaining: 11m 26s\n",
      "900:\tlearn: 0.7862567\ttest: 0.9901855\tbest: 0.9900724 (884)\ttotal: 2m 27s\tremaining: 11m 9s\n",
      "1000:\tlearn: 0.7680225\ttest: 0.9901662\tbest: 0.9900333 (979)\ttotal: 2m 43s\tremaining: 10m 52s\n",
      "1100:\tlearn: 0.7505651\ttest: 0.9900602\tbest: 0.9900007 (1097)\ttotal: 2m 59s\tremaining: 10m 36s\n",
      "1200:\tlearn: 0.7371525\ttest: 0.9904793\tbest: 0.9900007 (1097)\ttotal: 3m 16s\tremaining: 10m 21s\n",
      "1300:\tlearn: 0.7220578\ttest: 0.9905804\tbest: 0.9900007 (1097)\ttotal: 3m 32s\tremaining: 10m 4s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.990000681\n",
      "bestIteration = 1097\n",
      "\n",
      "Shrink model to first 1098 iterations.\n",
      "Partial score of fold 3 is: 0.5871982472719199\n",
      "0:\tlearn: 1.2471519\ttest: 1.2471899\tbest: 1.2471899 (0)\ttotal: 168ms\tremaining: 14m 1s\n",
      "100:\tlearn: 0.9927415\ttest: 1.0239062\tbest: 1.0239062 (100)\ttotal: 16.4s\tremaining: 13m 15s\n",
      "200:\tlearn: 0.9481192\ttest: 1.0032461\tbest: 1.0032461 (200)\ttotal: 33.1s\tremaining: 13m 10s\n",
      "300:\tlearn: 0.9171445\ttest: 0.9951680\tbest: 0.9951680 (300)\ttotal: 49.3s\tremaining: 12m 50s\n",
      "400:\tlearn: 0.8869722\ttest: 0.9906759\tbest: 0.9906759 (400)\ttotal: 1m 5s\tremaining: 12m 33s\n",
      "500:\tlearn: 0.8625625\ttest: 0.9889427\tbest: 0.9889427 (500)\ttotal: 1m 22s\tremaining: 12m 19s\n",
      "600:\tlearn: 0.8401602\ttest: 0.9875350\tbest: 0.9875003 (596)\ttotal: 1m 38s\tremaining: 12m\n",
      "700:\tlearn: 0.8220685\ttest: 0.9867716\tbest: 0.9867716 (700)\ttotal: 1m 54s\tremaining: 11m 41s\n",
      "800:\tlearn: 0.8034378\ttest: 0.9858672\tbest: 0.9858664 (799)\ttotal: 2m 10s\tremaining: 11m 26s\n",
      "900:\tlearn: 0.7857146\ttest: 0.9856704\tbest: 0.9855732 (858)\ttotal: 2m 28s\tremaining: 11m 14s\n",
      "1000:\tlearn: 0.7700176\ttest: 0.9856418\tbest: 0.9855732 (858)\ttotal: 2m 44s\tremaining: 10m 58s\n",
      "1100:\tlearn: 0.7544152\ttest: 0.9855222\tbest: 0.9854866 (1095)\ttotal: 3m 1s\tremaining: 10m 41s\n",
      "1200:\tlearn: 0.7403689\ttest: 0.9857475\tbest: 0.9854866 (1095)\ttotal: 3m 17s\tremaining: 10m 24s\n",
      "1300:\tlearn: 0.7262397\ttest: 0.9859505\tbest: 0.9854866 (1095)\ttotal: 3m 34s\tremaining: 10m 9s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.9854866165\n",
      "bestIteration = 1095\n",
      "\n",
      "Shrink model to first 1096 iterations.\n",
      "Partial score of fold 4 is: 0.5846043895613817\n",
      "Our oof cohen kappa score is:  0.5899489599634775\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cat_model = Catb_Model(reduce_train, ajusted_test, features, categoricals=categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "The ``active_features_`` attribute was deprecated in version 0.20 and will be removed 0.22.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17690, 392)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 200)               78600     \n",
      "_________________________________________________________________\n",
      "layer_normalization (LayerNo (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "layer_normalization_1 (Layer (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_2 (Layer (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "layer_normalization_3 (Layer (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 105,801\n",
      "Trainable params: 105,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14151 samples, validate on 3539 samples\n",
      "Epoch 1/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 2.0369\n",
      "Epoch 00001: val_loss improved from inf to 1.11455, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 4s 317us/sample - loss: 2.0372 - val_loss: 1.1146\n",
      "Epoch 2/100\n",
      "13984/14151 [============================>.] - ETA: 0s - loss: 1.4673\n",
      "Epoch 00002: val_loss improved from 1.11455 to 1.07387, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 222us/sample - loss: 1.4649 - val_loss: 1.0739\n",
      "Epoch 3/100\n",
      "13952/14151 [============================>.] - ETA: 0s - loss: 1.3277\n",
      "Epoch 00003: val_loss improved from 1.07387 to 1.04926, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 193us/sample - loss: 1.3278 - val_loss: 1.0493\n",
      "Epoch 4/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 1.2724\n",
      "Epoch 00004: val_loss did not improve from 1.04926\n",
      "14151/14151 [==============================] - 3s 197us/sample - loss: 1.2713 - val_loss: 1.0572\n",
      "Epoch 5/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 1.2097\n",
      "Epoch 00005: val_loss did not improve from 1.04926\n",
      "14151/14151 [==============================] - 3s 191us/sample - loss: 1.2099 - val_loss: 1.0655\n",
      "Epoch 6/100\n",
      "13856/14151 [============================>.] - ETA: 0s - loss: 1.1922\n",
      "Epoch 00006: val_loss improved from 1.04926 to 1.03357, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 193us/sample - loss: 1.1909 - val_loss: 1.0336\n",
      "Epoch 7/100\n",
      "13856/14151 [============================>.] - ETA: 0s - loss: 1.1649\n",
      "Epoch 00007: val_loss improved from 1.03357 to 1.01823, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 190us/sample - loss: 1.1673 - val_loss: 1.0182\n",
      "Epoch 8/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 1.1355\n",
      "Epoch 00008: val_loss did not improve from 1.01823\n",
      "14151/14151 [==============================] - 3s 186us/sample - loss: 1.1357 - val_loss: 1.0260\n",
      "Epoch 9/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 1.1248\n",
      "Epoch 00009: val_loss improved from 1.01823 to 1.00833, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 188us/sample - loss: 1.1252 - val_loss: 1.0083\n",
      "Epoch 10/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 1.1057\n",
      "Epoch 00010: val_loss improved from 1.00833 to 1.00290, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 187us/sample - loss: 1.1069 - val_loss: 1.0029\n",
      "Epoch 11/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 1.0933\n",
      "Epoch 00011: val_loss did not improve from 1.00290\n",
      "14151/14151 [==============================] - 3s 184us/sample - loss: 1.0931 - val_loss: 1.0106\n",
      "Epoch 12/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.0880\n",
      "Epoch 00012: val_loss improved from 1.00290 to 1.00215, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 186us/sample - loss: 1.0867 - val_loss: 1.0021\n",
      "Epoch 13/100\n",
      "13888/14151 [============================>.] - ETA: 0s - loss: 1.0714\n",
      "Epoch 00013: val_loss did not improve from 1.00215\n",
      "14151/14151 [==============================] - 3s 191us/sample - loss: 1.0725 - val_loss: 1.0125\n",
      "Epoch 14/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 1.0641\n",
      "Epoch 00014: val_loss improved from 1.00215 to 0.99234, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 187us/sample - loss: 1.0654 - val_loss: 0.9923\n",
      "Epoch 15/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 1.0634\n",
      "Epoch 00015: val_loss did not improve from 0.99234\n",
      "14151/14151 [==============================] - 3s 185us/sample - loss: 1.0632 - val_loss: 0.9946\n",
      "Epoch 16/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 1.0461\n",
      "Epoch 00016: val_loss did not improve from 0.99234\n",
      "14151/14151 [==============================] - 3s 185us/sample - loss: 1.0463 - val_loss: 0.9950\n",
      "Epoch 17/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 1.0322\n",
      "Epoch 00017: val_loss improved from 0.99234 to 0.98036, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 192us/sample - loss: 1.0325 - val_loss: 0.9804\n",
      "Epoch 18/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 1.0173\n",
      "Epoch 00018: val_loss did not improve from 0.98036\n",
      "14151/14151 [==============================] - 3s 187us/sample - loss: 1.0169 - val_loss: 0.9943\n",
      "Epoch 19/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 1.0203\n",
      "Epoch 00019: val_loss did not improve from 0.98036\n",
      "14151/14151 [==============================] - 3s 186us/sample - loss: 1.0187 - val_loss: 1.0109\n",
      "Epoch 20/100\n",
      "13888/14151 [============================>.] - ETA: 0s - loss: 1.0083\n",
      "Epoch 00020: val_loss did not improve from 0.98036\n",
      "14151/14151 [==============================] - 3s 183us/sample - loss: 1.0076 - val_loss: 0.9820\n",
      "Epoch 21/100\n",
      "13920/14151 [============================>.] - ETA: 0s - loss: 0.9964\n",
      "Epoch 00021: val_loss did not improve from 0.98036\n",
      "14151/14151 [==============================] - 3s 187us/sample - loss: 0.9958 - val_loss: 1.0123\n",
      "Epoch 22/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 0.9956\n",
      "Epoch 00022: val_loss did not improve from 0.98036\n",
      "14151/14151 [==============================] - 3s 184us/sample - loss: 0.9954 - val_loss: 0.9816\n",
      "Epoch 23/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.9783\n",
      "Epoch 00023: val_loss did not improve from 0.98036\n",
      "14151/14151 [==============================] - 3s 184us/sample - loss: 0.9783 - val_loss: 0.9840\n",
      "Epoch 24/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.9761\n",
      "Epoch 00024: val_loss improved from 0.98036 to 0.97742, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 187us/sample - loss: 0.9756 - val_loss: 0.9774\n",
      "Epoch 25/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.9618\n",
      "Epoch 00025: val_loss did not improve from 0.97742\n",
      "14151/14151 [==============================] - 3s 198us/sample - loss: 0.9632 - val_loss: 0.9791\n",
      "Epoch 26/100\n",
      "13952/14151 [============================>.] - ETA: 0s - loss: 0.9551\n",
      "Epoch 00026: val_loss did not improve from 0.97742\n",
      "14151/14151 [==============================] - 3s 195us/sample - loss: 0.9544 - val_loss: 0.9951\n",
      "Epoch 27/100\n",
      "13856/14151 [============================>.] - ETA: 0s - loss: 0.9507\n",
      "Epoch 00027: val_loss did not improve from 0.97742\n",
      "14151/14151 [==============================] - 3s 188us/sample - loss: 0.9515 - val_loss: 0.9849\n",
      "Epoch 28/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.9363\n",
      "Epoch 00028: val_loss did not improve from 0.97742\n",
      "14151/14151 [==============================] - 3s 194us/sample - loss: 0.9340 - val_loss: 0.9858\n",
      "Epoch 29/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.9278\n",
      "Epoch 00029: val_loss improved from 0.97742 to 0.97191, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 189us/sample - loss: 0.9280 - val_loss: 0.9719\n",
      "Epoch 30/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.9299\n",
      "Epoch 00030: val_loss did not improve from 0.97191\n",
      "14151/14151 [==============================] - 3s 186us/sample - loss: 0.9304 - val_loss: 0.9813\n",
      "Epoch 31/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.9188\n",
      "Epoch 00031: val_loss did not improve from 0.97191\n",
      "14151/14151 [==============================] - 3s 186us/sample - loss: 0.9194 - val_loss: 0.9782\n",
      "Epoch 32/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 0.9136\n",
      "Epoch 00032: val_loss improved from 0.97191 to 0.97109, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 191us/sample - loss: 0.9136 - val_loss: 0.9711\n",
      "Epoch 33/100\n",
      "13984/14151 [============================>.] - ETA: 0s - loss: 0.9061\n",
      "Epoch 00033: val_loss did not improve from 0.97109\n",
      "14151/14151 [==============================] - 3s 189us/sample - loss: 0.9045 - val_loss: 0.9741\n",
      "Epoch 34/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.8992\n",
      "Epoch 00034: val_loss did not improve from 0.97109\n",
      "14151/14151 [==============================] - 3s 186us/sample - loss: 0.8990 - val_loss: 0.9731\n",
      "Epoch 35/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.8945\n",
      "Epoch 00035: val_loss did not improve from 0.97109\n",
      "14151/14151 [==============================] - 3s 187us/sample - loss: 0.8946 - val_loss: 0.9817\n",
      "Epoch 36/100\n",
      "13952/14151 [============================>.] - ETA: 0s - loss: 0.8901\n",
      "Epoch 00036: val_loss improved from 0.97109 to 0.96985, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 3s 190us/sample - loss: 0.8902 - val_loss: 0.9699\n",
      "Epoch 37/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.8786\n",
      "Epoch 00037: val_loss did not improve from 0.96985\n",
      "14151/14151 [==============================] - 3s 186us/sample - loss: 0.8782 - val_loss: 0.9976\n",
      "Epoch 38/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.8733\n",
      "Epoch 00038: val_loss did not improve from 0.96985\n",
      "14151/14151 [==============================] - 3s 187us/sample - loss: 0.8741 - val_loss: 0.9803\n",
      "Epoch 39/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 0.8683\n",
      "Epoch 00039: val_loss did not improve from 0.96985\n",
      "14151/14151 [==============================] - 3s 190us/sample - loss: 0.8684 - val_loss: 0.9856\n",
      "Epoch 40/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.8720\n",
      "Epoch 00040: val_loss did not improve from 0.96985\n",
      "14151/14151 [==============================] - 3s 191us/sample - loss: 0.8718 - val_loss: 0.9839\n",
      "Epoch 41/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 0.8631\n",
      "Epoch 00041: val_loss did not improve from 0.96985\n",
      "14151/14151 [==============================] - 3s 185us/sample - loss: 0.8642 - val_loss: 0.9811\n",
      "Epoch 42/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.8494\n",
      "Epoch 00042: val_loss did not improve from 0.96985\n",
      "14151/14151 [==============================] - 3s 188us/sample - loss: 0.8489 - val_loss: 0.9840\n",
      "Epoch 43/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 0.8486\n",
      "Epoch 00043: val_loss did not improve from 0.96985\n",
      "14151/14151 [==============================] - 3s 191us/sample - loss: 0.8486 - val_loss: 1.0092\n",
      "Epoch 44/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.8441\n",
      "Epoch 00044: val_loss did not improve from 0.96985\n",
      "14151/14151 [==============================] - 3s 186us/sample - loss: 0.8456 - val_loss: 0.9933\n",
      "Epoch 45/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 0.8313\n",
      "Epoch 00045: val_loss did not improve from 0.96985\n",
      "14151/14151 [==============================] - 3s 185us/sample - loss: 0.8314 - val_loss: 0.9923\n",
      "Epoch 46/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.8406\n",
      "Epoch 00046: val_loss did not improve from 0.96985\n",
      "14151/14151 [==============================] - 3s 185us/sample - loss: 0.8389 - val_loss: 0.9824\n",
      "Epoch 47/100\n",
      "13920/14151 [============================>.] - ETA: 0s - loss: 0.8254\n",
      "Epoch 00047: val_loss did not improve from 0.96985\n",
      "14151/14151 [==============================] - 3s 193us/sample - loss: 0.8247 - val_loss: 1.0028\n",
      "Epoch 48/100\n",
      "13888/14151 [============================>.] - ETA: 0s - loss: 0.8187\n",
      "Epoch 00048: val_loss did not improve from 0.96985\n",
      "14151/14151 [==============================] - 3s 193us/sample - loss: 0.8217 - val_loss: 0.9940\n",
      "Epoch 49/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.8240\n",
      "Epoch 00049: val_loss did not improve from 0.96985\n",
      "14151/14151 [==============================] - 3s 200us/sample - loss: 0.8232 - val_loss: 0.9829\n",
      "Epoch 50/100\n",
      "13856/14151 [============================>.] - ETA: 0s - loss: 0.8096\n",
      "Epoch 00050: val_loss did not improve from 0.96985\n",
      "14151/14151 [==============================] - 3s 192us/sample - loss: 0.8096 - val_loss: 0.9957\n",
      "Epoch 51/100\n",
      "13920/14151 [============================>.] - ETA: 0s - loss: 0.7940\n",
      "Epoch 00051: val_loss did not improve from 0.96985\n",
      "14151/14151 [==============================] - 3s 193us/sample - loss: 0.7926 - val_loss: 1.0259\n",
      "Epoch 52/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.7953\n",
      "Epoch 00052: val_loss did not improve from 0.96985\n",
      "14151/14151 [==============================] - 3s 186us/sample - loss: 0.7971 - val_loss: 1.0047\n",
      "Epoch 53/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.7931\n",
      "Epoch 00053: val_loss did not improve from 0.96985\n",
      "14151/14151 [==============================] - 3s 185us/sample - loss: 0.7940 - val_loss: 1.0064\n",
      "Epoch 54/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.7909\n",
      "Epoch 00054: val_loss did not improve from 0.96985\n",
      "14151/14151 [==============================] - 3s 185us/sample - loss: 0.7924 - val_loss: 1.0313\n",
      "Epoch 55/100\n",
      "13920/14151 [============================>.] - ETA: 0s - loss: 0.7930\n",
      "Epoch 00055: val_loss did not improve from 0.96985\n",
      "14151/14151 [==============================] - 3s 187us/sample - loss: 0.7948 - val_loss: 1.0113\n",
      "Epoch 56/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 0.7870\n",
      "Epoch 00056: val_loss did not improve from 0.96985\n",
      "14151/14151 [==============================] - 3s 186us/sample - loss: 0.7870 - val_loss: 1.0067\n",
      "Partial score of fold 0 is: 0.595664883538561\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 200)               78600     \n",
      "_________________________________________________________________\n",
      "layer_normalization_4 (Layer (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "layer_normalization_5 (Layer (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_6 (Layer (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "layer_normalization_7 (Layer (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 105,801\n",
      "Trainable params: 105,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14152 samples, validate on 3538 samples\n",
      "Epoch 1/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.7792\n",
      "Epoch 00001: val_loss improved from inf to 1.10408, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 4s 273us/sample - loss: 1.7741 - val_loss: 1.1041\n",
      "Epoch 2/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.3795\n",
      "Epoch 00002: val_loss improved from 1.10408 to 1.06956, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 187us/sample - loss: 1.3790 - val_loss: 1.0696\n",
      "Epoch 3/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.2893\n",
      "Epoch 00003: val_loss improved from 1.06956 to 1.05496, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 1.2894 - val_loss: 1.0550\n",
      "Epoch 4/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.2220\n",
      "Epoch 00004: val_loss improved from 1.05496 to 1.03502, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 1.2220 - val_loss: 1.0350\n",
      "Epoch 5/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.1825\n",
      "Epoch 00005: val_loss did not improve from 1.03502\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 1.1835 - val_loss: 1.0384\n",
      "Epoch 6/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.1615\n",
      "Epoch 00006: val_loss improved from 1.03502 to 1.03463, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 1.1621 - val_loss: 1.0346\n",
      "Epoch 7/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.1506\n",
      "Epoch 00007: val_loss did not improve from 1.03463\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 1.1504 - val_loss: 1.0381\n",
      "Epoch 8/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.1210\n",
      "Epoch 00008: val_loss improved from 1.03463 to 1.02420, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 1.1207 - val_loss: 1.0242\n",
      "Epoch 9/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.1035\n",
      "Epoch 00009: val_loss did not improve from 1.02420\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 1.1038 - val_loss: 1.0335\n",
      "Epoch 10/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0967\n",
      "Epoch 00010: val_loss improved from 1.02420 to 1.02193, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 1.0968 - val_loss: 1.0219\n",
      "Epoch 11/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0909\n",
      "Epoch 00011: val_loss improved from 1.02193 to 1.01225, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 1.0907 - val_loss: 1.0123\n",
      "Epoch 12/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0662\n",
      "Epoch 00012: val_loss did not improve from 1.01225\n",
      "14152/14152 [==============================] - 3s 184us/sample - loss: 1.0664 - val_loss: 1.0146\n",
      "Epoch 13/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0609\n",
      "Epoch 00013: val_loss did not improve from 1.01225\n",
      "14152/14152 [==============================] - 3s 198us/sample - loss: 1.0615 - val_loss: 1.0414\n",
      "Epoch 14/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0531\n",
      "Epoch 00014: val_loss did not improve from 1.01225\n",
      "14152/14152 [==============================] - 3s 192us/sample - loss: 1.0513 - val_loss: 1.0126\n",
      "Epoch 15/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0259\n",
      "Epoch 00015: val_loss improved from 1.01225 to 1.01073, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 192us/sample - loss: 1.0246 - val_loss: 1.0107\n",
      "Epoch 16/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0326\n",
      "Epoch 00016: val_loss did not improve from 1.01073\n",
      "14152/14152 [==============================] - 3s 189us/sample - loss: 1.0324 - val_loss: 1.0152\n",
      "Epoch 17/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0171\n",
      "Epoch 00017: val_loss improved from 1.01073 to 0.99821, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 190us/sample - loss: 1.0169 - val_loss: 0.9982\n",
      "Epoch 18/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0036\n",
      "Epoch 00018: val_loss did not improve from 0.99821\n",
      "14152/14152 [==============================] - 3s 184us/sample - loss: 1.0041 - val_loss: 1.0006\n",
      "Epoch 19/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0036\n",
      "Epoch 00019: val_loss did not improve from 0.99821\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 1.0034 - val_loss: 1.0049\n",
      "Epoch 20/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9858\n",
      "Epoch 00020: val_loss did not improve from 0.99821\n",
      "14152/14152 [==============================] - 3s 189us/sample - loss: 0.9861 - val_loss: 0.9984\n",
      "Epoch 21/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9809\n",
      "Epoch 00021: val_loss did not improve from 0.99821\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 0.9801 - val_loss: 1.0067\n",
      "Epoch 22/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9541\n",
      "Epoch 00022: val_loss did not improve from 0.99821\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 0.9541 - val_loss: 1.0139\n",
      "Epoch 23/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9593\n",
      "Epoch 00023: val_loss did not improve from 0.99821\n",
      "14152/14152 [==============================] - 3s 187us/sample - loss: 0.9607 - val_loss: 1.0059\n",
      "Epoch 24/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9494\n",
      "Epoch 00024: val_loss did not improve from 0.99821\n",
      "14152/14152 [==============================] - 3s 191us/sample - loss: 0.9487 - val_loss: 1.0214\n",
      "Epoch 25/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9381\n",
      "Epoch 00025: val_loss did not improve from 0.99821\n",
      "14152/14152 [==============================] - 3s 190us/sample - loss: 0.9379 - val_loss: 1.0211\n",
      "Epoch 26/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9376\n",
      "Epoch 00026: val_loss did not improve from 0.99821\n",
      "14152/14152 [==============================] - 3s 187us/sample - loss: 0.9369 - val_loss: 1.0268\n",
      "Epoch 27/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9287\n",
      "Epoch 00027: val_loss did not improve from 0.99821\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.9311 - val_loss: 1.0251\n",
      "Epoch 28/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9212\n",
      "Epoch 00028: val_loss did not improve from 0.99821\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.9240 - val_loss: 1.0058\n",
      "Epoch 29/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9062\n",
      "Epoch 00029: val_loss did not improve from 0.99821\n",
      "14152/14152 [==============================] - 3s 182us/sample - loss: 0.9069 - val_loss: 1.0434\n",
      "Epoch 30/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9053\n",
      "Epoch 00030: val_loss did not improve from 0.99821\n",
      "14152/14152 [==============================] - 3s 184us/sample - loss: 0.9050 - val_loss: 1.0289\n",
      "Epoch 31/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.8853\n",
      "Epoch 00031: val_loss did not improve from 0.99821\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 0.8853 - val_loss: 1.0256\n",
      "Epoch 32/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9035\n",
      "Epoch 00032: val_loss did not improve from 0.99821\n",
      "14152/14152 [==============================] - 3s 187us/sample - loss: 0.9052 - val_loss: 1.0238\n",
      "Epoch 33/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.8797\n",
      "Epoch 00033: val_loss did not improve from 0.99821\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.8795 - val_loss: 1.0462\n",
      "Epoch 34/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.8789\n",
      "Epoch 00034: val_loss did not improve from 0.99821\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.8781 - val_loss: 1.0167\n",
      "Epoch 35/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.8768\n",
      "Epoch 00035: val_loss did not improve from 0.99821\n",
      "14152/14152 [==============================] - 3s 189us/sample - loss: 0.8767 - val_loss: 1.0473\n",
      "Epoch 36/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8708\n",
      "Epoch 00036: val_loss did not improve from 0.99821\n",
      "14152/14152 [==============================] - 3s 199us/sample - loss: 0.8719 - val_loss: 1.0354\n",
      "Epoch 37/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.8483\n",
      "Epoch 00037: val_loss did not improve from 0.99821\n",
      "14152/14152 [==============================] - 3s 197us/sample - loss: 0.8462 - val_loss: 1.0374\n",
      "Partial score of fold 1 is: 0.5814723451968014\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 200)               78600     \n",
      "_________________________________________________________________\n",
      "layer_normalization_8 (Layer (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "layer_normalization_9 (Layer (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_10 (Laye (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "layer_normalization_11 (Laye (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 105,801\n",
      "Trainable params: 105,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14152 samples, validate on 3538 samples\n",
      "Epoch 1/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.9791\n",
      "Epoch 00001: val_loss improved from inf to 1.14308, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 4s 278us/sample - loss: 1.9716 - val_loss: 1.1431\n",
      "Epoch 2/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.4223\n",
      "Epoch 00002: val_loss improved from 1.14308 to 1.08578, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 1.4216 - val_loss: 1.0858\n",
      "Epoch 3/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.2703\n",
      "Epoch 00003: val_loss improved from 1.08578 to 1.08411, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 201us/sample - loss: 1.2694 - val_loss: 1.0841\n",
      "Epoch 4/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.2118\n",
      "Epoch 00004: val_loss improved from 1.08411 to 1.06441, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 187us/sample - loss: 1.2128 - val_loss: 1.0644\n",
      "Epoch 5/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.1921\n",
      "Epoch 00005: val_loss did not improve from 1.06441\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 1.1933 - val_loss: 1.0719\n",
      "Epoch 6/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.1514\n",
      "Epoch 00006: val_loss did not improve from 1.06441\n",
      "14152/14152 [==============================] - 3s 184us/sample - loss: 1.1550 - val_loss: 1.0748\n",
      "Epoch 7/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1457\n",
      "Epoch 00007: val_loss improved from 1.06441 to 1.05381, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 1.1457 - val_loss: 1.0538\n",
      "Epoch 8/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.1299\n",
      "Epoch 00008: val_loss improved from 1.05381 to 1.04740, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 189us/sample - loss: 1.1268 - val_loss: 1.0474\n",
      "Epoch 9/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.1124\n",
      "Epoch 00009: val_loss did not improve from 1.04740\n",
      "14152/14152 [==============================] - 3s 189us/sample - loss: 1.1133 - val_loss: 1.0514\n",
      "Epoch 10/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1103\n",
      "Epoch 00010: val_loss improved from 1.04740 to 1.02718, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 189us/sample - loss: 1.1097 - val_loss: 1.0272\n",
      "Epoch 11/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0791\n",
      "Epoch 00011: val_loss did not improve from 1.02718\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 1.0790 - val_loss: 1.0471\n",
      "Epoch 12/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0733\n",
      "Epoch 00012: val_loss did not improve from 1.02718\n",
      "14152/14152 [==============================] - 3s 189us/sample - loss: 1.0708 - val_loss: 1.0383\n",
      "Epoch 13/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0474\n",
      "Epoch 00013: val_loss improved from 1.02718 to 1.02386, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 1.0487 - val_loss: 1.0239\n",
      "Epoch 14/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0580\n",
      "Epoch 00014: val_loss did not improve from 1.02386\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 1.0575 - val_loss: 1.0286\n",
      "Epoch 15/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0328\n",
      "Epoch 00015: val_loss did not improve from 1.02386\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 1.0342 - val_loss: 1.0312\n",
      "Epoch 16/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0353\n",
      "Epoch 00016: val_loss improved from 1.02386 to 1.01955, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 220us/sample - loss: 1.0345 - val_loss: 1.0196\n",
      "Epoch 17/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0150\n",
      "Epoch 00017: val_loss improved from 1.01955 to 1.00988, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 211us/sample - loss: 1.0150 - val_loss: 1.0099\n",
      "Epoch 18/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0118\n",
      "Epoch 00018: val_loss did not improve from 1.00988\n",
      "14152/14152 [==============================] - 3s 183us/sample - loss: 1.0112 - val_loss: 1.0114\n",
      "Epoch 19/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0002\n",
      "Epoch 00019: val_loss improved from 1.00988 to 1.00572, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 187us/sample - loss: 1.0010 - val_loss: 1.0057\n",
      "Epoch 20/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0070\n",
      "Epoch 00020: val_loss did not improve from 1.00572\n",
      "14152/14152 [==============================] - 3s 202us/sample - loss: 1.0064 - val_loss: 1.0180\n",
      "Epoch 21/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9812\n",
      "Epoch 00021: val_loss did not improve from 1.00572\n",
      "14152/14152 [==============================] - 3s 225us/sample - loss: 0.9813 - val_loss: 1.0293\n",
      "Epoch 22/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9873\n",
      "Epoch 00022: val_loss did not improve from 1.00572\n",
      "14152/14152 [==============================] - 3s 225us/sample - loss: 0.9876 - val_loss: 1.0184\n",
      "Epoch 23/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9744\n",
      "Epoch 00023: val_loss did not improve from 1.00572\n",
      "14152/14152 [==============================] - 3s 220us/sample - loss: 0.9745 - val_loss: 1.0061\n",
      "Epoch 24/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9635\n",
      "Epoch 00024: val_loss did not improve from 1.00572\n",
      "14152/14152 [==============================] - 3s 189us/sample - loss: 0.9632 - val_loss: 1.0087\n",
      "Epoch 25/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9547\n",
      "Epoch 00025: val_loss did not improve from 1.00572\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 0.9530 - val_loss: 1.0118\n",
      "Epoch 26/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9508\n",
      "Epoch 00026: val_loss did not improve from 1.00572\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.9518 - val_loss: 1.0171\n",
      "Epoch 27/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9352\n",
      "Epoch 00027: val_loss did not improve from 1.00572\n",
      "14152/14152 [==============================] - 3s 187us/sample - loss: 0.9371 - val_loss: 1.0146\n",
      "Epoch 28/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9292\n",
      "Epoch 00028: val_loss did not improve from 1.00572\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.9304 - val_loss: 1.0166\n",
      "Epoch 29/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9353\n",
      "Epoch 00029: val_loss did not improve from 1.00572\n",
      "14152/14152 [==============================] - 3s 183us/sample - loss: 0.9349 - val_loss: 1.0244\n",
      "Epoch 30/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9264\n",
      "Epoch 00030: val_loss did not improve from 1.00572\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.9260 - val_loss: 1.0210\n",
      "Epoch 31/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9197\n",
      "Epoch 00031: val_loss did not improve from 1.00572\n",
      "14152/14152 [==============================] - 3s 204us/sample - loss: 0.9189 - val_loss: 1.0244\n",
      "Epoch 32/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9042\n",
      "Epoch 00032: val_loss did not improve from 1.00572\n",
      "14152/14152 [==============================] - 3s 246us/sample - loss: 0.9034 - val_loss: 1.0395\n",
      "Epoch 33/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9035\n",
      "Epoch 00033: val_loss did not improve from 1.00572\n",
      "14152/14152 [==============================] - 3s 202us/sample - loss: 0.9062 - val_loss: 1.0258\n",
      "Epoch 34/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.8866\n",
      "Epoch 00034: val_loss did not improve from 1.00572\n",
      "14152/14152 [==============================] - 3s 205us/sample - loss: 0.8854 - val_loss: 1.0249\n",
      "Epoch 35/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.8789\n",
      "Epoch 00035: val_loss did not improve from 1.00572\n",
      "14152/14152 [==============================] - 3s 201us/sample - loss: 0.8778 - val_loss: 1.0173\n",
      "Epoch 36/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.8806\n",
      "Epoch 00036: val_loss did not improve from 1.00572\n",
      "14152/14152 [==============================] - 3s 199us/sample - loss: 0.8810 - val_loss: 1.0453\n",
      "Epoch 37/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.8834\n",
      "Epoch 00037: val_loss did not improve from 1.00572\n",
      "14152/14152 [==============================] - 3s 200us/sample - loss: 0.8830 - val_loss: 1.0499\n",
      "Epoch 38/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.8740\n",
      "Epoch 00038: val_loss did not improve from 1.00572\n",
      "14152/14152 [==============================] - 3s 200us/sample - loss: 0.8744 - val_loss: 1.0301\n",
      "Epoch 39/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.8625\n",
      "Epoch 00039: val_loss did not improve from 1.00572\n",
      "14152/14152 [==============================] - 3s 199us/sample - loss: 0.8626 - val_loss: 1.0230\n",
      "Partial score of fold 2 is: 0.5807566074374115\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 200)               78600     \n",
      "_________________________________________________________________\n",
      "layer_normalization_12 (Laye (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "layer_normalization_13 (Laye (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_14 (Laye (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "layer_normalization_15 (Laye (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 105,801\n",
      "Trainable params: 105,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14152 samples, validate on 3538 samples\n",
      "Epoch 1/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.9837\n",
      "Epoch 00001: val_loss improved from inf to 1.11342, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 4s 300us/sample - loss: 1.9779 - val_loss: 1.1134\n",
      "Epoch 2/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.3831\n",
      "Epoch 00002: val_loss improved from 1.11342 to 1.07889, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 212us/sample - loss: 1.3832 - val_loss: 1.0789\n",
      "Epoch 3/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.2946\n",
      "Epoch 00003: val_loss improved from 1.07889 to 1.05848, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 210us/sample - loss: 1.2968 - val_loss: 1.0585\n",
      "Epoch 4/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.2165\n",
      "Epoch 00004: val_loss improved from 1.05848 to 1.05185, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 215us/sample - loss: 1.2161 - val_loss: 1.0519\n",
      "Epoch 5/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1787\n",
      "Epoch 00005: val_loss improved from 1.05185 to 1.04915, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 201us/sample - loss: 1.1800 - val_loss: 1.0491\n",
      "Epoch 6/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.1633\n",
      "Epoch 00006: val_loss improved from 1.04915 to 1.03480, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 204us/sample - loss: 1.1624 - val_loss: 1.0348\n",
      "Epoch 7/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1448\n",
      "Epoch 00007: val_loss improved from 1.03480 to 1.03164, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 202us/sample - loss: 1.1437 - val_loss: 1.0316\n",
      "Epoch 8/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1167\n",
      "Epoch 00008: val_loss did not improve from 1.03164\n",
      "14152/14152 [==============================] - 3s 203us/sample - loss: 1.1154 - val_loss: 1.0385\n",
      "Epoch 9/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1067\n",
      "Epoch 00009: val_loss improved from 1.03164 to 1.03017, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 198us/sample - loss: 1.1046 - val_loss: 1.0302\n",
      "Epoch 10/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0932\n",
      "Epoch 00010: val_loss did not improve from 1.03017\n",
      "14152/14152 [==============================] - 3s 198us/sample - loss: 1.0938 - val_loss: 1.0365\n",
      "Epoch 11/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0799\n",
      "Epoch 00011: val_loss improved from 1.03017 to 1.02982, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 200us/sample - loss: 1.0779 - val_loss: 1.0298\n",
      "Epoch 12/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0647\n",
      "Epoch 00012: val_loss improved from 1.02982 to 1.02139, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 201us/sample - loss: 1.0667 - val_loss: 1.0214\n",
      "Epoch 13/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0621\n",
      "Epoch 00013: val_loss did not improve from 1.02139\n",
      "14152/14152 [==============================] - 3s 196us/sample - loss: 1.0610 - val_loss: 1.0261\n",
      "Epoch 14/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0388\n",
      "Epoch 00014: val_loss did not improve from 1.02139\n",
      "14152/14152 [==============================] - 3s 194us/sample - loss: 1.0378 - val_loss: 1.0316\n",
      "Epoch 15/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0421\n",
      "Epoch 00015: val_loss did not improve from 1.02139\n",
      "14152/14152 [==============================] - 3s 196us/sample - loss: 1.0429 - val_loss: 1.0329\n",
      "Epoch 16/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0173\n",
      "Epoch 00016: val_loss did not improve from 1.02139\n",
      "14152/14152 [==============================] - 3s 193us/sample - loss: 1.0205 - val_loss: 1.0238\n",
      "Epoch 17/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0221\n",
      "Epoch 00017: val_loss did not improve from 1.02139\n",
      "14152/14152 [==============================] - 3s 191us/sample - loss: 1.0222 - val_loss: 1.0353\n",
      "Epoch 18/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0123\n",
      "Epoch 00018: val_loss improved from 1.02139 to 1.00734, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 195us/sample - loss: 1.0113 - val_loss: 1.0073\n",
      "Epoch 19/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9999\n",
      "Epoch 00019: val_loss did not improve from 1.00734\n",
      "14152/14152 [==============================] - 3s 192us/sample - loss: 0.9987 - val_loss: 1.0249\n",
      "Epoch 20/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9918\n",
      "Epoch 00020: val_loss did not improve from 1.00734\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 0.9910 - val_loss: 1.0157\n",
      "Epoch 21/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9852\n",
      "Epoch 00021: val_loss improved from 1.00734 to 1.00616, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 191us/sample - loss: 0.9861 - val_loss: 1.0062\n",
      "Epoch 22/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9725\n",
      "Epoch 00022: val_loss did not improve from 1.00616\n",
      "14152/14152 [==============================] - 3s 199us/sample - loss: 0.9733 - val_loss: 1.0156\n",
      "Epoch 23/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9623\n",
      "Epoch 00023: val_loss improved from 1.00616 to 1.00453, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 195us/sample - loss: 0.9631 - val_loss: 1.0045\n",
      "Epoch 24/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9533\n",
      "Epoch 00024: val_loss did not improve from 1.00453\n",
      "14152/14152 [==============================] - 3s 192us/sample - loss: 0.9529 - val_loss: 1.0242\n",
      "Epoch 25/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9525\n",
      "Epoch 00025: val_loss improved from 1.00453 to 1.00130, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 194us/sample - loss: 0.9523 - val_loss: 1.0013\n",
      "Epoch 26/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9566\n",
      "Epoch 00026: val_loss did not improve from 1.00130\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 0.9571 - val_loss: 1.0356\n",
      "Epoch 27/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9273\n",
      "Epoch 00027: val_loss did not improve from 1.00130\n",
      "14152/14152 [==============================] - 3s 184us/sample - loss: 0.9275 - val_loss: 1.0046\n",
      "Epoch 28/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9286\n",
      "Epoch 00028: val_loss improved from 1.00130 to 0.99633, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.9293 - val_loss: 0.9963\n",
      "Epoch 29/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9197\n",
      "Epoch 00029: val_loss did not improve from 0.99633\n",
      "14152/14152 [==============================] - 3s 190us/sample - loss: 0.9198 - val_loss: 1.0157\n",
      "Epoch 30/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9214\n",
      "Epoch 00030: val_loss did not improve from 0.99633\n",
      "14152/14152 [==============================] - 3s 186us/sample - loss: 0.9219 - val_loss: 1.0128\n",
      "Epoch 31/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.8938\n",
      "Epoch 00031: val_loss did not improve from 0.99633\n",
      "14152/14152 [==============================] - 3s 184us/sample - loss: 0.8939 - val_loss: 1.0159\n",
      "Epoch 32/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9052\n",
      "Epoch 00032: val_loss did not improve from 0.99633\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.9059 - val_loss: 1.0307\n",
      "Epoch 33/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.8880\n",
      "Epoch 00033: val_loss did not improve from 0.99633\n",
      "14152/14152 [==============================] - 3s 189us/sample - loss: 0.8880 - val_loss: 1.0260\n",
      "Epoch 34/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.8921\n",
      "Epoch 00034: val_loss did not improve from 0.99633\n",
      "14152/14152 [==============================] - 3s 187us/sample - loss: 0.8924 - val_loss: 1.0200\n",
      "Epoch 35/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.8844\n",
      "Epoch 00035: val_loss did not improve from 0.99633\n",
      "14152/14152 [==============================] - 3s 183us/sample - loss: 0.8850 - val_loss: 1.0218\n",
      "Epoch 36/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.8754\n",
      "Epoch 00036: val_loss did not improve from 0.99633\n",
      "14152/14152 [==============================] - 3s 183us/sample - loss: 0.8760 - val_loss: 1.0556\n",
      "Epoch 37/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.8783\n",
      "Epoch 00037: val_loss did not improve from 0.99633\n",
      "14152/14152 [==============================] - 3s 188us/sample - loss: 0.8780 - val_loss: 1.0160\n",
      "Epoch 38/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8570\n",
      "Epoch 00038: val_loss did not improve from 0.99633\n",
      "14152/14152 [==============================] - 3s 185us/sample - loss: 0.8571 - val_loss: 1.0353\n",
      "Epoch 39/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.8541\n",
      "Epoch 00039: val_loss did not improve from 0.99633\n",
      "14152/14152 [==============================] - 3s 184us/sample - loss: 0.8543 - val_loss: 1.0349\n",
      "Epoch 40/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8603\n",
      "Epoch 00040: val_loss did not improve from 0.99633\n",
      "14152/14152 [==============================] - 3s 199us/sample - loss: 0.8593 - val_loss: 1.0214\n",
      "Epoch 41/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.8526\n",
      "Epoch 00041: val_loss did not improve from 0.99633\n",
      "14152/14152 [==============================] - 3s 199us/sample - loss: 0.8533 - val_loss: 1.0313\n",
      "Epoch 42/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.8506\n",
      "Epoch 00042: val_loss did not improve from 0.99633\n",
      "14152/14152 [==============================] - 3s 195us/sample - loss: 0.8512 - val_loss: 1.0290\n",
      "Epoch 43/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.8348\n",
      "Epoch 00043: val_loss did not improve from 0.99633\n",
      "14152/14152 [==============================] - 3s 196us/sample - loss: 0.8353 - val_loss: 1.0136\n",
      "Epoch 44/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.8363\n",
      "Epoch 00044: val_loss did not improve from 0.99633\n",
      "14152/14152 [==============================] - 3s 201us/sample - loss: 0.8359 - val_loss: 1.0368\n",
      "Epoch 45/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.8354\n",
      "Epoch 00045: val_loss did not improve from 0.99633\n",
      "14152/14152 [==============================] - 3s 215us/sample - loss: 0.8360 - val_loss: 1.0452\n",
      "Epoch 46/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.8145\n",
      "Epoch 00046: val_loss did not improve from 0.99633\n",
      "14152/14152 [==============================] - 3s 209us/sample - loss: 0.8151 - val_loss: 1.0419\n",
      "Epoch 47/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.8214\n",
      "Epoch 00047: val_loss did not improve from 0.99633\n",
      "14152/14152 [==============================] - 3s 204us/sample - loss: 0.8208 - val_loss: 1.0400\n",
      "Epoch 48/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.8154\n",
      "Epoch 00048: val_loss did not improve from 0.99633\n",
      "14152/14152 [==============================] - 3s 203us/sample - loss: 0.8152 - val_loss: 1.0175\n",
      "Partial score of fold 3 is: 0.5911348049485639\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 200)               78600     \n",
      "_________________________________________________________________\n",
      "layer_normalization_16 (Laye (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "layer_normalization_17 (Laye (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_18 (Laye (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "layer_normalization_19 (Laye (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 105,801\n",
      "Trainable params: 105,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14153 samples, validate on 3537 samples\n",
      "Epoch 1/100\n",
      "13920/14153 [============================>.] - ETA: 0s - loss: 2.0591\n",
      "Epoch 00001: val_loss improved from inf to 1.12157, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 4s 300us/sample - loss: 2.0521 - val_loss: 1.1216\n",
      "Epoch 2/100\n",
      "14144/14153 [============================>.] - ETA: 0s - loss: 1.4280\n",
      "Epoch 00002: val_loss improved from 1.12157 to 1.10132, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 205us/sample - loss: 1.4283 - val_loss: 1.1013\n",
      "Epoch 3/100\n",
      "13952/14153 [============================>.] - ETA: 0s - loss: 1.2842\n",
      "Epoch 00003: val_loss improved from 1.10132 to 1.08731, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 199us/sample - loss: 1.2850 - val_loss: 1.0873\n",
      "Epoch 4/100\n",
      "13888/14153 [============================>.] - ETA: 0s - loss: 1.2269\n",
      "Epoch 00004: val_loss improved from 1.08731 to 1.06847, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 197us/sample - loss: 1.2292 - val_loss: 1.0685\n",
      "Epoch 5/100\n",
      "13920/14153 [============================>.] - ETA: 0s - loss: 1.1885\n",
      "Epoch 00005: val_loss improved from 1.06847 to 1.05945, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 198us/sample - loss: 1.1899 - val_loss: 1.0595\n",
      "Epoch 6/100\n",
      "14112/14153 [============================>.] - ETA: 0s - loss: 1.1642\n",
      "Epoch 00006: val_loss did not improve from 1.05945\n",
      "14153/14153 [==============================] - 3s 198us/sample - loss: 1.1652 - val_loss: 1.0626\n",
      "Epoch 7/100\n",
      "14144/14153 [============================>.] - ETA: 0s - loss: 1.1360\n",
      "Epoch 00007: val_loss improved from 1.05945 to 1.05557, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 199us/sample - loss: 1.1356 - val_loss: 1.0556\n",
      "Epoch 8/100\n",
      "14112/14153 [============================>.] - ETA: 0s - loss: 1.1133\n",
      "Epoch 00008: val_loss improved from 1.05557 to 1.04649, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 196us/sample - loss: 1.1122 - val_loss: 1.0465\n",
      "Epoch 9/100\n",
      "14080/14153 [============================>.] - ETA: 0s - loss: 1.1109\n",
      "Epoch 00009: val_loss improved from 1.04649 to 1.03814, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 196us/sample - loss: 1.1092 - val_loss: 1.0381\n",
      "Epoch 10/100\n",
      "14016/14153 [============================>.] - ETA: 0s - loss: 1.1066\n",
      "Epoch 00010: val_loss did not improve from 1.03814\n",
      "14153/14153 [==============================] - 3s 187us/sample - loss: 1.1078 - val_loss: 1.0425\n",
      "Epoch 11/100\n",
      "13856/14153 [============================>.] - ETA: 0s - loss: 1.0771\n",
      "Epoch 00011: val_loss improved from 1.03814 to 1.02917, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 190us/sample - loss: 1.0774 - val_loss: 1.0292\n",
      "Epoch 12/100\n",
      "14016/14153 [============================>.] - ETA: 0s - loss: 1.0625\n",
      "Epoch 00012: val_loss improved from 1.02917 to 1.02313, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 189us/sample - loss: 1.0618 - val_loss: 1.0231\n",
      "Epoch 13/100\n",
      "14112/14153 [============================>.] - ETA: 0s - loss: 1.0628\n",
      "Epoch 00013: val_loss did not improve from 1.02313\n",
      "14153/14153 [==============================] - 3s 189us/sample - loss: 1.0640 - val_loss: 1.0293\n",
      "Epoch 14/100\n",
      "14144/14153 [============================>.] - ETA: 0s - loss: 1.0431\n",
      "Epoch 00014: val_loss improved from 1.02313 to 1.01784, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 184us/sample - loss: 1.0427 - val_loss: 1.0178\n",
      "Epoch 15/100\n",
      "14016/14153 [============================>.] - ETA: 0s - loss: 1.0335\n",
      "Epoch 00015: val_loss improved from 1.01784 to 1.01488, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 187us/sample - loss: 1.0322 - val_loss: 1.0149\n",
      "Epoch 16/100\n",
      "13952/14153 [============================>.] - ETA: 0s - loss: 1.0340\n",
      "Epoch 00016: val_loss did not improve from 1.01488\n",
      "14153/14153 [==============================] - 3s 188us/sample - loss: 1.0332 - val_loss: 1.0258\n",
      "Epoch 17/100\n",
      "13856/14153 [============================>.] - ETA: 0s - loss: 1.0182\n",
      "Epoch 00017: val_loss did not improve from 1.01488\n",
      "14153/14153 [==============================] - 3s 199us/sample - loss: 1.0193 - val_loss: 1.0307\n",
      "Epoch 18/100\n",
      "13984/14153 [============================>.] - ETA: 0s - loss: 1.0126\n",
      "Epoch 00018: val_loss did not improve from 1.01488\n",
      "14153/14153 [==============================] - 3s 197us/sample - loss: 1.0136 - val_loss: 1.0163\n",
      "Epoch 19/100\n",
      "14016/14153 [============================>.] - ETA: 0s - loss: 0.9992\n",
      "Epoch 00019: val_loss improved from 1.01488 to 1.01387, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 198us/sample - loss: 0.9995 - val_loss: 1.0139\n",
      "Epoch 20/100\n",
      "14016/14153 [============================>.] - ETA: 0s - loss: 0.9901\n",
      "Epoch 00020: val_loss did not improve from 1.01387\n",
      "14153/14153 [==============================] - 3s 199us/sample - loss: 0.9894 - val_loss: 1.0182\n",
      "Epoch 21/100\n",
      "14016/14153 [============================>.] - ETA: 0s - loss: 0.9820\n",
      "Epoch 00021: val_loss did not improve from 1.01387\n",
      "14153/14153 [==============================] - 3s 187us/sample - loss: 0.9824 - val_loss: 1.0424\n",
      "Epoch 22/100\n",
      "14080/14153 [============================>.] - ETA: 0s - loss: 0.9794\n",
      "Epoch 00022: val_loss did not improve from 1.01387\n",
      "14153/14153 [==============================] - 3s 185us/sample - loss: 0.9794 - val_loss: 1.0284\n",
      "Epoch 23/100\n",
      "14080/14153 [============================>.] - ETA: 0s - loss: 0.9682\n",
      "Epoch 00023: val_loss improved from 1.01387 to 1.01128, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 187us/sample - loss: 0.9690 - val_loss: 1.0113\n",
      "Epoch 24/100\n",
      "14144/14153 [============================>.] - ETA: 0s - loss: 0.9596\n",
      "Epoch 00024: val_loss did not improve from 1.01128\n",
      "14153/14153 [==============================] - 3s 191us/sample - loss: 0.9594 - val_loss: 1.0210\n",
      "Epoch 25/100\n",
      "14144/14153 [============================>.] - ETA: 0s - loss: 0.9351\n",
      "Epoch 00025: val_loss did not improve from 1.01128\n",
      "14153/14153 [==============================] - 3s 183us/sample - loss: 0.9349 - val_loss: 1.0379\n",
      "Epoch 26/100\n",
      "13888/14153 [============================>.] - ETA: 0s - loss: 0.9486\n",
      "Epoch 00026: val_loss improved from 1.01128 to 1.01000, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 184us/sample - loss: 0.9495 - val_loss: 1.0100\n",
      "Epoch 27/100\n",
      "14080/14153 [============================>.] - ETA: 0s - loss: 0.9346\n",
      "Epoch 00027: val_loss improved from 1.01000 to 1.00833, saving model to nn_model.w8\n",
      "14153/14153 [==============================] - 3s 188us/sample - loss: 0.9344 - val_loss: 1.0083\n",
      "Epoch 28/100\n",
      "14112/14153 [============================>.] - ETA: 0s - loss: 0.9330\n",
      "Epoch 00028: val_loss did not improve from 1.00833\n",
      "14153/14153 [==============================] - 3s 197us/sample - loss: 0.9346 - val_loss: 1.0510\n",
      "Epoch 29/100\n",
      "14112/14153 [============================>.] - ETA: 0s - loss: 0.9101\n",
      "Epoch 00029: val_loss did not improve from 1.00833\n",
      "14153/14153 [==============================] - 3s 200us/sample - loss: 0.9102 - val_loss: 1.0368\n",
      "Epoch 30/100\n",
      "13888/14153 [============================>.] - ETA: 0s - loss: 0.9246\n",
      "Epoch 00030: val_loss did not improve from 1.00833\n",
      "14153/14153 [==============================] - 3s 199us/sample - loss: 0.9241 - val_loss: 1.0105\n",
      "Epoch 31/100\n",
      "14144/14153 [============================>.] - ETA: 0s - loss: 0.9066\n",
      "Epoch 00031: val_loss did not improve from 1.00833\n",
      "14153/14153 [==============================] - 3s 203us/sample - loss: 0.9069 - val_loss: 1.0435\n",
      "Epoch 32/100\n",
      "14144/14153 [============================>.] - ETA: 0s - loss: 0.8955\n",
      "Epoch 00032: val_loss did not improve from 1.00833\n",
      "14153/14153 [==============================] - 3s 199us/sample - loss: 0.8952 - val_loss: 1.0551\n",
      "Epoch 33/100\n",
      "13920/14153 [============================>.] - ETA: 0s - loss: 0.8920\n",
      "Epoch 00033: val_loss did not improve from 1.00833\n",
      "14153/14153 [==============================] - 3s 199us/sample - loss: 0.8952 - val_loss: 1.0195\n",
      "Epoch 34/100\n",
      "14016/14153 [============================>.] - ETA: 0s - loss: 0.8854\n",
      "Epoch 00034: val_loss did not improve from 1.00833\n",
      "14153/14153 [==============================] - 3s 202us/sample - loss: 0.8847 - val_loss: 1.0272\n",
      "Epoch 35/100\n",
      "14080/14153 [============================>.] - ETA: 0s - loss: 0.8675\n",
      "Epoch 00035: val_loss did not improve from 1.00833\n",
      "14153/14153 [==============================] - 3s 205us/sample - loss: 0.8672 - val_loss: 1.0465\n",
      "Epoch 36/100\n",
      "13952/14153 [============================>.] - ETA: 0s - loss: 0.8794\n",
      "Epoch 00036: val_loss did not improve from 1.00833\n",
      "14153/14153 [==============================] - 3s 199us/sample - loss: 0.8781 - val_loss: 1.0316\n",
      "Epoch 37/100\n",
      "13952/14153 [============================>.] - ETA: 0s - loss: 0.8742\n",
      "Epoch 00037: val_loss did not improve from 1.00833\n",
      "14153/14153 [==============================] - 3s 224us/sample - loss: 0.8734 - val_loss: 1.0257\n",
      "Epoch 38/100\n",
      "14048/14153 [============================>.] - ETA: 0s - loss: 0.8619\n",
      "Epoch 00038: val_loss did not improve from 1.00833\n",
      "14153/14153 [==============================] - 3s 244us/sample - loss: 0.8625 - val_loss: 1.0121\n",
      "Epoch 39/100\n",
      "13888/14153 [============================>.] - ETA: 0s - loss: 0.8480\n",
      "Epoch 00039: val_loss did not improve from 1.00833\n",
      "14153/14153 [==============================] - 3s 216us/sample - loss: 0.8520 - val_loss: 1.0118\n",
      "Epoch 40/100\n",
      "14112/14153 [============================>.] - ETA: 0s - loss: 0.8525\n",
      "Epoch 00040: val_loss did not improve from 1.00833\n",
      "14153/14153 [==============================] - 3s 208us/sample - loss: 0.8532 - val_loss: 1.0365\n",
      "Epoch 41/100\n",
      "13984/14153 [============================>.] - ETA: 0s - loss: 0.8471\n",
      "Epoch 00041: val_loss did not improve from 1.00833\n",
      "14153/14153 [==============================] - 3s 215us/sample - loss: 0.8477 - val_loss: 1.0287\n",
      "Epoch 42/100\n",
      "13920/14153 [============================>.] - ETA: 0s - loss: 0.8316\n",
      "Epoch 00042: val_loss did not improve from 1.00833\n",
      "14153/14153 [==============================] - 3s 209us/sample - loss: 0.8341 - val_loss: 1.0490\n",
      "Epoch 43/100\n",
      "14016/14153 [============================>.] - ETA: 0s - loss: 0.8432\n",
      "Epoch 00043: val_loss did not improve from 1.00833\n",
      "14153/14153 [==============================] - 3s 226us/sample - loss: 0.8434 - val_loss: 1.0314\n",
      "Epoch 44/100\n",
      "13984/14153 [============================>.] - ETA: 0s - loss: 0.8301\n",
      "Epoch 00044: val_loss did not improve from 1.00833\n",
      "14153/14153 [==============================] - 3s 225us/sample - loss: 0.8303 - val_loss: 1.0447\n",
      "Epoch 45/100\n",
      "14080/14153 [============================>.] - ETA: 0s - loss: 0.8198\n",
      "Epoch 00045: val_loss did not improve from 1.00833\n",
      "14153/14153 [==============================] - 3s 221us/sample - loss: 0.8211 - val_loss: 1.0561\n",
      "Epoch 46/100\n",
      "14080/14153 [============================>.] - ETA: 0s - loss: 0.8119\n",
      "Epoch 00046: val_loss did not improve from 1.00833\n",
      "14153/14153 [==============================] - 3s 200us/sample - loss: 0.8125 - val_loss: 1.0499\n",
      "Epoch 47/100\n",
      "14112/14153 [============================>.] - ETA: 0s - loss: 0.8093\n",
      "Epoch 00047: val_loss did not improve from 1.00833\n",
      "14153/14153 [==============================] - 3s 186us/sample - loss: 0.8096 - val_loss: 1.0234\n",
      "Partial score of fold 4 is: 0.5742217369410672\n",
      "Our oof cohen kappa score is:  0.5854657034709244\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#cnn_model = Cnn_Model(reduce_train, ajusted_test, features, categoricals=categoricals)\n",
    "nn_model = Nn_Model(reduce_train, ajusted_test, features, categoricals=categoricals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Median Stacking models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memo  \n",
    "My first try is simple median stacking. Just use raw prediction data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lgb = lgb_model.y_pred\n",
    "cat = cat_model.y_pred\n",
    "xgb = xgb_model.y_pred\n",
    "nn = nn_model.y_pred\n",
    "\n",
    "concat = sample_submission.drop(\"accuracy_group\", axis=1)\n",
    "concat[\"lgb\"] = lgb\n",
    "concat[\"cat\"] = cat\n",
    "concat[\"xgb\"] = xgb\n",
    "concat[\"nn\"] = nn\n",
    "\n",
    "concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat['median'] = concat.iloc[:, 1:].median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_pred=concat[\"median\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dist = Counter(reduce_train['accuracy_group'])\n",
    "for k in dist:\n",
    "    dist[k] /= len(reduce_train)\n",
    "reduce_train['accuracy_group'].hist()\n",
    "\n",
    "acum = 0\n",
    "bound = {}\n",
    "for i in range(3):\n",
    "    acum += dist[i]\n",
    "    bound[i] = np.percentile(final_pred, acum * 100)\n",
    "print(bound)\n",
    "\n",
    "def classify(x):\n",
    "    if x <= bound[0]:\n",
    "        return 0\n",
    "    elif x <= bound[1]:\n",
    "        return 1\n",
    "    elif x <= bound[2]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "    \n",
    "final_pred = np.array(list(map(classify, final_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample_submission['accuracy_group'] = final_pred.astype(int)\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "sample_submission['accuracy_group'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class Model2Linear:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "\n",
    "    def fit(self, tr_x, tr_y, va_x, va_y):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(tr_x)\n",
    "        tr_x = self.scaler.transform(tr_x)\n",
    "        self.model = Ridge()\n",
    "        self.model.fit(tr_x, tr_y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = self.scaler.transform(x)\n",
    "        pred = self.model.predict(x)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = reduce_train.drop([\"installation_id\", \"accuracy_group\"],axis=1)\n",
    "train_y = reduce_train[\"accuracy_group\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# \n",
    "def predict_cv(model, train_x, train_y, test_x):\n",
    "    preds = []\n",
    "    preds_test = []\n",
    "    va_idxes = []\n",
    "\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "\n",
    "    # \n",
    "    for i, (tr_idx, va_idx) in enumerate(kf.split(train_x)):\n",
    "        tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "        #model.fit(tr_x, tr_y, va_x, va_y)\n",
    "        pred = model.oof_pred[va_idx]\n",
    "        preds.append(pred)\n",
    "        pred_test = model.y_pred\n",
    "        preds_test.append(pred_test)\n",
    "        va_idxes.append(va_idx)\n",
    "\n",
    "    # \n",
    "    va_idxes = np.concatenate(va_idxes)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    order = np.argsort(va_idxes)\n",
    "    pred_train = preds[order]\n",
    "\n",
    "    # \n",
    "    preds_test = np.mean(preds_test, axis=0)\n",
    "\n",
    "    return pred_train, preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for attr in dir(lgb_model.__init__):\n",
    "#    if callable(getattr(lgb_model.__init__, str(attr))):\n",
    "#        print(attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.18068038  2.71382171  2.31838337  1.89889971  1.82758286  2.28066765\n",
      "  1.68685053  1.86519308  0.34752583  1.00100099  2.14744238  1.77239758\n",
      "  2.193064    2.38311114  0.81024001  1.27630131  2.14131819  1.92161654\n",
      "  2.76031266  1.99871579  2.12539527  1.75096208  2.18335078  2.5174695\n",
      "  1.45235426  0.88785893  2.46057802  1.4149322   1.98737722  2.44812231\n",
      "  0.27115814  1.86590142  0.37717168  1.26410088  2.57071936  2.03946562\n",
      "  0.68446935  1.53297937  2.12500382  0.91420114  1.76659537  2.03362076\n",
      "  0.70711792  1.28326599  2.25619535  2.48044898  2.137893    0.96436011\n",
      "  2.32568331  2.37388824  0.17198205  1.18557448  2.58934358  2.41662527\n",
      "  2.16759641  1.20434047  0.18694111  1.92617584  1.51130951  0.94967475\n",
      "  1.86059785  1.98274942  1.31923009  2.08176128  1.18466468  2.34119962\n",
      "  2.31474795  2.25550639  1.41231044  0.32938455  1.92451609  2.43872031\n",
      "  1.32107323  2.05196435  2.32786087  2.34009394  1.77551166  2.40272383\n",
      "  2.02279814  1.30631718  2.05622392  1.55610661  2.43616298  2.5876084\n",
      "  1.97887452  2.2735843   2.33519888  0.49476529  2.72220735  2.38200085\n",
      "  2.35330337  1.66620636  1.79537854  0.9856177  -0.07528285  2.127277\n",
      "  2.21132876  1.57974843  0.67138841  1.81894307  2.46028684  0.18625645\n",
      "  2.27961949  1.7623926   0.45461663  2.75517169  1.92451609  1.93504175\n",
      "  2.20501154  2.14321171  1.6060496   2.7507544   1.65730474  1.88594657\n",
      "  1.44619934  1.97857179  1.43010903  2.2390783   2.2737028   1.32418597\n",
      "  2.55202713  2.04751514  1.60763323  2.39463312  2.53305848  0.66316466\n",
      "  2.61641613  2.10008079  2.02561496  2.18098623  2.10069194  1.18393206\n",
      "  1.59367789  1.92451609  2.08389642  2.09409816  2.6733748   0.58617853\n",
      "  1.500597    2.05869278  2.44100576  2.34177596  1.05467791  0.27513909\n",
      "  2.13672319  2.39130498  1.0951691   1.88581725  1.8541643   1.73146981\n",
      "  1.56917308  1.12873772  2.0351778   0.33930317  1.87718649  0.93622704\n",
      "  2.26846161  0.3515544   1.89338762  2.60268062  1.60944354  0.32060387\n",
      "  2.7300608   2.70724766  0.91919263  2.25782497  2.41831176  1.17250918\n",
      "  2.49643471  1.16890854  1.63316732  1.73967878  2.53367731  2.72158511\n",
      "  2.20474218  2.31236695  1.77778468  2.42454679  0.61856465  0.31151464\n",
      "  2.19894923  1.33247662  1.15692828  0.49488572  2.23150164  1.41736949\n",
      "  2.41153723  1.79102385  2.31234231  2.28855316  2.79149435  1.56318799\n",
      "  0.75379543  0.61681111  1.82271407  2.6181636   1.29527431  0.68856115\n",
      "  1.43200904  2.33035179  2.29148168  1.92366572  0.48246658  0.20832086\n",
      "  2.75083831  0.67062927  2.32351373 -0.04223599  1.60807091  1.85192286\n",
      "  2.30907551  1.20081726  0.62370567  2.62333336  2.59533889  1.69214139\n",
      "  1.97139916  1.502373    0.15573991  0.29533877  1.72846831  2.31970487\n",
      "  2.40392194  0.2234077   0.69861088  2.37519742  2.31825652  0.33646312\n",
      "  1.99788687  1.43722487  0.96400628  2.20568635  2.33601828  2.61565121\n",
      "  2.20849992  2.30593803  1.31788865  1.57659603  1.94201309  1.76290833\n",
      "  1.8058053   1.23820975  2.38193847  2.10058034  2.39303373  2.3749548\n",
      "  2.09630594  2.12321562  0.48903514  0.67379058  2.29556082  2.0297439\n",
      "  2.21676077  1.61628513  2.75879504  2.09252778  1.40534984  2.38138812\n",
      "  2.50025773  2.41802367  1.30353474  2.3626353   2.14700456  1.12255954\n",
      "  1.47684973  2.24035133  2.05014203  1.89492822  0.35499034  1.62708792\n",
      "  0.38001981  1.69915956  2.04989014  0.31265749  1.04926032  1.91965049\n",
      "  0.54419802  1.19509684  2.34315035  1.69079329  2.15643721  2.12174203\n",
      "  1.08861217  1.9074925   1.88970746  1.87909518  1.92457189  0.01448186\n",
      "  2.25479755  2.46930298  2.19611254  2.25479755  1.46344256  1.30204915\n",
      "  2.01057083  2.4459902   1.42798627  2.23432159  2.13834287  2.62172867\n",
      "  0.16330818  2.22558714  0.90583766  0.88352844  2.29267966  1.47910995\n",
      "  2.02717766  2.12728262  2.17704933  2.2065464   2.38555233  1.67416928\n",
      "  2.01781564  1.07766744  1.02844312  2.49092685  1.91720314  1.64455099\n",
      "  0.20602884  2.0881742   2.3702247   2.10323923  2.43760804  2.35682409\n",
      "  1.90794233  2.26526424  2.48621193  1.61626538  1.92451609  1.81365702\n",
      "  1.61803704  0.8597106   0.57239999  2.1188608   1.84204178  2.14144622\n",
      "  1.98665069  1.44480553  1.81032005  0.66342315  2.60357326  1.68913566\n",
      "  1.53956601  1.28475782  0.87703701  2.18424008  2.18906515  1.15959007\n",
      "  1.83839634  2.17011448  2.26030659  0.27173605  2.33264569  2.36519376\n",
      "  2.4644375   2.02308835  2.65409017  2.07660681  0.74760797  2.83119562\n",
      "  1.78378984  2.02425368  2.07068734  2.33741886  2.20147319  2.2795743\n",
      "  2.53090717  2.37709111  2.59959111  2.46315727  2.13624934  1.42777767\n",
      "  1.6513239   2.39537896  1.74308871  1.777737    2.07464976  1.28416755\n",
      "  2.31867257  2.1959051   2.58737629  2.475153    1.96726494  0.86948627\n",
      "  1.7572007   0.28298326  2.2065464   1.3791516   1.24338973  2.30465707\n",
      "  1.64371843  1.9442711   1.29901528  2.60512994  2.27212469  1.23593113\n",
      "  2.1770141   1.69139707  0.20495055  2.34244006  2.34352148  2.57865872\n",
      "  1.5610527   1.94734653  2.1566389   1.32573058  1.78384047  2.39775237\n",
      "  2.04433048  2.08760291  1.46731481  1.47469786  2.53486436  1.78595643\n",
      "  2.93174427  2.0268455   0.47624814  2.48315015  0.56117973  0.03053038\n",
      "  1.41300294  1.15091253  2.21414171  1.88291297  0.50051891  2.37781093\n",
      "  2.64576729  2.42285153  1.81295779  1.6230605   2.18129199  0.27607984\n",
      "  2.45383345  0.16938662  2.50832155  1.78267451  2.22903169  1.07328727\n",
      "  0.25915122  1.87105077  2.34772415  0.65982374  1.74711538  2.01008082\n",
      "  0.37114925  0.70233233  0.25021672  2.04139067  2.41141295  0.7592312\n",
      "  0.82190616  1.96357059  2.64054776  2.25117024  2.26029126  1.79636578\n",
      "  2.66593366  2.08535136  1.30791194  2.05029318  1.4860008   0.635953\n",
      "  2.63430176  2.3255166   2.59590826  2.02900833  1.05602096  2.12112416\n",
      "  0.97712752  2.37585119  2.34296441  2.56672353  2.33444527  2.40059428\n",
      "  1.83896528  2.1152722   1.51079335  2.58146319  1.58689125  0.73294219\n",
      "  2.26721687  1.97881547  2.24991747  1.94495378  1.77717933  1.07999575\n",
      "  1.88495277  2.23582404  0.31727036  2.34612992  0.9783531   2.35752971\n",
      "  1.13494524  2.36681533  1.91550105  2.69063668  2.84831007  2.44776004\n",
      "  0.38867838  2.45040251  2.33478575  2.3032425   1.78332719  2.58627145\n",
      "  2.13518352  1.29643382  2.12543643  2.51390284  2.20384915  1.43162205\n",
      "  0.81209651  1.37384043  1.49387432  2.46471228  1.26153247  1.58158355\n",
      "  2.11841365  2.13508259  2.34858514  2.23018848  2.55117672  2.53763726\n",
      "  2.35481443  1.92239685  0.72483608  1.55483336  0.52496315  2.11818147\n",
      "  1.77284632  0.63367111  1.66755271  1.49158063  0.77734701  2.49102296\n",
      "  2.24082001  1.43469259  1.3845943   2.69794332  1.89007202  0.43098767\n",
      "  2.66414176  1.75638086  2.51746851  2.07164059  2.1576527   2.49783366\n",
      "  1.14602107  2.13624934  1.532945    1.10361849  1.81926742  2.13788897\n",
      "  1.89989184  0.83810695  1.91675849  1.98432872  0.54351633  2.29228419\n",
      "  2.30646098  1.82725934  2.1391802   1.5610527   2.33301031  1.13763426\n",
      "  2.32577779  1.42621268  2.48401154  2.41289444  2.22856778  2.35654831\n",
      "  0.06315207  2.40177678  1.82969707  2.65599848  2.54669152  2.24007791\n",
      "  2.17799718  2.00398915  2.31322294  2.66405861  0.69733496  2.68120196\n",
      "  1.28789123  2.11258172  1.27730294  0.74991811  2.19303798  2.19623341\n",
      "  1.05467791  2.1057501   1.92451609  2.6821558   2.2727212   1.91190775\n",
      "  2.50455419  2.20530575  2.58151302  2.03528293  1.69413942  2.25124278\n",
      "  2.15957425  0.42281655  0.84354781  1.55713248  1.48560388  1.23072514\n",
      "  0.87335734  2.34454688  1.90911566  2.06534463  1.2774825   1.89395411\n",
      "  2.5284062   1.50199997  2.04155476  2.5935411   2.17605625  2.33789451\n",
      "  0.83689974  0.41328401  2.47483995  0.56056091  2.21287044  1.18522423\n",
      "  1.73241765  2.39424638  0.89191553  1.2604126   1.52579738  2.19631212\n",
      "  2.07441819  2.46484309  2.54135965  1.02107524  2.24880718  1.10633399\n",
      "  1.76504078  1.95095109  1.87862157  2.48616401  1.02575556  1.45313965\n",
      "  2.2049376   1.496909    2.16512642  2.03340664  1.85527221  0.46669682\n",
      "  2.26217061  2.3272252   1.60145318  1.70103429  2.23186543  1.05467791\n",
      "  2.4140343   2.86154936  2.59492794  0.38380708  2.23450774  2.48578728\n",
      "  1.6504001   0.56310373  0.99371966  2.11187315  0.54831251  1.48814657\n",
      "  1.92451609  1.32047723  1.86834809  2.56023976  2.57339794  2.17529286\n",
      "  2.2108133   2.03939374  1.37726807  2.18906515  2.20656822  0.23611098\n",
      "  2.26890687  0.96476833  0.84885083  0.79616634  1.51284564  1.98274942\n",
      "  1.95963928  2.21513957  0.32119502  1.5610527   2.27476872  1.54983582\n",
      "  0.78335155  1.17126083  2.59828558  1.79382214  2.36978339  1.88165069\n",
      "  0.38950986  1.3138898   2.44562238  2.1157254   0.89653831  2.57988592\n",
      "  1.83900919  2.49390634  1.69678684  2.05263601  2.55979233  1.92066626\n",
      "  2.45549299  2.37099761  2.21648909  2.08634753  1.21551455  2.40782284\n",
      "  1.67912958  1.89137288  2.07630346  1.88394521  2.48850989  2.25046574\n",
      "  2.23341774  2.45070981  1.5994782   1.31504316  1.99924141  2.43937112\n",
      "  1.75355061  2.58969678  0.50640908  2.21278427  0.93842152  1.87467789\n",
      "  0.89712374  0.17880152  0.16475065  2.10963424  1.83947503  2.71454554\n",
      "  2.60525233  2.06194846  2.23910231  1.781997    0.54975937  1.74967737\n",
      "  2.69952071  2.29626545  1.68365638  2.14146242  1.76262054  0.48087015\n",
      "  2.49883431  2.14382071  2.49063331  2.55640203  1.66294705  1.67770706\n",
      "  2.73713465  1.858318    1.8772214   2.25479755  2.16418178  2.47635766\n",
      "  1.46272687  2.1397306   2.00972743  1.20375004  1.62864464  1.72121815\n",
      "  2.03382801  1.9942857   0.14200479  2.1601959   2.25479755  0.93158224\n",
      "  2.43768494  0.89143261  1.28495015  1.97626595  1.32862901  2.57154307\n",
      "  0.29203595  0.42625866  2.34533703  2.29653859  1.53856317  1.02487339\n",
      "  2.25479755  2.30279707  2.19233978  2.3488022   0.56573661  1.96458743\n",
      "  2.68848034  1.86705671  1.92208541  1.92752817  2.13478572  1.7126174\n",
      "  0.45153111  1.80553788  1.10275993  2.02714543  1.14548478  1.73648547\n",
      "  0.20529077  1.68113515  1.73200871  1.08568876  1.4809394   1.71635692\n",
      "  0.38999235  0.52577703  2.70869102  1.91051764  2.68543458  2.25479755\n",
      "  1.93203655  2.23085275  1.67882481  2.62122129  1.73772248  2.4983804\n",
      "  1.78017862  1.6024299   0.58812865  2.67741642  1.1898508   2.08813288\n",
      "  1.12773923  2.07673458  0.64922447  2.17735875  2.5195166   1.56585707\n",
      "  1.58118967  2.59321801  1.26077143  1.99871579  2.39059211  2.35081754\n",
      "  2.35835165  1.15404521  2.10714622  0.6237098   1.78335086  2.14144622\n",
      "  1.90367554  2.34258943  0.32134497  2.05970902  2.799479    1.70159729\n",
      "  2.2853257   2.72205683  2.02115552  2.10989626  1.50998691  2.46198776\n",
      "  2.25560294  1.44562174  2.26628509  0.7416365   2.14623172  2.53384639\n",
      "  1.60645727  1.47099866  2.64171257  2.27480577  2.48486171  2.14685445\n",
      "  2.42303108  1.94043343  2.59799565  0.46221234  0.4921123   1.47194033\n",
      "  0.81663323  1.99447525  2.0703032   0.46333743  0.2335567   0.41360719\n",
      "  2.02103958  2.18075521  2.42206958  2.67711021  2.06564285  2.19175706\n",
      "  2.59249287  2.40008652  1.03413735  1.92451609  0.60228386  1.69554076\n",
      "  0.94818968  2.30909479  2.70438135  2.08559913  1.67003893  1.90928421\n",
      "  1.95195819  1.55420401  1.9838276   2.15976607  1.04921977  1.18144008\n",
      "  2.5233039   1.83899308  1.75674548  2.32181064  0.86316363  0.78153148\n",
      "  2.48100321  2.3687751   1.62210745  0.26851759  2.32520917  1.92251741\n",
      "  1.71967367  2.40266102  1.87044708  1.59479507  1.77605876  0.13125431\n",
      "  0.70287339  2.35130656  2.43671971  2.56312575  2.29502781  0.84174991\n",
      "  2.11583957  2.36294545  1.55534597  1.69100295  2.23820266  1.55876648\n",
      "  1.77666147  0.83887432  1.3770067   2.50286993  0.29129483  2.18906515\n",
      "  1.66421607  1.99052993  2.013034    0.67494427  2.35219049  2.08226292\n",
      "  1.53767177  1.97376606  2.63576032  2.52062685  2.34217624  1.15763577\n",
      "  0.5989972   2.39135491  1.57502741  0.92360433  2.56630465  1.16550624\n",
      "  0.30849955  1.96241067  2.32527799  1.01672285  2.34728438  1.76250013\n",
      "  2.08121545  2.21821323  1.08844404  1.87474719  0.57378712  0.28169479\n",
      "  0.76488121  1.101946    1.14513226  2.08954424  0.37018028  2.36231648\n",
      "  2.30790763  1.80420799  2.4532674   1.27623383  1.85961658  2.61718973\n",
      "  0.79809056  2.45784113  1.10773299  2.34525677  0.96121845  2.50231555\n",
      "  1.78286633  2.14700731  1.1269668   0.89713823  1.56951123  2.55396768\n",
      "  0.66604566  2.1555936   1.50200974  1.46725764]\n"
     ]
    }
   ],
   "source": [
    "print(lgb_model.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.2423043  2.51824874 2.32587478 2.07301134 1.83499292 2.20469362\n",
      " 1.50762552 1.4876648  0.57155286 0.81924877 2.11794716 1.70977017\n",
      " 2.00017807 2.38337231 0.83881487 1.50267175 2.13414562 1.85045847\n",
      " 2.73098296 1.91281953 2.01065606 1.58209366 2.09495625 2.1309742\n",
      " 1.041789   1.02932601 1.86778167 1.50224295 1.79861307 2.41179606\n",
      " 0.54533494 1.55515447 0.39702424 1.1110895  2.4865323  2.1966837\n",
      " 0.87114038 1.51166312 1.61514735 1.05094579 1.66420266 1.46805413\n",
      " 0.96102829 1.22307314 1.87580734 2.33949623 2.15156496 1.22635543\n",
      " 2.23464119 2.32940024 0.3865218  1.3897329  2.52725029 2.36690956\n",
      " 1.76796883 1.18831196 0.42679524 2.01408845 1.35572144 0.88108769\n",
      " 1.94018781 1.60452271 0.96503988 1.99169376 1.29266807 2.29577681\n",
      " 1.85601357 2.37312689 1.0426396  0.32065721 1.33985427 2.30074865\n",
      " 1.42482954 1.71374765 2.41447237 2.21737805 1.58424625 2.24214202\n",
      " 1.96396348 1.36541975 1.6219053  1.56495352 2.3306815  2.52491355\n",
      " 1.8906849  2.13630271 2.05152339 0.4938774  2.52000591 2.0946762\n",
      " 2.36053532 1.67176053 1.82613218 1.25819221 0.09658174 2.05025065\n",
      " 1.84804463 1.87506115 0.64881894 1.59135312 2.39209896 0.39132292\n",
      " 2.0557943  1.67248946 0.52739857 2.72573602 1.33985427 1.98871884\n",
      " 1.88261589 2.00397292 1.57409221 2.68586558 1.79792655 1.85411322\n",
      " 1.72901499 1.79864338 1.51584676 2.10303026 2.3956289  1.26758024\n",
      " 2.52613339 1.48862433 1.69066209 2.3088631  2.57136416 0.59984648\n",
      " 2.43224704 1.70888484 2.13211921 1.71820822 1.98174879 1.16343798\n",
      " 1.51965117 1.33985427 1.82040337 1.58761415 2.4247759  0.68643788\n",
      " 1.36860347 1.98102388 2.36404952 2.33894777 1.06056261 0.40021848\n",
      " 1.75373062 2.24131241 0.80292191 1.93303055 1.73882607 1.58088696\n",
      " 1.78859395 0.97370395 1.95648262 0.33257845 1.91608101 0.68879329\n",
      " 2.22886923 0.49198999 1.84655622 2.46875691 1.77359051 0.66464544\n",
      " 2.378445   2.518511   0.95944245 1.97687975 2.24935666 1.08212321\n",
      " 2.40807632 1.27015409 1.52530578 1.2391786  2.47996083 2.47954017\n",
      " 1.85225374 2.19581816 1.77247792 2.36135325 0.66696288 0.34988602\n",
      " 2.10825542 1.05087402 1.48670167 0.49005023 1.88661343 1.16222982\n",
      " 2.2466841  1.57134232 2.28098947 2.16098705 2.59876242 1.58839095\n",
      " 0.79667431 0.59848457 1.97502068 2.55632633 1.24914585 0.65617716\n",
      " 1.24673422 2.24202293 2.14794368 1.69260311 0.70249927 0.3071652\n",
      " 2.63407505 0.78164677 1.98810577 0.30883431 1.73898456 1.69421765\n",
      " 2.06181666 1.10569195 0.66502359 2.38985339 2.00613075 1.70054437\n",
      " 1.91448197 1.43582836 0.40101179 0.19858603 1.60412788 2.14325428\n",
      " 2.3442893  0.09757071 0.57981343 2.2112062  2.39588222 0.54830968\n",
      " 1.89401439 1.15308991 1.01479283 2.23962682 2.1533393  2.3139205\n",
      " 2.19295189 2.07392532 1.28050345 1.27586713 1.59277564 1.72852108\n",
      " 1.16766429 1.33298703 2.14649487 1.9644011  2.13813728 2.21077269\n",
      " 1.83335418 2.04439843 0.55227452 0.5362452  2.1743331  1.94040704\n",
      " 2.23003569 1.52321717 2.73286909 1.993503   1.23529711 2.34155884\n",
      " 2.36114442 2.31571773 1.25362322 2.09632006 2.0981769  0.93325275\n",
      " 1.58501896 2.35380641 2.04090479 2.01163152 0.16140964 1.24892373\n",
      " 0.53842063 1.18584788 2.11463639 0.32593818 0.74630566 2.04811478\n",
      " 0.86395666 1.08811142 2.10094249 1.279965   1.99669537 2.10023177\n",
      " 1.03195149 2.23684415 1.71844414 1.91092157 1.36535135 0.25029476\n",
      " 2.24004832 2.38288346 1.84000757 2.24004832 1.5318011  1.26825881\n",
      " 1.66943094 2.06658515 1.15532763 2.23475987 2.16174266 1.97638211\n",
      " 0.40244132 1.92145857 0.83828329 0.75644194 2.36930582 1.34823659\n",
      " 1.83842328 2.07787293 1.75161967 2.19831723 2.22919291 1.55681255\n",
      " 2.10038364 1.1157784  1.05239654 2.49644855 2.02128264 1.56027204\n",
      " 0.362236   2.07895628 2.38589185 1.44267008 2.41348058 2.29807752\n",
      " 2.06009993 2.11102775 1.93597975 1.36809985 1.33985427 1.66794345\n",
      " 1.32562453 1.03246191 0.47306945 1.91695639 1.71480605 2.20848349\n",
      " 1.86846679 1.5645352  1.68454912 0.53150124 2.59532237 1.85089335\n",
      " 1.20934696 1.27108289 1.01807553 2.09519759 2.12493753 1.13982567\n",
      " 1.7820622  2.09193811 2.28644836 0.32694348 2.10639048 2.38034311\n",
      " 2.28517795 1.78555483 2.56187439 2.09868419 0.7913323  2.73733985\n",
      " 1.77873799 1.79314452 2.06723979 2.28364637 1.59471375 2.02798757\n",
      " 2.29059571 2.36115259 2.45676184 2.05293912 2.24571562 1.57357991\n",
      " 1.59036687 2.31631804 1.73753756 1.61717018 1.79130596 1.39585301\n",
      " 2.22596058 2.03818682 2.44606867 2.27215633 1.85021368 0.68632035\n",
      " 1.63527289 0.45599559 2.19831723 1.16523145 1.39277819 1.86344996\n",
      " 1.69783911 2.1886279  1.43039137 2.57176876 2.17306304 1.0117249\n",
      " 1.96445614 1.76071933 0.29857552 2.22837806 2.2552664  2.40345305\n",
      " 1.73029563 2.06256321 2.15892297 1.32611305 1.42379975 2.09428358\n",
      " 1.71589285 1.96900746 1.33864392 1.61172664 2.38147029 1.79791865\n",
      " 2.80891514 1.9719477  0.6142998  2.35140237 0.57379846 0.47891989\n",
      " 1.03479254 1.12967782 2.0134553  1.61873865 0.48788678 2.2263191\n",
      " 2.59650102 2.28206623 1.77027452 1.61751658 2.18153813 0.3468188\n",
      " 2.42427769 0.43070682 2.41815433 1.65611136 2.05439597 1.02992083\n",
      " 0.62965728 1.73608363 2.15056556 0.61674415 1.53545158 1.44939728\n",
      " 0.71000478 0.69225791 0.44958635 1.99121392 2.28811553 0.99969292\n",
      " 0.89387353 1.84208941 2.46627116 2.18074018 2.09103405 1.84673703\n",
      " 2.50286162 2.07624722 1.26819508 1.8894439  1.45483133 0.51151572\n",
      " 2.39332002 2.10536435 2.12699798 1.78962439 0.96090686 2.0920451\n",
      " 1.20198946 2.34362835 1.92615098 2.51650262 2.16852045 2.33805481\n",
      " 1.69260085 1.61706077 1.480468   2.51981398 1.50705412 0.85704794\n",
      " 2.24883854 1.96023092 2.03369525 1.93008205 1.901739   1.06512143\n",
      " 1.59449944 1.84289333 0.20693597 2.18711177 0.76053917 2.37817183\n",
      " 0.98576844 2.4423165  1.55249599 2.61078852 2.69034249 2.45485851\n",
      " 0.58939951 2.28698084 2.34374514 2.11363149 1.67610896 2.2720007\n",
      " 2.00379178 1.3665821  1.74946323 2.42325804 2.18017855 1.52993995\n",
      " 0.70116507 1.54814553 1.37671722 2.44159693 1.27560799 1.10000612\n",
      " 2.03163901 1.93485075 2.21226349 2.21808124 2.3389878  2.41828778\n",
      " 2.25042245 1.7370204  0.61576188 1.5824053  0.78417923 1.93258628\n",
      " 1.35056895 0.47800521 1.7122575  1.70466837 1.10026079 2.49942759\n",
      " 2.05036527 1.30094635 1.13418835 2.67058241 1.77819374 0.32453435\n",
      " 2.34974265 1.5712744  1.91064328 1.68265614 2.16422853 2.36499751\n",
      " 1.17612629 2.24571562 1.37397367 1.34501335 1.81227732 1.9904584\n",
      " 1.76544636 0.66595919 1.82056585 1.80804235 0.65611425 1.90104544\n",
      " 2.17032024 1.74594051 1.90064266 1.73029563 2.30439425 1.18310685\n",
      " 2.28308496 1.02115366 2.52665862 2.38548654 2.06951302 2.17743734\n",
      " 0.12769498 2.27756611 1.873826   2.65626812 2.48639932 1.96669719\n",
      " 2.13603565 1.50379986 2.21609136 2.45630118 0.7399749  2.531398\n",
      " 1.4096716  2.06775531 1.09265952 0.59540879 1.92379275 1.69490066\n",
      " 1.06056261 2.15376097 1.33985427 2.6753965  1.9641026  1.53780796\n",
      " 2.35577717 2.05254027 2.55243641 1.87935922 1.43831718 1.99777198\n",
      " 2.1828737  0.87979938 0.88479887 1.23632282 1.54484877 1.4359622\n",
      " 1.04246919 2.31791657 1.90835506 1.99769339 1.16240692 1.95880923\n",
      " 2.37097603 1.13806832 1.99758035 2.51005319 2.13058257 1.96763924\n",
      " 0.44541422 0.47772607 2.47913679 0.74937963 1.98086783 1.04588933\n",
      " 1.58593065 1.97172886 1.0559188  1.46533939 1.54546124 2.11474887\n",
      " 2.20407987 2.33913234 2.54091063 0.88018414 1.62848555 1.18273695\n",
      " 1.78794163 1.73791036 1.57375768 2.40863889 0.86137079 1.45176615\n",
      " 1.81641743 1.24278332 1.99352464 1.90088215 1.78840205 0.39609864\n",
      " 1.84943563 2.17513517 1.55985467 1.75012589 1.88068447 1.06056261\n",
      " 2.20193726 2.69001293 2.38092607 0.43648314 2.41005397 2.47554898\n",
      " 1.56010157 0.43074802 0.95304273 1.9478426  0.56369478 1.4836387\n",
      " 1.33985427 1.34768991 1.37618899 2.34540099 2.48167804 1.91297314\n",
      " 1.99151686 1.90581423 1.19318929 2.12493753 2.16566536 0.41376851\n",
      " 1.94764209 0.97001344 0.84251583 0.8105445  1.51414081 1.60452271\n",
      " 1.66605574 2.16741353 0.14916758 1.73029563 2.09521949 1.62615159\n",
      " 0.91723894 1.35336488 2.52086264 1.89934817 2.33734944 1.80277625\n",
      " 0.62681174 1.29031393 2.27615011 1.77750012 0.85071649 2.49102381\n",
      " 1.91243649 2.21162558 1.7473498  1.92126071 2.13844731 1.68374282\n",
      " 2.21776566 2.00923982 2.09780124 1.81743407 1.25480798 2.42339018\n",
      " 1.54828843 1.72685638 1.59914792 1.76832214 2.45291197 1.97865358\n",
      " 2.01951051 2.4710356  1.78692955 1.2781664  1.9916718  2.24677002\n",
      " 1.71653846 2.40673786 0.95163907 2.23537818 0.95910858 1.89346457\n",
      " 0.68919004 0.28674271 0.43958851 2.31054723 1.7728678  2.41810355\n",
      " 2.37288016 1.87933365 1.93157402 1.46163499 0.75713827 1.50244842\n",
      " 2.47521219 2.31839165 1.66618454 1.68003562 1.80498463 0.79750393\n",
      " 2.47156844 2.1681895  2.54014122 2.49154377 1.6770874  1.67617303\n",
      " 2.60824305 1.56464714 1.83565217 2.24004832 1.89962763 2.36393583\n",
      " 1.28349635 2.0943267  1.8284384  1.48645011 1.64557555 1.45275456\n",
      " 1.64588702 2.07499349 0.43280227 2.1545513  2.24004832 0.45523058\n",
      " 2.43087614 0.87883183 1.35462064 2.04126251 1.58763954 2.43464467\n",
      " 0.60073023 0.37290766 2.28040019 1.99142778 1.47790803 0.98598754\n",
      " 2.24004832 2.29034775 1.73700097 2.36885744 0.54503107 1.97343984\n",
      " 2.50619027 1.56521249 1.63833737 1.71443051 2.04394615 1.10270037\n",
      " 0.4677085  1.80219647 1.0271996  1.89418    1.14259881 1.4696579\n",
      " 0.32307241 1.78353915 1.81604615 1.08635175 1.75251526 1.5691883\n",
      " 0.63024599 0.46189005 2.31657732 1.87336871 2.45020014 2.24004832\n",
      " 2.02474245 2.18492317 1.47964433 2.56135106 1.73065433 2.39232543\n",
      " 1.55158567 1.79341894 0.65185865 2.52762762 1.21622789 1.60572386\n",
      " 1.32018825 1.96369398 0.79947406 2.17522785 2.47010526 1.51061809\n",
      " 1.44977346 2.33124879 1.09590374 1.91281953 2.24509817 2.3781212\n",
      " 2.29444739 1.05258056 2.03311992 0.61129972 1.83921206 2.20848349\n",
      " 1.68223335 1.76306847 0.31337007 2.09600562 2.75738686 1.63030452\n",
      " 2.28550541 2.25229424 1.79767859 2.13178259 1.56574839 2.33870456\n",
      " 2.09264088 1.57889539 2.06689885 0.94753054 2.08189839 2.41175234\n",
      " 1.71836117 1.63034078 2.59195963 2.0019277  2.43372807 1.53297117\n",
      " 2.10494861 1.53071226 2.55643073 0.30094215 0.6138281  1.674247\n",
      " 0.73133513 1.987984   1.87712985 0.29142401 0.46409975 0.75485442\n",
      " 2.00559747 1.88641855 2.34798175 2.51745284 2.11492991 2.03715482\n",
      " 2.50705945 2.30897525 1.26230897 1.33985427 0.49346888 1.15879835\n",
      " 0.92288695 1.82165349 2.56299007 1.76336566 1.68271095 1.67737231\n",
      " 1.81906894 1.08319017 1.89645761 2.22449261 1.00154239 0.86175674\n",
      " 2.43266258 1.78622937 1.34413971 2.24648264 0.93684551 0.69867017\n",
      " 2.248656   2.18141574 1.410955   0.02159109 2.35603261 1.71652934\n",
      " 1.65439227 2.39552987 1.69933024 1.62313491 1.8509739  0.41348961\n",
      " 0.78734501 2.05015787 2.26726967 2.32620025 2.2668471  0.63482773\n",
      " 1.99486557 2.40144032 1.36354317 1.4722873  2.0348967  1.66569278\n",
      " 1.59430945 0.72333369 1.39707129 2.28293756 0.45070308 2.12493753\n",
      " 1.6827805  1.84013802 1.86418316 0.62386233 2.22219312 1.92183068\n",
      " 1.58362833 1.88605735 2.18343446 2.50618815 2.24020624 1.49769029\n",
      " 0.69477871 2.11719269 1.74569201 0.74188055 2.5974319  1.00756116\n",
      " 0.52856429 2.02248511 2.18652749 0.72233053 2.17758071 1.64403841\n",
      " 1.97495911 2.09755638 1.42880474 1.54462111 0.83720648 0.52792411\n",
      " 0.57955842 1.12654485 1.31860732 1.84473082 0.29736792 2.35743409\n",
      " 2.36460173 1.69527212 2.36949375 1.29461901 1.83817986 2.41775918\n",
      " 0.8418732  2.41909644 0.74171343 2.4146125  1.03078872 2.38301057\n",
      " 1.41974498 2.15051329 1.11425449 0.78692921 1.76935005 2.26380426\n",
      " 0.65319435 1.98020467 1.43560697 1.15508991]\n"
     ]
    }
   ],
   "source": [
    "print(xgb_model.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.20189137  2.6735855   2.31471881  2.03151571  1.88154426  2.33420868\n",
      "  1.60699657  1.70868044  0.47305051  1.22841702  2.15219131  1.88634951\n",
      "  2.27246161  2.28916318  0.67889841  1.2394547   2.00302079  1.94537443\n",
      "  2.74311558  2.00324798  1.86568521  1.83117434  2.06407341  2.37625872\n",
      "  1.30516165  1.11622325  2.44510668  1.49546185  2.00161479  2.34044586\n",
      "  0.77831549  1.53593449  0.48143955  1.25711835  2.58569671  2.22124543\n",
      "  0.47403192  1.70022837  2.02206333  0.81493304  1.78198376  1.92777828\n",
      "  0.58759581  1.46129796  2.12835291  2.49592089  2.17945537  1.15348355\n",
      "  2.32084809  2.38848053 -0.07269071  0.85539306  2.5118389   2.39654613\n",
      "  2.2384093   1.30695171  0.17080599  2.05641027  1.71990037  0.79957183\n",
      "  1.91246629  1.96119547  1.15629654  2.0506857   1.48357514  2.39298152\n",
      "  2.22213924  2.1261792   1.2434744   0.39907874  1.83502617  2.32720062\n",
      "  1.2970673   2.00861618  2.38041389  2.31206187  1.67717082  2.41914813\n",
      "  1.99779868  1.37195646  1.87859652  1.53165203  2.4674723   2.56048277\n",
      "  2.00297505  2.37823944  2.44747231  0.76725455  2.74275307  2.221869\n",
      "  2.25313367  1.56763448  1.95424973  0.9110893  -0.10625979  2.32656183\n",
      "  2.2799258   1.53667692  0.95960957  1.85741051  2.27408376  0.28486157\n",
      "  2.23641366  1.82699932  0.52146143  2.67176643  1.83502617  1.9527248\n",
      "  2.17970285  2.32166688  1.63708724  2.76981296  1.65697172  2.01096112\n",
      "  1.61572555  1.82192029  1.50508712  2.19056806  2.14165393  1.38531086\n",
      "  2.43060429  1.9230682   1.50870309  2.21436406  2.49638222  0.66200201\n",
      "  2.55484353  2.02572298  2.02839793  2.17806156  2.16837996  1.32239767\n",
      "  1.63683109  1.83502617  2.06912172  2.05407757  2.67940067  0.59534845\n",
      "  1.42274394  2.04606665  2.33365343  2.39040093  0.98017818  0.19066726\n",
      "  2.13316273  2.40013854  1.11118675  1.98436577  1.8195385   1.73964579\n",
      "  1.69494335  1.01050622  2.08251018  0.14541811  1.86563861  1.11028622\n",
      "  2.15021186  0.26214832  1.92070251  2.53821028  1.76402384  0.38727521\n",
      "  2.73059936  2.71821026  0.8275441   2.11099007  2.32921966  1.22401849\n",
      "  2.47627089  1.34001789  1.67162814  1.65766665  2.49304558  2.68056393\n",
      "  2.03791352  2.26586536  1.85556479  2.45857777  0.67932617  0.42928267\n",
      "  2.14251118  1.18517675  1.22108224  0.51751006  2.10872967  1.23960104\n",
      "  2.52462341  1.96255147  2.35646318  2.32488598  2.87563863  1.64787191\n",
      "  0.75955539  0.56348685  1.76399833  2.72014635  1.41129798  0.8172811\n",
      "  1.41147205  2.32286122  2.20309443  1.83636336  0.62999676  0.08344023\n",
      "  2.69571037  0.66810619  2.14447257  0.03167298  1.66791491  1.82222382\n",
      "  2.28961328  1.47472057  0.67720485  2.58743101  2.26561044  1.66048981\n",
      "  1.93073361  1.53449037  0.01850754  0.17555422  1.67544643  2.32829653\n",
      "  2.49151192  0.23966637  0.63198211  2.31989506  2.22214197  0.86497746\n",
      "  2.0255011   1.54464772  0.83924155  2.25781399  2.15126603  2.63537517\n",
      "  2.22477492  2.27662054  1.36361747  1.43739292  1.82279501  1.75289258\n",
      "  1.86524657  1.13288395  2.38004279  1.96051462  2.2419402   2.31026963\n",
      "  2.13685023  2.35206742  0.79967928  0.56299742  2.258267    2.13233546\n",
      "  2.24941043  1.67751703  2.73826222  2.09766852  1.45750123  2.27721268\n",
      "  2.44423808  2.50965906  1.31347607  2.3543781   2.17199481  1.10269708\n",
      "  1.77057185  2.22129319  1.96259754  1.94180068  0.20961305  1.44766929\n",
      "  0.4809831   1.9065746   2.03515123  0.31169498  1.05912265  1.99779777\n",
      "  0.73177898  1.08358517  2.37022695  1.77304499  2.35374482  2.10977226\n",
      "  0.83565568  1.89947796  2.02662671  1.85504547  1.84353189  0.17618042\n",
      "  2.25529088  2.38175471  2.17299547  2.25529088  1.60005394  1.43359787\n",
      "  2.16724569  2.45028885  1.46651912  2.22663569  2.16646798  2.56061513\n",
      " -0.00362738  2.12831733  0.8786393   0.94393341  2.32250068  1.49418386\n",
      "  1.84637976  2.28738595  2.09283055  2.25490369  2.20106629  1.79200107\n",
      "  1.99174355  1.26512804  0.95939741  2.47614997  2.06625827  1.88302734\n",
      "  0.57644479  2.20552139  2.44999865  2.02205782  2.55929383  2.25879299\n",
      "  2.04072435  2.1611695   2.42807781  1.4569797   1.83502617  1.80260224\n",
      "  1.52396789  0.67905459  0.49559122  2.24389849  1.84478928  2.10076351\n",
      "  2.22618142  1.35439171  1.89731119  0.54795756  2.40277828  1.77368533\n",
      "  1.37732985  1.51433069  0.6530217   2.13658363  2.24402131  1.36505146\n",
      "  1.86267957  2.11663731  2.15209855  0.2119112   2.3185773   2.23611456\n",
      "  2.48370806  2.0901264   2.54362828  2.10895309  0.70441466  2.80572196\n",
      "  1.75119187  2.21160895  2.03467426  2.36990934  2.02197295  2.25295671\n",
      "  2.59932747  2.34132851  2.37490137  2.50837645  2.12749473  1.64399723\n",
      "  1.44693444  2.34870602  1.72851954  1.74203221  2.06610748  1.29530199\n",
      "  2.22611601  2.15758153  2.45498717  2.52915228  2.02359628  0.73383895\n",
      "  1.78184763  0.40395706  2.25490369  1.63256274  1.51161407  2.34340475\n",
      "  1.46027251  1.86532018  1.4324845   2.48017331  2.22289544  1.36841317\n",
      "  2.15098706  1.70159948  0.43075868  2.29575015  2.28329021  2.49387273\n",
      "  1.41826491  1.89767668  1.97335637  1.31924176  2.02470767  2.33040654\n",
      "  2.0769211   2.15827748  1.60272292  1.19506383  2.4905749   1.87457526\n",
      "  2.89403427  1.97951303  0.5465907   2.42345698  0.48717491  0.14505883\n",
      "  1.34680194  1.12245752  2.19727913  1.71577597  0.43567502  2.28301345\n",
      "  2.69626199  2.37158789  1.75439065  1.38644163  2.07605886  0.41011826\n",
      "  2.46020985  0.30838374  2.38514263  1.77732618  2.1461782   0.96880828\n",
      "  0.57656429  1.87402746  2.32777201  0.64305823  1.72940449  2.24396214\n",
      "  0.74806209  0.79998524  0.67738262  2.06332545  2.35017945  0.75219768\n",
      "  0.93372266  2.05199281  2.60958942  2.21733218  2.26450186  1.79827977\n",
      "  2.63237976  1.94085274  1.57438201  1.94031138  1.58019634  0.86785551\n",
      "  2.5888563   2.1657809   2.54504851  2.15978511  1.01316634  2.09166427\n",
      "  1.29948287  2.25800431  2.17007449  2.59472502  2.36660275  2.28409974\n",
      "  1.92231585  2.15567486  1.50094517  2.64323921  1.75479447  0.62371942\n",
      "  2.02849825  1.92439728  2.50592854  1.85431669  1.76553917  0.91255447\n",
      "  1.55022869  2.24706943  0.29021527  2.39820089  1.09308016  2.20722456\n",
      "  1.12690043  2.35651286  2.286738    2.61828574  2.77063539  2.4723552\n",
      "  0.51758724  2.44880685  2.29094607  2.01330378  1.90209872  2.69896292\n",
      "  2.20719201  1.50992855  2.01317534  2.57137772  2.06602334  1.65019832\n",
      "  0.79035714  1.58211809  1.54365214  2.31865966  1.32506669  1.83990239\n",
      "  2.11650944  2.17418234  2.31639377  2.34069451  2.52997295  2.47440398\n",
      "  2.17237962  1.81444251  0.58560725  1.65485045  0.57878529  2.0938148\n",
      "  1.77675253  0.6211922   1.91886059  1.50021646  0.88957482  2.59393681\n",
      "  2.30910162  1.2199267   1.35586103  2.70934518  1.8710832   0.40632739\n",
      "  2.55098195  1.74415538  2.60701397  2.09429537  2.30156789  2.39976732\n",
      "  1.31333832  2.12749473  1.73778701  1.35130838  1.76525348  2.104075\n",
      "  1.93953059  1.17733579  2.19182753  1.96814007  0.771849    2.15787097\n",
      "  2.24215111  1.98917514  2.1018831   1.41826491  2.3394224   1.09345812\n",
      "  2.28410278  1.33161528  2.44095171  2.34012322  2.13686369  2.3240258\n",
      " -0.09226397  2.28384294  1.83543055  2.60736627  2.60259806  2.21440153\n",
      "  2.19400234  1.89297483  2.22765083  2.67715917  0.80500884  2.68588771\n",
      "  1.34807055  1.88760979  1.40037375  0.97683132  2.02928874  2.22856734\n",
      "  0.98017818  2.16918998  1.83502617  2.66988115  2.26251016  1.8757818\n",
      "  2.37498971  2.245492    2.65012369  1.96837915  1.25195375  2.16415391\n",
      "  2.32080648  0.45742835  0.93085083  1.49888058  1.66964376  1.20486253\n",
      "  0.73365799  2.29632159  1.95455245  2.09527329  1.28795888  1.83392321\n",
      "  2.42263309  1.59164837  2.09366712  2.6292208   2.29866524  2.28875324\n",
      "  0.74720389  0.67332494  2.35026858  0.61263973  2.2310117   1.26883115\n",
      "  1.82416042  2.47089907  1.10645999  1.26463605  1.76599937  2.13833692\n",
      "  2.06939378  2.37663629  2.46659689  0.90390539  2.1463073   0.98479857\n",
      "  1.75179516  2.01858991  1.79524679  2.43678974  0.73521597  1.47572483\n",
      "  2.14622692  1.58465418  2.2695596   2.0269287   1.73943779  0.4665078\n",
      "  2.1289477   2.37395704  1.69599709  1.76294557  2.03076403  0.98017818\n",
      "  2.49173209  2.84190686  2.49968596  0.48081676  2.23039017  2.31754649\n",
      "  1.44457081  0.48514828  1.03039494  2.12934953  0.47672072  1.38383512\n",
      "  1.83502617  1.51906617  1.77174749  2.40863631  2.57225124  2.12264753\n",
      "  2.04972969  2.00260787  1.3907458   2.24402131  2.11087033  0.1584013\n",
      "  2.20715739  0.81125516  0.97079579  1.03751477  1.61092573  1.96119547\n",
      "  1.9623209   2.09574066  0.25002465  1.41826491  2.40416259  1.60278939\n",
      "  0.92300254  0.95783251  2.52607481  1.78225784  2.27642734  1.78952036\n",
      "  0.50985548  1.43754991  2.3602645   2.18160175  0.67390188  2.63984521\n",
      "  1.91736027  2.43482861  1.78052784  2.05886477  2.37833609  1.87924472\n",
      "  2.55215682  2.29336657  2.24835749  2.19936312  1.04231082  2.37151747\n",
      "  1.59047412  2.06451487  1.91992547  2.01770379  2.51518928  2.19974637\n",
      "  2.21787487  2.39495455  1.71059577  1.28695204  2.02879509  2.34379022\n",
      "  1.86215341  2.54760716  0.45217663  2.35785521  0.97616033  1.77254663\n",
      "  0.88778604  0.0742745   0.193283    2.10355821  1.84030256  2.63944923\n",
      "  2.68818817  2.02185456  2.18555384  1.92439941  0.61976685  1.65671108\n",
      "  2.64869886  2.31749573  1.77887354  1.96746166  1.7477796   0.56454101\n",
      "  2.52528887  2.1709372   2.46321537  2.46399562  1.68939197  1.68422073\n",
      "  2.81019792  1.95868267  1.86145999  2.25529088  2.25973859  2.44641563\n",
      "  1.50413413  2.04405155  2.13796549  1.0451351   1.91403793  1.76421448\n",
      "  2.10699108  1.95411093  0.15651558  2.12505945  2.25529088  0.87868915\n",
      "  2.3906396   1.0968615   1.34805912  1.93510989  1.47881697  2.58922713\n",
      "  0.55586923  0.36167928  2.25047617  2.26515062  1.55990707  1.21082189\n",
      "  2.25529088  2.23567658  2.08947992  2.2550877   0.68260351  2.02224543\n",
      "  2.8029054   1.62973324  2.02944948  1.7302629   2.10217424  1.86380423\n",
      "  0.39797403  1.70443247  1.17827605  2.0703382   1.56377674  1.80754038\n",
      "  0.39829052  1.8540311   1.60819009  1.02699684  1.59159106  1.73987382\n",
      "  0.76652624  0.45259377  2.59100609  1.91899798  2.69639725  2.25529088\n",
      "  2.12234545  2.01549636  1.59648     2.60291468  1.83851889  2.32442772\n",
      "  1.76427003  1.82424092  0.59026504  2.59108503  1.332485    2.16071265\n",
      "  1.18271727  2.10561871  0.68194605  2.11926933  2.51465554  1.71089105\n",
      "  1.65645209  2.55778278  1.05907554  2.00324798  2.24898871  2.19764676\n",
      "  2.35765746  1.17117068  1.9751863   0.77242542  1.73096758  2.10076351\n",
      "  2.0711719   2.3104964   0.41903845  2.13052268  2.67726718  1.75847205\n",
      "  2.37950038  2.65143931  1.9484798   2.04991966  1.60249069  2.33612604\n",
      "  2.0858798   1.69586625  1.92519897  0.91530788  2.33206502  2.45764605\n",
      "  1.56117944  1.54466173  2.6632435   2.18576122  2.40701842  2.22481074\n",
      "  2.33428111  1.83950617  2.6011369   0.57768243  0.56393149  1.65742791\n",
      "  0.87251392  1.88461752  1.87708406  0.46357278  0.07222914  0.30409721\n",
      "  2.22516941  2.13943725  2.39759174  2.69976939  2.0493823   2.09249056\n",
      "  2.54869143  2.23832659  1.16066098  1.83502617  0.64191085  1.84945022\n",
      "  1.22331605  2.11799297  2.73588349  1.9540238   1.60665921  2.09414851\n",
      "  2.05767756  1.37397733  2.2228008   2.29740871  0.93116055  1.19672931\n",
      "  2.55604298  1.876618    1.5333151   2.235152    0.94068341  0.79473978\n",
      "  2.3746299   2.37194221  1.6927811   0.27416055  2.2360031   1.99999503\n",
      "  1.75267589  2.43603067  1.72433023  1.60259314  1.76620156  0.16157257\n",
      "  0.75914707  2.27282327  2.22637067  2.62416429  2.28365848  0.86440433\n",
      "  2.24395429  2.20505152  1.51323133  1.66873228  2.25074724  1.59786591\n",
      "  1.85524203  0.87091905  1.35635473  2.48116457  0.39557608  2.24402131\n",
      "  1.70754188  2.2113801   2.20370999  0.81161224  2.28266214  2.22842363\n",
      "  1.69625843  2.0336118   2.75385042  2.59529519  2.18479494  0.8816205\n",
      "  0.64352653  2.47967494  1.58553387  1.10521394  2.54854034  1.18069015\n",
      "  0.45795856  1.94761891  2.28076259  0.99000824  2.32950436  1.8606316\n",
      "  2.15538564  2.15520529  1.20823603  1.65556847  0.43803203  0.55929784\n",
      "  0.85245578  1.03714494  1.23596891  2.00200253  0.46358497  2.3720513\n",
      "  2.25224021  1.83032899  2.39453918  1.24699115  1.9065745   2.62742488\n",
      "  0.63373348  2.29031948  1.23838007  2.30820367  1.03115062  2.40068712\n",
      "  1.85078749  2.03416095  1.04564782  0.96795851  1.52947444  2.40729674\n",
      "  0.55464392  2.18897376  1.50893713  1.40647297]\n"
     ]
    }
   ],
   "source": [
    "print(cat_model.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.53581542 2.68260652 2.39942941 2.54262412 2.34701592 2.71188259\n",
      " 2.0958434  2.1693055  0.27966329 0.81915407 2.14865151 2.3891567\n",
      " 2.48634079 2.47660705 0.49202851 1.86244789 1.94541653 2.3787846\n",
      " 2.6181457  2.09991795 2.2596972  2.55060297 2.23537144 2.34398511\n",
      " 1.22973461 0.67010719 2.51872033 1.02701327 2.15925172 2.35379136\n",
      " 0.1848483  2.31946498 0.27875059 1.00429431 2.54766425 2.27015778\n",
      " 0.406938   1.82956335 2.06385541 1.46080607 1.83402178 1.95142993\n",
      " 1.26859593 1.4982295  2.45895508 2.67458826 1.18677862 1.30833939\n",
      " 2.42422011 2.58792108 0.2290515  1.34024093 2.57795304 2.58120748\n",
      " 2.47631475 1.12490935 0.36670125 2.50663733 1.50460322 0.97321814\n",
      " 2.29434267 2.10290483 0.82124987 1.94599056 0.68049467 2.65677673\n",
      " 2.31533232 2.15583199 1.96149838 0.68468684 2.04917997 2.53594404\n",
      " 1.30041417 0.22075969 2.43322697 2.53648323 2.17145237 2.53627521\n",
      " 2.14693114 1.53825212 0.89201113 1.99323159 2.43903989 2.50739124\n",
      " 1.86673936 2.44109395 2.31928283 0.28169324 2.64013284 2.41284806\n",
      " 2.47874647 2.27933121 2.2776933  1.49744719 0.03296589 2.14182934\n",
      " 2.53214878 1.88501087 0.76616976 2.16669694 2.2910248  0.24074633\n",
      " 2.5063282  1.89250708 0.80802027 2.61241972 2.04917997 2.29147229\n",
      " 2.57640362 2.56333011 1.57609065 2.65300685 2.05710107 2.43608895\n",
      " 1.80333486 1.84237835 2.20029703 2.60791624 2.33008528 1.33551831\n",
      " 2.6101563  2.56833285 0.72640564 2.44633031 2.69770736 0.52506014\n",
      " 2.55834699 2.15339366 2.33262512 2.22325686 2.42843115 1.54413816\n",
      " 1.28639106 2.04917997 2.31227183 2.41489473 2.69244868 0.7021245\n",
      " 0.99580272 2.19564375 2.26146376 2.29964727 0.74475239 0.43460462\n",
      " 2.49159375 2.47386298 1.06510061 2.49495947 1.93654189 2.18771046\n",
      " 1.85006759 1.22329646 1.7569657  0.48028651 2.14528668 0.5412567\n",
      " 2.65960538 0.87147318 1.95530862 2.68383592 2.457111   0.37087254\n",
      " 2.62680757 2.67230773 0.64794179 2.51060185 2.50719154 1.61370973\n",
      " 2.59434772 0.97704051 1.77344969 2.1794388  2.19862884 2.65926063\n",
      " 2.36242613 2.24660033 1.40079679 2.59108615 0.79071771 0.43853258\n",
      " 2.62886679 1.33733672 1.29307853 1.31019013 1.57759279 1.33196551\n",
      " 2.66649157 1.69989571 2.66550422 2.15009993 2.67371333 1.72072138\n",
      " 0.63622609 0.67156028 2.05513769 2.63368595 1.9848403  0.58840524\n",
      " 1.37993243 2.4374207  2.65518397 2.35597509 0.66408155 0.19944634\n",
      " 2.64659047 0.19006465 2.6261977  0.00655082 2.17499441 1.97715482\n",
      " 2.32843763 1.38036323 0.3948543  2.65961957 2.54405832 2.28662962\n",
      " 2.09223384 1.86347377 0.27221681 0.50715372 2.24700457 2.35003245\n",
      " 2.44950861 0.55763646 0.29542742 2.22058809 2.55324861 0.68479439\n",
      " 2.47681114 1.97332877 1.49873918 2.50897378 2.39505458 2.60265914\n",
      " 2.40574056 2.49842906 1.68361004 1.28654374 2.3359189  1.85126877\n",
      " 1.31883915 0.99384704 2.54977959 1.40194698 2.44264674 2.6468181\n",
      " 2.05222952 2.31736353 0.38278772 0.33808017 2.49220368 1.54870425\n",
      " 2.2541393  0.7456862  2.65022105 2.48240474 1.04386497 2.33248273\n",
      " 2.44329071 2.3603555  0.82128048 2.6036579  2.34592003 1.56221539\n",
      " 1.96235254 2.27306911 2.11797488 2.18815449 0.57177029 1.26265562\n",
      " 0.47674765 2.68497998 2.11758938 0.5641882  0.90868951 2.12409848\n",
      " 0.33234235 0.91763061 2.46856135 2.19150066 2.67438412 2.34870163\n",
      " 1.67107935 1.86247924 1.44552623 2.31942335 2.11676687 0.54686888\n",
      " 2.33495092 2.35803571 2.3338168  2.33495092 1.52168031 0.91957908\n",
      " 2.11132994 2.28656566 2.16931736 2.14577416 2.19199294 2.63794386\n",
      " 0.278999   2.62843382 1.5506901  0.81358674 2.12920406 1.70716462\n",
      " 2.00550729 2.58204687 2.28990835 2.21890432 2.11928019 2.10397896\n",
      " 2.11145699 1.0415847  2.21546742 2.67116511 2.59334838 2.12554878\n",
      " 0.46287342 2.56787539 2.51322305 2.0116533  2.56982455 2.24616608\n",
      " 2.3677687  2.54308012 2.51442796 1.50693488 2.04917997 2.17855984\n",
      " 2.32480356 0.94267097 0.5721458  2.5998525  1.58428206 2.24898809\n",
      " 2.45403028 2.36320603 1.31901711 0.52328838 1.95100357 2.45523232\n",
      " 1.47554058 0.48487332 0.60707282 2.42910904 2.27279991 0.64640914\n",
      " 1.93767816 2.48964489 2.37300694 0.49425989 2.19404343 2.27802032\n",
      " 2.65722251 2.52799171 2.68337697 2.59719491 0.88657141 2.66125464\n",
      " 1.44125427 2.61508697 1.85691211 2.37894654 2.39965254 2.39307433\n",
      " 2.63775915 2.39072025 2.2992608  2.43381652 2.1395185  1.98772508\n",
      " 2.34963796 2.54155657 1.82652223 2.15983915 2.29961035 0.90495612\n",
      " 2.26969841 2.13875163 2.69620347 2.68899804 2.18125737 0.42114819\n",
      " 2.22077194 0.25775857 2.21890432 0.88794169 1.82616714 2.42434171\n",
      " 2.20933974 2.03594032 1.41498107 2.5012911  2.24516341 0.63771686\n",
      " 2.44860786 1.80057105 0.0448027  2.61320758 2.44122493 2.66663647\n",
      " 1.30739611 1.84802729 2.19548109 1.90703598 0.7278166  2.57404548\n",
      " 1.38052663 2.06411815 1.99250957 1.98504218 2.66405922 2.23501164\n",
      " 2.66232777 1.3825397  0.54134808 2.57927418 0.5043838  0.18107324\n",
      " 1.65927432 1.05318183 2.37549612 2.20115319 0.52555991 2.56983259\n",
      " 2.63446724 2.68961853 1.43214346 2.11686239 2.56362316 0.19545418\n",
      " 2.67441362 0.49610715 2.39422581 1.93981943 2.31096905 1.79744422\n",
      " 0.6693427  2.00109303 2.33097178 1.22416076 1.36162707 2.09341416\n",
      " 0.31737148 0.62040442 0.53547267 2.59376797 2.63403594 0.68481015\n",
      " 1.05685332 2.34439451 2.6241042  2.29773441 2.36757782 1.5200785\n",
      " 2.6519556  2.23773155 1.32696196 1.61146358 1.7606103  0.37470181\n",
      " 2.58740953 2.63048261 2.672961   2.45165092 1.18544075 2.37940326\n",
      " 0.41928022 2.32719597 2.46527389 2.62764823 2.58783054 2.33215249\n",
      " 1.88410199 2.40026152 1.53884548 2.62866992 1.24180042 2.09332111\n",
      " 2.25891924 2.21184433 2.39433223 2.14246285 1.69846523 1.54707003\n",
      " 2.54706869 2.5907864  0.04492571 2.44602719 1.26959941 2.31593338\n",
      " 1.01696022 2.33825135 2.19733784 2.58144438 2.65591443 2.47809216\n",
      " 0.83368279 2.59954047 2.6348744  2.63925129 1.94575888 2.58182746\n",
      " 1.91527358 1.43250219 2.3015978  2.67891985 2.38131052 0.33091383\n",
      " 0.45755824 0.77618786 2.11639595 2.35923618 0.3154777  2.07345402\n",
      " 2.58241761 2.40667126 2.241056   2.63179255 2.70201111 2.50554344\n",
      " 2.26272434 2.17532447 0.23567541 1.09725122 0.38577895 2.61333883\n",
      " 1.98543307 0.3390958  2.54295513 0.87689356 0.72205051 2.59497267\n",
      " 2.38058814 1.00026436 2.25521073 2.62216622 1.99623182 0.87564699\n",
      " 2.65903753 1.00355156 2.62730879 2.37787658 2.6543321  2.22617728\n",
      " 0.533678   2.1395185  1.77427679 1.15587626 1.63136904 2.48854691\n",
      " 2.29607543 0.40501357 2.32653639 1.91484672 0.67175857 2.62256175\n",
      " 2.42142746 1.71044958 2.48618123 1.30739611 2.62947065 0.21497697\n",
      " 2.61169499 1.52960983 2.4982644  2.35698721 2.48210675 2.53803384\n",
      " 0.58538803 2.65443349 2.22732121 2.64136422 2.58341986 2.38551289\n",
      " 1.92457408 2.26647404 2.21656188 2.64845443 0.28683393 2.6505965\n",
      " 0.99146207 2.33555737 1.37073493 0.68461357 2.28488296 1.766761\n",
      " 0.74475239 2.28934127 2.04917997 2.64632106 2.37622288 2.02614978\n",
      " 2.61355692 1.99700719 2.53967509 1.71521875 1.59817635 2.53399414\n",
      " 2.58482736 0.39657232 0.88593926 1.48255312 1.73624921 1.5620392\n",
      " 1.82709533 2.59506392 1.92164075 2.17065543 1.75441578 2.04685757\n",
      " 2.55407068 0.92801322 2.32511488 2.41151935 2.64707142 2.50423458\n",
      " 0.53581125 0.63580319 2.33158976 0.32330985 2.62904084 1.15940389\n",
      " 1.81125015 2.55048418 0.20032831 0.8099138  1.58752297 2.34122628\n",
      " 2.49692225 2.66792625 2.5314194  0.96240346 2.58052361 0.79322262\n",
      " 0.9644718  1.95580414 2.13763988 2.68723416 0.75670549 1.54004949\n",
      " 2.00595292 1.88586509 2.55667406 1.81975099 2.16786116 0.93895605\n",
      " 2.57673216 2.45754313 1.68353254 0.85972811 2.260818   0.74475239\n",
      " 2.65234095 2.6721788  2.6308462  0.42259712 2.26506269 2.44179571\n",
      " 2.27872881 0.73383948 0.98639645 2.59959769 0.28011557 2.13975787\n",
      " 2.04917997 0.50139438 0.93040957 2.63257951 2.62716347 2.65694505\n",
      " 1.79831079 2.43574268 2.22073945 2.27279991 1.88176912 0.14738296\n",
      " 2.46815544 1.58468005 0.53201064 1.48872361 2.16435409 2.10290483\n",
      " 2.08573559 1.80013442 0.51733293 1.30739611 2.55217439 2.30451283\n",
      " 0.517253   1.50263998 2.63479769 1.90548024 2.29003131 1.86859396\n",
      " 0.85148898 2.14222395 2.35582319 2.27547881 1.18070662 2.61342102\n",
      " 2.31741363 2.62204903 1.96178931 2.2566807  2.57326269 2.14829311\n",
      " 2.62993675 2.25903428 2.58523935 2.31842571 1.82836083 2.37772518\n",
      " 1.51116346 2.28303593 0.54698025 0.63503745 2.43857047 2.6079241\n",
      " 2.43410674 2.66240561 1.91017237 1.50628816 2.48872855 2.38586855\n",
      " 2.08860993 2.57896072 0.42912901 2.40377772 0.74309448 1.97140086\n",
      " 1.62579417 0.4232965  0.36779547 2.12787819 1.13215154 2.66182917\n",
      " 2.57058418 2.4964841  2.55614209 2.34884125 0.61283851 1.95946959\n",
      " 2.6525138  2.40160951 1.67457014 2.09388775 2.25937521 0.50051094\n",
      " 2.57683766 2.21155438 2.65923256 2.36619303 2.06844679 1.9351635\n",
      " 2.64153469 2.10560873 1.91606531 2.33495092 2.58566701 2.43023106\n",
      " 2.15833378 2.0683901  2.08798689 1.45163034 1.9882189  2.15717554\n",
      " 2.36925536 2.25919798 0.38382195 2.49711239 2.33495092 0.31074904\n",
      " 2.5682767  0.65487674 1.09936464 1.61294407 2.04030299 2.54217744\n",
      " 0.6988579  0.22231173 2.48081478 2.40639064 1.83776355 0.7640637\n",
      " 2.33495092 2.51072419 2.35097301 2.535788   0.38063041 2.36721879\n",
      " 2.65438247 1.25637361 2.45349199 2.26892179 2.46438828 1.19549366\n",
      " 0.43197694 2.31358346 0.52708287 1.76071177 1.3816198  2.55869296\n",
      " 0.19245916 2.49109244 1.6953072  0.88253638 1.75730461 2.23765734\n",
      " 0.5506753  0.36893088 2.63142121 1.80438673 2.66320395 2.33495092\n",
      " 2.51586652 2.15119001 1.11024092 2.63053924 1.86439571 2.31968409\n",
      " 1.722242   1.86750519 0.56126466 2.67213875 0.38213837 2.39324194\n",
      " 1.64845413 2.57991683 1.40321022 2.4189038  2.52798972 1.66349843\n",
      " 2.03229874 2.54582584 0.86706404 2.09991795 2.5470168  2.28730908\n",
      " 2.61026973 2.01101711 2.23456478 2.01469453 1.77773908 2.24898809\n",
      " 2.18208337 2.55795053 1.04675575 2.20173076 2.57317016 2.29822481\n",
      " 2.52079096 2.68590987 1.58786353 2.45945543 0.93156816 2.66932797\n",
      " 2.54816732 1.0660858  2.53127062 0.68720862 2.59080285 2.20801461\n",
      " 2.1746943  1.60606434 2.68711138 2.38240302 2.24108362 2.47620267\n",
      " 2.59684086 1.98151729 2.58454257 0.30009814 0.58202049 0.81870596\n",
      " 0.22767321 2.12201542 2.09854478 0.69534937 0.37446177 0.65093648\n",
      " 2.40958449 2.18619519 2.51489502 2.70084214 1.91598508 2.55586317\n",
      " 2.53017506 2.35642576 1.37237144 2.04917997 1.26795091 1.77483007\n",
      " 0.75804381 2.34284401 2.4490931  2.25996691 1.56993002 0.86299604\n",
      " 1.17523548 1.7448045  2.308837   2.63180852 1.42684416 2.30518046\n",
      " 2.64427972 2.25516719 1.9811939  2.3494038  0.78794431 0.5282267\n",
      " 2.67080349 2.40915939 2.14472744 0.60580784 2.47011009 2.37249833\n",
      " 1.88645169 2.63621837 2.24396288 1.88202628 2.26480451 0.31508486\n",
      " 0.4069155  2.29089364 2.41972372 2.55914456 2.171841   1.14664686\n",
      " 2.55753952 2.28788793 2.31749332 1.40305531 2.33388907 2.51610607\n",
      " 1.36878875 0.88887693 0.4585002  2.54084876 0.66946384 2.27279991\n",
      " 1.45994425 2.48171759 2.17674413 0.73330269 2.43790337 2.20190883\n",
      " 1.42619756 2.4496173  2.68039817 2.67231029 2.12814346 1.27636436\n",
      " 0.54069667 2.6511929  2.04920766 0.63676704 2.68477702 0.66894602\n",
      " 0.75239655 2.14858091 2.43253267 0.98468328 2.63687223 1.42434512\n",
      " 2.54481494 2.33414799 0.66809072 2.4274278  0.26518613 0.01270603\n",
      " 0.46345274 0.697992   0.23724038 2.18549892 0.58445156 2.58181441\n",
      " 2.22777355 1.33073174 2.68365771 1.55308795 2.40435359 2.57639354\n",
      " 1.22718148 2.31498471 1.78141415 2.39793715 1.92251042 2.62081259\n",
      " 1.83790547 2.14355034 1.20421584 0.35543651 1.46235169 2.61468101\n",
      " 1.10161938 2.54240617 1.55580838 2.00807291]\n"
     ]
    }
   ],
   "source": [
    "print(nn_model.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.9830\n",
      "rmse: 0.9939\n",
      "rmse: 0.9873\n",
      "rmse: 0.9978\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "# pred_train_1a, pred_train_1b\n",
    "# pred_test_1a, pred_test_1b\n",
    "model_1a = lgb_model\n",
    "pred_train_1a, pred_test_1a = predict_cv(model_1a, train_x, train_y, reduce_test)\n",
    "model_1b = xgb_model\n",
    "pred_train_1b, pred_test_1b = predict_cv(model_1b, train_x, train_y, reduce_test)\n",
    "model_1c = cat_model\n",
    "pred_train_1c, pred_test_1c = predict_cv(model_1c, train_x, train_y, reduce_test)\n",
    "model_1d = nn_model\n",
    "pred_train_1d, pred_test_1d = predict_cv(model_1d, train_x, train_y, reduce_test)\n",
    "\n",
    "# 1\n",
    "print(f'rmse: {np.sqrt(mean_squared_error(train_y, pred_train_1a)):.4f}')\n",
    "print(f'rmse: {np.sqrt(mean_squared_error(train_y, pred_train_1b)):.4f}')\n",
    "print(f'rmse: {np.sqrt(mean_squared_error(train_y, pred_train_1c)):.4f}')\n",
    "print(f'rmse: {np.sqrt(mean_squared_error(train_y, pred_train_1d)):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "train_x_2 = pd.DataFrame({'pred_1a': pred_train_1a, 'pred_1b': pred_train_1b,'pred_1c': pred_train_1c,'pred_1d': pred_train_1d})\n",
    "test_x_2 = pd.DataFrame({'pred_1a': pred_test_1a, 'pred_1b': pred_test_1b,'pred_1c': pred_test_1c,'pred_1d': pred_test_1d})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# \n",
    "def predict_cv2(model, train_x, train_y, test_x):\n",
    "    preds = []\n",
    "    preds_test = []\n",
    "    va_idxes = []\n",
    "\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "\n",
    "    # \n",
    "    for i, (tr_idx, va_idx) in enumerate(kf.split(train_x)):\n",
    "        tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "        model.fit(tr_x, tr_y, va_x, va_y)\n",
    "        pred = model.predict(va_x)\n",
    "        preds.append(pred)\n",
    "        pred_test = model.predict(test_x)\n",
    "        preds_test.append(pred_test)\n",
    "        va_idxes.append(va_idx)\n",
    "\n",
    "    # \n",
    "    va_idxes = np.concatenate(va_idxes)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    order = np.argsort(va_idxes)\n",
    "    pred_train = preds[order]\n",
    "\n",
    "    # \n",
    "    preds_test = np.mean(preds_test, axis=0)\n",
    "\n",
    "    return pred_train, preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.9772\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "# pred_train_22\n",
    "# pred_test_22\n",
    "model_2 = Model2Linear()\n",
    "pred_train_2, pred_test_2 = predict_cv2(model_2, train_x_2, train_y, test_x_2)\n",
    "print(f'rmse: {np.sqrt(mean_squared_error(train_y, pred_train_2)):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.32008398  2.68430172  2.3609019   2.15507401  2.0000299   2.41917811\n",
      "  1.7696084   1.87040538  0.34616545  0.8999541   2.15250489  1.96331916\n",
      "  2.26174575  2.42305231  0.68018587  1.49611855  2.07796646  2.0574205\n",
      "  2.73500438  2.01906682  2.13375122  1.97351896  2.18339104  2.38815013\n",
      "  1.27271871  0.84322876  2.36730907  1.30784614  2.00801638  2.42122856\n",
      "  0.30119689  1.92028178  0.32061479  1.13438483  2.56839376  2.16842384\n",
      "  0.59265492  1.62884313  1.99643554  1.08734117  1.76710302  1.88346807\n",
      "  0.90350011  1.33872412  2.24141714  2.53214821  1.85285311  1.12346248\n",
      "  2.35227282  2.45006362  0.17589056  1.23942271  2.58919712  2.47456106\n",
      "  2.19454811  1.16835487  0.25268802  2.14396637  1.48404501  0.90912712\n",
      "  2.02242664  1.94269111  1.05981864  2.02565909  1.05518835  2.45124692\n",
      "  2.22289526  2.25379195  1.48371894  0.40659706  1.83402703  2.4497675\n",
      "  1.32374824  1.40205568  2.39986072  2.38944808  1.85253424  2.42886011\n",
      "  2.05481368  1.38557585  1.58694305  1.68954508  2.43494381  2.56919565\n",
      "  1.9319749   2.31863028  2.29224055  0.41299875  2.67984732  2.33411853\n",
      "  2.40362995  1.8522374   1.96808181  1.18127099 -0.0553578   2.14005572\n",
      "  2.25110799  1.73339074  0.68848848  1.88406782  2.39714708  0.2131542\n",
      "  2.31366273  1.78905926  0.5522851   2.72459755  1.83402703  2.06616166\n",
      "  2.26264226  2.26987435  1.58789183  2.73417532  1.81232365  2.06640194\n",
      "  1.62570628  1.89051037  1.68997866  2.33593174  2.32271945  1.30827983\n",
      "  2.57758213  2.08906896  1.33507531  2.39637171  2.61223962  0.57576045\n",
      "  2.57663981  2.03525621  2.15399586  2.10448531  2.19311817  1.28850797\n",
      "  1.47890983  1.83402703  2.10655702  2.09123541  2.65052042  0.61385689\n",
      "  1.29896591  2.09254384  2.37801529  2.34748866  0.93316059  0.30663035\n",
      "  2.17559246  2.40271371  1.00491597  2.10033438  1.85524857  1.84324105\n",
      "  1.71071066  1.09901774  1.94094537  0.32967723  1.97330629  0.74746939\n",
      "  2.38914354  0.5024448   1.9086621   2.61747915  1.92186228  0.37789976\n",
      "  2.64697595  2.68166743  0.81250606  2.27933126  2.42103397  1.28069078\n",
      "  2.52692741  1.12707912  1.65366219  1.76308358  2.43393851  2.67217006\n",
      "  2.17748362  2.27780128  1.66355368  2.48438473  0.65742059  0.32972825\n",
      "  2.32342933  1.24996891  1.26093122  0.71985552  1.95250857  1.31208176\n",
      "  2.48266005  1.72780912  2.43603741  2.23402385  2.74649525  1.61983441\n",
      "  0.69915431  0.59529225  1.92738004  2.64072378  1.50005832  0.63030782\n",
      "  1.36467413  2.36017173  2.3835313   2.00835908  0.56466425  0.17585307\n",
      "  2.7153249   0.5129106   2.34831335  0.00678327  1.81709596  1.85762104\n",
      "  2.27478935  1.24226306  0.53343056  2.60477339  2.44848433  1.87856234\n",
      "  2.0002251   1.59791974  0.19275982  0.29275827  1.86111977  2.30678322\n",
      "  2.42959513  0.26142309  0.51108401  2.30311841  2.41809647  0.49405923\n",
      "  2.13578743  1.54441969  1.11401123  2.32555819  2.31692254  2.5704309\n",
      "  2.2813098   2.32908351  1.41727487  1.40361633  1.98676457  1.78296785\n",
      "  1.51744459  1.15967217  2.40062127  1.84827902  2.35917973  2.43738839\n",
      "  2.03712211  2.19464678  0.45787606  0.49959947  2.34342047  1.87280808\n",
      "  2.24662571  1.32064656  2.7445976   2.20415775  1.24789179  2.36657591\n",
      "  2.46795058  2.40228415  1.1281881   2.39676585  2.21202627  1.20191694\n",
      "  1.66886335  2.28748955  2.07126812  2.02088639  0.33249426  1.41194133\n",
      "  0.41554276  1.91333408  2.09258344  0.35607711  0.91990715  2.02302511\n",
      "  0.52811012  1.06022125  2.34798926  1.76404213  2.3113949   2.19821011\n",
      "  1.22408914  1.96903259  1.72524644  2.02741285  1.86170377  0.1996898\n",
      "  2.2907368   2.42813444  2.17216284  2.2907368   1.49919137  1.17087252\n",
      "  1.98648076  2.33154273  1.5974164   2.21891885  2.17311451  2.50464905\n",
      "  0.19673402  2.29241227  1.07050098  0.81434642  2.27458073  1.5164669\n",
      "  1.97303486  2.28281763  2.12497402  2.22465279  2.27041906  1.79142033\n",
      "  2.07092585  1.07014244  1.38533981  2.56834811  2.17003564  1.79343679\n",
      "  0.30712459  2.25618319  2.44179187  1.93302021  2.50198513  2.31806923\n",
      "  2.10124111  2.32584986  2.39030719  1.51170802  1.83402703  1.89833752\n",
      "  1.76750616  0.88616363  0.51273505  2.24646949  1.73508853  2.19784508\n",
      "  2.13318646  1.7472462   1.63501073  0.55186245  2.40310163  1.97213031\n",
      "  1.42909135  1.03241226  0.78186203  2.2506444   2.21757584  0.99163938\n",
      "  1.86171482  2.26176168  2.30760449  0.3096557   2.25390482  2.34799954\n",
      "  2.50716033  2.1435582   2.65956117  2.2578121   0.77070855  2.78425758\n",
      "  1.67250759  2.18235146  2.0081553   2.35718499  2.13031827  2.27286117\n",
      "  2.53846063  2.39231011  2.47946536  2.38672218  2.17110322  1.64455434\n",
      "  1.84153671  2.43849444  1.76702542  1.86180891  2.09208909  1.17646759\n",
      "  2.29140666  2.15248591  2.60387231  2.52222552  2.01955223  0.65306463\n",
      "  1.87949923  0.28187242  2.22465279  1.18517269  1.46703206  2.26389824\n",
      "  1.81840829  2.02627242  1.36216713  2.57886406  2.25239885  0.99287918\n",
      "  2.22624218  1.74033346  0.14931479  2.4159579   2.36682917  2.58441319\n",
      "  1.50189499  1.94273935  2.16688568  1.49810062  1.38880632  2.3997818\n",
      "  1.77224292  2.06857819  1.60800922  1.63827327  2.56028439  1.93852009\n",
      "  2.84873946  1.81446117  0.49762901  2.50027147  0.50801243  0.13814313\n",
      "  1.39411024  1.0967932   2.23269986  1.91713494  0.46680213  2.4155442\n",
      "  2.65950298  2.49109645  1.68015631  1.75738854  2.30649949  0.23577293\n",
      "  2.53692455  0.29748878  2.46385158  1.80523916  2.22361462  1.26722399\n",
      "  0.4533443   1.88620413  2.31397727  0.79838665  1.57704935  1.93808364\n",
      "  0.41872755  0.65281591  0.37519831  2.21511498  2.46843687  0.76076216\n",
      "  0.89517235  2.07008136  2.61907158  2.26160863  2.27140971  1.72114309\n",
      "  2.64819306  2.13005056  1.31296072  1.87584915  1.56603128  0.512669\n",
      "  2.5873078   2.37778026  2.53683803  2.12787829  1.05410253  2.20455133\n",
      "  0.85231796  2.3616187   2.29370853  2.59933276  2.39672529  2.37421861\n",
      "  1.8297884   2.10941505  1.50576033  2.61012766  1.46794031  1.15572847\n",
      "  2.25688121  2.05080349  2.28093926  2.00287771  1.77934298  1.19414669\n",
      "  2.01008843  2.27634422  0.16564477  2.3630409   1.01046309  2.35420878\n",
      "  1.04745149  2.39029749  1.95739507  2.65872399  2.77679326  2.47973142\n",
      "  0.54624239  2.48082048  2.44455948  2.36176647  1.82104267  2.54711878\n",
      "  2.05245321  1.35841948  2.10078485  2.57141877  2.2570351   1.11158993\n",
      "  0.64814635  1.2276644   1.66232307  2.4347801   0.95594449  1.64662985\n",
      "  2.25641925  2.19070414  2.29849154  2.3761561   2.57273702  2.51803145\n",
      "  2.30562466  1.95863972  0.50749409  1.41782311  0.50820099  2.24274189\n",
      "  1.74862463  0.47476465  1.9713651   1.33687976  0.81235888  2.55362367\n",
      "  2.26182379  1.24303108  1.59372569  2.69454719  1.90159646  0.5108893\n",
      "  2.61009964  1.47628923  2.44687114  2.09394638  2.33837253  2.39562334\n",
      "  0.95434435  2.17110322  1.58391667  1.17382959  1.7559804   2.22481762\n",
      "  2.00307593  0.66334688  2.05057915  1.92881162  0.59291213  2.31606664\n",
      "  2.32343974  1.78648742  2.20476444  1.50189499  2.43708436  0.83497146\n",
      "  2.4196556   1.35437526  2.51463486  2.40177358  2.28031876  2.38893451\n",
      "  0.18539095  2.46365251  1.96803262  2.67244667  2.57094691  2.23771212\n",
      "  2.10123885  1.97628459  2.27023485  2.63942572  0.55576754  2.66435073\n",
      "  1.21240164  2.16660935  1.26270603  0.68492489  2.16304632  1.96470364\n",
      "  0.93316059  2.1888602   1.83402703  2.69375286  2.2513734   1.86786616\n",
      "  2.51742299  2.12112204  2.58958199  1.90237545  1.57325542  2.29253296\n",
      "  2.32264337  0.48099432  0.84778324  1.45357323  1.58450498  1.36466499\n",
      "  1.17859776  2.4307414   1.92059218  2.09469727  1.39139974  1.95608265\n",
      "  2.51536467  1.24008561  2.13362696  2.54311082  2.33615174  2.3218561\n",
      "  0.62406217  0.48083668  2.44080464  0.4981129   2.30794846  1.13729358\n",
      "  1.73162149  2.37443094  0.70102787  1.14937186  1.56137554  2.23221648\n",
      "  2.24525184  2.51459339  2.55407561  0.94300206  2.22404075  0.99717045\n",
      "  1.51554182  1.91627335  1.89124997  2.54929787  0.86306446  1.47395501\n",
      "  2.06470036  1.56417454  2.27073676  1.94381031  1.93398734  0.56599773\n",
      "  2.275527    2.35439066  1.62130776  1.44819163  2.1622591   0.93316059\n",
      "  2.46696616  2.7926896   2.57533809  0.37760054  2.29551427  2.47710278\n",
      "  1.81185604  0.55009501  0.96456871  2.24173661  0.42853253  1.67890245\n",
      "  1.83402703  1.06967306  1.45905829  2.54688709  2.59266146  2.27821569\n",
      "  2.03249112  2.14094438  1.59525754  2.21757584  2.0998871   0.19984511\n",
      "  2.27122346  1.12969596  0.73144554  1.01086698  1.72054406  1.94269111\n",
      "  1.94118041  2.07682754  0.3014649   1.50189499  2.34727764  1.80434756\n",
      "  0.71229196  1.28460327  2.61060977  1.85265207  2.34729568  1.85693464\n",
      "  0.55985883  1.5687002   2.39258368  2.10710546  0.93680258  2.59807426\n",
      "  2.01503046  2.4885402   1.79664319  2.09723465  2.48029518  1.94248519\n",
      "  2.48509287  2.26734161  2.32247235  2.11855221  1.3910294   2.41687331\n",
      "  1.5886388   1.99621761  1.48396581  1.47606009  2.48693305  2.31411097\n",
      "  2.26217753  2.53721261  1.74319985  1.35364488  2.16177246  2.39184713\n",
      "  1.85983125  2.56591659  0.54150458  2.30165148  0.86166141  1.9052434\n",
      "  1.05870298  0.22939162  0.24825998  2.16874688  1.60301137  2.65353722\n",
      "  2.57276471  2.16523554  2.2815837   1.90299386  0.58766623  1.75509611\n",
      "  2.65752075  2.35094297  1.68217269  2.02298078  1.92865393  0.52807566\n",
      "  2.5399252   2.18334564  2.57305356  2.49693257  1.79420576  1.75750079\n",
      "  2.71085372  1.88277189  1.88269028  2.2907368   2.2582366   2.45462221\n",
      "  1.63928485  2.11062962  2.01138915  1.31760636  1.76424603  1.80303608\n",
      "  2.06834163  2.09961941  0.23964668  2.27434045  2.2907368   0.60405479\n",
      "  2.49281219  0.80547711  1.23333559  1.87834616  1.61072503  2.55589072\n",
      "  0.46853434  0.30899267  2.38321157  2.27704946  1.61595402  0.92765507\n",
      "  2.2907368   2.37605039  2.14726116  2.42181365  0.47887342  2.10431294\n",
      "  2.67175215  1.59276843  2.04107896  1.97926185  2.22744013  1.4251509\n",
      "  0.40948654  1.95990093  0.89156498  1.924232    1.23430319  1.94366264\n",
      "  0.19924238  1.97137954  1.72905703  0.99890976  1.62894101  1.85023785\n",
      "  0.48446987  0.4232129   2.61561185  1.8736829   2.65310353  2.2907368\n",
      "  2.15656677  2.19259139  1.44676372  2.6333945   1.78346212  2.42589952\n",
      "  1.71132262  1.74075963  0.5620415   2.66173869  0.93475577  2.09376942\n",
      "  1.32214499  2.22256219  0.89367938  2.26078851  2.53194138  1.59043871\n",
      "  1.69642122  2.54109151  1.07143904  2.01906682  2.41522179  2.34161091\n",
      "  2.44073     1.38860687  2.13122951  1.0424735   1.79079003  2.19784508\n",
      "  1.96004454  2.29764318  0.51796443  2.12636205  2.73795637  1.8780679\n",
      "  2.38158273  2.62886711  1.83672321  2.23086738  1.33904928  2.51074579\n",
      "  2.3138172   1.36470478  2.29533668  0.7548758   2.2976913   2.41941256\n",
      "  1.80388297  1.54657877  2.67106245  2.25678958  2.41054228  2.13337211\n",
      "  2.42017013  1.8613494   2.6079177   0.3489203   0.51832394  1.31525107\n",
      "  0.58983938  2.0319659   2.03127434  0.46418508  0.27542744  0.51906716\n",
      "  2.16339046  2.12662033  2.45172918  2.67655687  2.03632941  2.27798607\n",
      "  2.57366968  2.3719348   1.18125873  1.83402703  0.7612024   1.61299876\n",
      "  0.88089271  2.2140022   2.62073705  2.06963321  1.63414193  1.54466826\n",
      "  1.68981627  1.4927004   2.09184284  2.34500444  1.13064333  1.45340789\n",
      "  2.56527177  1.96460483  1.72108267  2.32316068  0.83701146  0.65760086\n",
      "  2.5022683   2.35758679  1.74332305  0.28141427  2.38692459  2.03051546\n",
      "  1.75985199  2.49539173  1.94380992  1.68841402  1.94742005  0.20951065\n",
      "  0.60313256  2.27667948  2.39725058  2.53659923  2.26374888  0.87001647\n",
      "  2.24886437  2.35250752  1.7465732   1.54846019  2.23819936  1.88347463\n",
      "  1.61387428  0.80694022  1.07836138  2.48558782  0.41412838  2.21757584\n",
      "  1.60434033  2.13627826  2.05363099  0.66308722  2.36209547  2.10492717\n",
      "  1.51834156  2.11597004  2.58367606  2.59207043  2.25635876  1.23312196\n",
      "  0.57340658  2.43768843  1.75878346  0.78375522  2.63157811  0.95858208\n",
      "  0.46929247  2.03926709  2.34122179  0.92006406  2.41685936  1.63688852\n",
      "  2.21946089  2.23658426  1.02053528  1.96478595  0.49146529  0.23091171\n",
      "  0.60830848  0.95671519  0.88542829  2.06904928  0.39149562  2.44822657\n",
      "  2.30593974  1.63355386  2.52274045  1.35295179  2.03412121  2.58490189\n",
      "  0.90559602  2.41084768  1.23336967  2.39058557  1.26500761  2.52666643\n",
      "  1.72666558  2.1490751   1.12563202  0.68317727  1.57138056  2.52061983\n",
      "  0.76353472  2.25333589  1.49824455  1.55813079]\n"
     ]
    }
   ],
   "source": [
    "print(pred_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.311407128171276, 1: 1.7485942434901263, 2: 1.9725825865666295}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_pred = pred_test_2\n",
    "dist = Counter(reduce_train['accuracy_group'])\n",
    "for k in dist:\n",
    "    dist[k] /= len(reduce_train)\n",
    "reduce_train['accuracy_group'].hist()\n",
    "\n",
    "acum = 0\n",
    "bound = {}\n",
    "for i in range(3):\n",
    "    acum += dist[i]\n",
    "    bound[i] = np.percentile(final_pred, acum * 100)\n",
    "print(bound)\n",
    "\n",
    "def classify(x):\n",
    "    if x <= bound[0]:\n",
    "        return 0\n",
    "    elif x <= bound[1]:\n",
    "        return 1\n",
    "    elif x <= bound[2]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "    \n",
    "final_pred = np.array(list(map(classify, final_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 3 3 3 2 2 0 0 3 2 3 3 0 1 3 3 3 3 3 3 3 3 0 0 3 0 3 3 0 2 0 0 3 3 0\n",
      " 1 3 0 2 2 0 1 3 3 2 0 3 3 0 0 3 3 3 0 0 3 1 0 3 2 0 3 0 3 3 3 1 0 2 3 1 1\n",
      " 3 3 2 3 3 1 1 1 3 3 2 3 3 0 3 3 3 2 2 0 0 3 3 1 0 2 3 0 3 2 0 3 2 3 3 3 1\n",
      " 3 2 3 1 2 1 3 3 0 3 3 1 3 3 0 3 3 3 3 3 0 1 2 3 3 3 0 0 3 3 3 0 0 3 3 0 3\n",
      " 2 2 1 0 2 0 3 0 3 0 2 3 2 0 3 3 0 3 3 0 3 0 1 2 3 3 3 3 1 3 0 0 3 0 0 0 2\n",
      " 1 3 1 3 3 3 1 0 0 2 3 1 0 1 3 3 3 0 0 3 0 3 0 2 2 3 0 0 3 3 2 3 1 0 0 2 3\n",
      " 3 0 0 3 3 0 3 1 0 3 3 3 3 3 1 1 3 2 1 0 3 2 3 3 3 3 0 0 3 2 3 1 3 3 0 3 3\n",
      " 3 0 3 3 0 1 3 3 3 0 1 0 2 3 0 0 3 0 0 3 2 3 3 0 2 1 3 2 0 3 3 3 3 1 0 3 3\n",
      " 1 3 3 3 0 3 0 0 3 1 3 3 3 3 3 2 3 0 1 3 3 2 0 3 3 2 3 3 3 3 3 1 2 2 2 0 0\n",
      " 3 1 3 3 1 1 0 3 2 1 0 0 3 3 0 2 3 3 0 3 3 3 3 3 3 0 3 1 3 3 3 3 3 3 3 3 3\n",
      " 3 1 2 3 2 2 3 0 3 3 3 3 3 0 2 0 3 0 1 3 2 3 1 3 3 0 3 1 0 3 3 3 1 2 3 1 1\n",
      " 3 2 3 1 1 3 2 3 2 0 3 0 0 1 0 3 2 0 3 3 3 1 2 3 0 3 0 3 2 3 0 0 2 3 0 1 2\n",
      " 0 0 0 3 3 0 0 3 3 3 3 1 3 3 1 2 1 0 3 3 3 3 0 3 0 3 3 3 3 3 2 3 1 3 1 0 3\n",
      " 3 3 3 2 0 3 3 0 3 0 3 0 3 2 3 3 3 0 3 3 3 2 3 3 1 3 3 3 0 0 0 1 3 0 1 3 3\n",
      " 3 3 3 3 3 2 0 1 0 3 2 0 2 1 0 3 3 0 1 3 2 0 3 1 3 3 3 3 0 3 1 0 2 3 3 0 3\n",
      " 2 0 3 3 2 3 1 3 0 3 1 3 3 3 3 0 3 2 3 3 3 3 3 3 3 0 3 0 3 0 0 3 2 0 3 2 3\n",
      " 3 2 3 3 3 2 1 3 3 0 0 1 1 1 0 3 2 3 1 2 3 0 3 3 3 3 0 0 3 0 3 0 1 3 0 0 1\n",
      " 3 3 3 3 0 3 0 1 2 2 3 0 1 3 1 3 2 2 0 3 3 1 1 3 0 3 3 3 0 3 3 2 0 0 3 0 1\n",
      " 2 0 1 3 3 3 3 3 1 3 3 0 3 0 0 0 1 2 2 3 0 1 3 2 0 0 3 2 3 2 0 1 3 3 0 3 3\n",
      " 3 2 3 3 2 3 3 3 3 1 3 1 3 1 1 3 3 3 3 1 1 3 3 2 3 0 3 0 2 0 0 0 3 1 3 3 3\n",
      " 3 2 0 2 3 3 1 3 2 0 3 3 3 3 2 2 3 2 2 3 3 3 1 3 3 1 2 2 3 3 0 3 3 0 3 0 0\n",
      " 2 1 3 0 0 3 3 1 0 3 3 3 3 0 3 3 1 3 3 3 1 0 2 0 2 0 2 0 2 1 0 1 2 0 0 3 2\n",
      " 3 3 3 3 1 3 2 3 1 1 0 3 0 3 1 3 0 3 3 1 1 3 0 3 3 3 3 1 3 0 2 3 2 3 0 3 3\n",
      " 2 3 3 2 3 1 3 3 1 3 0 3 3 2 1 3 3 3 3 3 2 3 0 0 1 0 3 3 0 0 0 3 3 3 3 3 3\n",
      " 3 3 0 2 0 1 0 3 3 3 1 1 1 1 3 3 0 1 3 2 1 3 0 0 3 3 1 0 3 3 2 3 2 1 2 0 0\n",
      " 3 3 3 3 0 3 3 1 1 3 2 1 0 0 3 0 3 1 3 3 0 3 3 1 3 3 3 3 0 0 3 2 0 3 0 0 3\n",
      " 3 0 3 1 3 3 0 2 0 0 0 0 0 3 0 3 3 1 3 1 3 3 0 3 0 3 0 3 1 3 0 0 1 3 0 3 1\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "print(final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    0.500\n",
       "0    0.239\n",
       "1    0.136\n",
       "2    0.125\n",
       "Name: accuracy_group, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission['accuracy_group'] = final_pred.astype(int)\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "sample_submission['accuracy_group'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights = {'lbg': 0.75, 'cat': 0.05, 'xgb': 0.10, 'nn': 0.1}\n",
    "\n",
    "#final_pred = (lgb_model.y_pred * weights['lbg']) +(cat_model.y_pred * weights['cat']) +(xgb_model.y_pred * weights['xgb']) + (nn_model.y_pred * weights['nn'])\n",
    "#final_pred = cnn_model.y_pred\n",
    "#print(final_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame([(round(a, 2), round(b, 2), round(c, 2), round(d, 2)) for a, b, c, d in zip(lgb_model.y_pred, cat_model.y_pred, xgb_model.y_pred, nn_model.y_pred)], columns=['lgb', 'cat', 'xgb', 'nn']).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dist = Counter(reduce_train['accuracy_group'])\n",
    "for k in dist:\n",
    "    dist[k] /= len(reduce_train)\n",
    "reduce_train['accuracy_group'].hist()\n",
    "\n",
    "acum = 0\n",
    "bound = {}\n",
    "for i in range(3):\n",
    "    acum += dist[i]\n",
    "    bound[i] = np.percentile(final_pred, acum * 100)\n",
    "print(bound)\n",
    "\n",
    "def classify(x):\n",
    "    if x <= bound[0]:\n",
    "        return 0\n",
    "    elif x <= bound[1]:\n",
    "        return 1\n",
    "    elif x <= bound[2]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "    \n",
    "final_pred = np.array(list(map(classify, final_pred)))\n",
    "\n",
    "sample_submission['accuracy_group'] = final_pred.astype(int)\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "sample_submission['accuracy_group'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "05d049dfd8a9416097ef6ae578acd4d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e6d9440dc95341ad9f988afec2e34f20",
       "placeholder": "",
       "style": "IPY_MODEL_4c37c3262103414a9c3525c12ee26a23",
       "value": " 16361/17000 [1:07:57&lt;08:46,  1.21it/s]"
      }
     },
     "4c37c3262103414a9c3525c12ee26a23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5d33acef9afc4fbbb692aa0e031a907a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c5248d915a554175a77695222741ad19",
        "IPY_MODEL_05d049dfd8a9416097ef6ae578acd4d9"
       ],
       "layout": "IPY_MODEL_7c4bc254e32f487f8713b6f1b7d19ec7"
      }
     },
     "7c395715d7294316bf2ecc42074eebba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "7c4bc254e32f487f8713b6f1b7d19ec7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "982bb40029a64d2b9ceb0b2717050bba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c5248d915a554175a77695222741ad19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": " 96%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_982bb40029a64d2b9ceb0b2717050bba",
       "max": 17000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7c395715d7294316bf2ecc42074eebba",
       "value": 16362
      }
     },
     "e6d9440dc95341ad9f988afec2e34f20": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
